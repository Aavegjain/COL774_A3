{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \n",
    "    def __init__(self, id , is_output_node = False ): \n",
    "        self.id = id \n",
    "        self.value = None \n",
    "        self.net = None \n",
    "        self.delta = None \n",
    "        self.downstream_nodes = np.array([]) \n",
    "        self.upstream_nodes = np.array([]) \n",
    "        self.downstream_weights = np.array([]) \n",
    "        self.upstream_idxs = np.array([])  # index of current node in upstream node's downstream list \n",
    "        self.is_output_node = is_output_node \n",
    "        self.actual_output = None # relevant only when the node is output node \n",
    "\n",
    "    def add_downstream_node(self, node): # returns idx in the downstream_nodes list \n",
    "        self.downstream_nodes = np.append(self.downstream_nodes, node) \n",
    "        # get a random number b/w -0.5 and 0.5\n",
    "        x = np.random.rand() - 0.5\n",
    "        self.downstream_weights = np.append(self.downstream_weights, x )   \n",
    "        # self.downstream_weights = np.append(self.downstream_weights, 0 ) \n",
    "        return len(self.downstream_nodes) - 1 \n",
    "    \n",
    "    def add_upstream_node(self, node, idx):\n",
    "        self.upstream_nodes = np.append(self.upstream_nodes, node) \n",
    "        self.upstream_idxs = np.append(self.upstream_idxs, idx)  \n",
    "    \n",
    "    def compute_net(self):\n",
    "        net = 0 \n",
    "        for i in range(len(self.upstream_nodes)):\n",
    "            idx = self.upstream_idxs[i] \n",
    "            # print(\"idx is \", idx) \n",
    "            # print(\"i is \", i)\n",
    "            net += ( (self.upstream_nodes[i].value) * ((self.upstream_nodes[i]).downstream_weights[int(idx)])  ) \n",
    "        self.net = net \n",
    "        return net \n",
    "    # write a print function for this class \n",
    "    def __repr__(self):\n",
    "        if (not self.is_output_node): return \"Node: \" + str(self.id)  \n",
    "        else : return \"Output Node: \" + str(self.id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticNode(Node):\n",
    "    # i assume loss as cross entropy loss in compute delta\n",
    "    def activation_function(self, x):\n",
    "        return 1 / (1 + math.exp(-x)) \n",
    "\n",
    "    def compute_value(self):\n",
    "        net = self.compute_net() \n",
    "        self.value = self.activation_function(net) \n",
    "\n",
    "    def compute_delta(self, actual_output = None):\n",
    "        assert(self.value != None) # value should be computed before delta\n",
    "        if (self.is_output_node):\n",
    "            pass # no need as logistic node not in output layer for multiclass classification \n",
    "        else:\n",
    "            sum = 0 \n",
    "            multiply = self.value * (1 - self.value) \n",
    "            for i in range(len(self.downstream_nodes)):\n",
    "                sum += (self.downstream_nodes[i].delta * self.downstream_weights[i]) \n",
    "            self.delta = multiply * sum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxNode(Node):\n",
    "    def __init__(self, id, is_output_node = False):\n",
    "        super().__init__(id, is_output_node) \n",
    "        self.j = None # class represented by the output node \n",
    "        self.output_layer = None \n",
    "    \n",
    "    def set_output_layer(self, output_layer):\n",
    "        self.output_layer = output_layer\n",
    "        for i in range(len(output_layer)):\n",
    "            if (output_layer[i] == self):\n",
    "                self.j = i \n",
    "                break\n",
    "        \n",
    "    \n",
    "    def compute_output_value(self):\n",
    "        # assuming here net for all output nodes have been computed \n",
    "        sum = np.sum(np.exp([node.net for node in self.output_layer])) \n",
    "        self.value = math.exp(self.net) / sum \n",
    "    \n",
    "    def compute_delta(self, actual_output = None): \n",
    "        assert(self.value != None) # value should be computed before delta \n",
    "        if (self.is_output_node):\n",
    "            if (self.j == int(actual_output)):\n",
    "                self.delta = 1 - self.value\n",
    "            else : \n",
    "                self.delta =  -self.value  \n",
    "        else :\n",
    "            sum = 0 \n",
    "            multiply = self.value * (1 - self.value) \n",
    "            for i in range(len(self.downstream_nodes)):\n",
    "                sum += (self.downstream_nodes[i].delta * self.downstream_weights[i]) \n",
    "            self.delta = multiply * sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "\n",
    "    def __init__(self, minibatch_size, no_of_features, hidden_layers, no_of_classes):\n",
    "        self.M = minibatch_size \n",
    "        self.n = no_of_features\n",
    "        self.hidden_layers = hidden_layers \n",
    "        self.K = no_of_classes \n",
    "        self.training_data = None # should be already processed\n",
    "        self.training_labels = None # should be already processed\n",
    "    \n",
    "    \n",
    "    def make_network(self):\n",
    "        cnt = 1 \n",
    "        network = [] \n",
    "        input_layer, output_layer = np.array([]), np.array([]) \n",
    "\n",
    "        for i in range(self.n):\n",
    "            input_layer = np.append(input_layer, LogisticNode(cnt, False)) \n",
    "            cnt += 1\n",
    "        \n",
    "        # print(input_layer)\n",
    "        network.append(input_layer)\n",
    "        for size in self.hidden_layers:\n",
    "            hidden_layer = np.array([]) \n",
    "            for i in range(size):\n",
    "                hidden_layer = np.append(hidden_layer, LogisticNode(cnt))  \n",
    "                cnt += 1  \n",
    "            # print(hidden_layer)\n",
    "            network.append(hidden_layer)  \n",
    "        \n",
    "        for i in range(self.K):\n",
    "            output_layer = np.append(output_layer, SoftmaxNode(cnt, is_output_node = True)) \n",
    "            cnt += 1\n",
    "        # print(output_layer)\n",
    "\n",
    "        for node in output_layer:\n",
    "            node.set_output_layer(output_layer)\n",
    "        network.append(output_layer)\n",
    "        # inserting the edges \n",
    "        # fully connected architecture \n",
    "\n",
    "        for i in range(len(network) - 1):\n",
    "            # print(f\"connecting layers {i} and {i+1}\") \n",
    "            for node in network[i]:\n",
    "                for next_node in network[i+1]:\n",
    "                    next_node.add_upstream_node(node, node.add_downstream_node(next_node)) \n",
    "\n",
    "        print(network)\n",
    "        self.network = network \n",
    "        print(\"network created\") \n",
    "    \n",
    "    def forward_propagation(self, eg):\n",
    "        # eg is training example which we will propagate forwards \n",
    "        for feature, node in zip(eg, self.network[0]):\n",
    "            node.value = feature\n",
    "        # print(\"input layer initialized\") \n",
    "\n",
    "        for i in range(1, len(self.network) - 1):\n",
    "            for node in self.network[i]:\n",
    "                node.compute_value()\n",
    "        \n",
    "        for node in self.network[len(self.network) - 1]:\n",
    "            node.compute_net() \n",
    "        for node in self.network[len(self.network) - 1]:\n",
    "            node.compute_output_value() \n",
    "        \n",
    "        # print(\"forward propagation done\")\n",
    "    \n",
    "\n",
    "    def back_propagation(self, actual_output):\n",
    "        # actual_output is the actual output of the training example\n",
    "        # output layer\n",
    "        for node in self.network[len(self.network) - 1]:\n",
    "            node.compute_delta(actual_output)\n",
    "        # hidden layers\n",
    "        for i in range(len(self.network) - 2 , -1, -1):\n",
    "            for node in self.network[i]:\n",
    "                node.compute_delta()\n",
    "        # print(\"back propagation done\") \n",
    "    \n",
    "    def propagate(self, eg, label):\n",
    "        self.forward_propagation(eg) \n",
    "        self.back_propagation(label) \n",
    "    \n",
    "    \n",
    "\n",
    "    def get_weight_diff(self, w1, w2):\n",
    "        sum = 0 \n",
    "        for i in range(len(w1)):\n",
    "            sum += abs(w1[i] - w2[i])\n",
    "        return sum \n",
    "\n",
    "\n",
    "    def weight_update(self, delta_list, value_list, learning_rate):\n",
    "\n",
    "\n",
    "        for i in range(len(self.network) - 1):\n",
    "            for l  in range(len(self.network[i])):\n",
    "                node = self.network[i][l] \n",
    "                for j in range(len(node.downstream_nodes)):\n",
    "                    # node.downstream_weights[j] += (learning_rate * delta_list[i][j] * value_list[i][j])/self.M \n",
    "                    update = 0 \n",
    "                    for k in range(len(delta_list)):\n",
    "                        # assert(value_list[k][i][l] != 0)\n",
    "                        # assert(delta_list[k][i + 1][j] != 0) \n",
    "                        \n",
    "                        update += (delta_list[k][i + 1][j] * value_list[k][i][l])\n",
    "                    \n",
    "                    update *= learning_rate/self.M\n",
    "                    update *= learning_rate\n",
    "                    node.downstream_weights[j] += update \n",
    "                    # assert(update != 0)\n",
    "        # print(\"len of delta list was \", len(delta_list))\n",
    "        # print(\"weight update done\")\n",
    "\n",
    "    def train(self, training_data, training_labels, learning_rate):\n",
    "        self.training_data, self.training_labels =  self.shuffle(training_data, training_labels) \n",
    "        prev_weights =[]\n",
    "        weights = []\n",
    "        for layer in self.network:\n",
    "            for node in layer:\n",
    "                for i in range(len(node.downstream_weights)):\n",
    "                            weights.append(node.downstream_weights[i])\n",
    "        cnt = 0 \n",
    "        while (cnt < 400):\n",
    "        \n",
    "            for i in range(0, len(training_data), self.M):\n",
    "                minibatch_data = training_data[i : i + self.M] \n",
    "                minibatch_labels = training_labels[i : i + self.M]\n",
    "                delta_list = []\n",
    "                value_list = []\n",
    "                for eg, label in zip(minibatch_data, minibatch_labels):\n",
    "                \n",
    "                        self.propagate(eg, label) \n",
    "                        network_delta, network_values = [],  [] \n",
    "                        for layer in self.network:\n",
    "                            layer_deltas, layer_values = [],  []\n",
    "                            for node in layer:\n",
    "                                # layer_deltas = np.append(layer_deltas, node.delta) \n",
    "                                layer_deltas.append(node.delta)\n",
    "                                # layer_values = np.append(layer_values, node.value)\n",
    "                                layer_values.append(node.value)  \n",
    "                            # print(\"shape of layer_deltas is \", layer_deltas.shape)\n",
    "                            # network_delta = np.append(network_delta, layer_deltas)\n",
    "                            network_delta.append(layer_deltas)\n",
    "                            # network_values = np.append(network_values, layer_values) \n",
    "                            network_values.append(layer_values)\n",
    "                        # print(\"shape of network_delta is \", network_delta.shape)\n",
    "                        # input() \n",
    "                        # delta_list = np.append(delta_list, network_delta)\n",
    "                        delta_list.append(network_delta)\n",
    "                        # value_list = np.append(value_list, network_values)\n",
    "                        value_list.append(network_values)\n",
    "                # print(\"shape of delta_list is \", delta_list.shape)\n",
    "                # print(\"shape of value_list is \", value_list.shape) \n",
    "                # prev_weights = weights \n",
    "                # weights = [] \n",
    "                self.weight_update(delta_list, value_list, learning_rate)  \n",
    "                # for layer in self.network:\n",
    "                    # for node in layer:\n",
    "                        # for i in range(len(node.downstream_weights)):\n",
    "                            # weights.append(node.downstream_weights[i])\n",
    "                # diff = self.get_weight_diff(weights, prev_weights)\n",
    "                # print(\"weights updated by \", diff)\n",
    "            \n",
    "            cnt += 1 \n",
    "            print(f\"epoch {cnt} done\")\n",
    "            # computed_accuracy, loss = self.compute_accuracy_and_loss(X_train, Y_train) \n",
    "            # loss = self.compute_loss(training_data, training_labels)\n",
    "            # print(\"accuracy is \", computed_accuracy)\n",
    "            # print(\"loss is \", loss)\n",
    "        \n",
    "        print(\"training done\")\n",
    "    \n",
    "    def predict(self, eg, answer):\n",
    "        self.forward_propagation(eg) \n",
    "        max = -1 \n",
    "        loss = 0 \n",
    "        for node in self.network[len(self.network) - 1]:\n",
    "            if (node.value > max):\n",
    "                max = node.value \n",
    "                prediction = node.j \n",
    "        for k in range(len(self.network[len(self.network) - 1])):\n",
    "            node = self.network[len(self.network) - 1][k] \n",
    "            if (node.j == answer):\n",
    "                loss -= math.log(node.value)\n",
    "        return prediction,loss \n",
    "\n",
    "    def compute_accuracy_and_loss(self, test_data, test_labels):\n",
    "        correct = 0 \n",
    "        total_loss = 0 \n",
    "        for eg, label in zip(test_data, test_labels):\n",
    "            prediction, loss = self.predict(eg, label) \n",
    "            if (prediction == label):\n",
    "                correct += 1 \n",
    "            total_loss += loss \n",
    "        print(\"Correct: \", correct) \n",
    "        print(\"Total: \", len(test_data))\n",
    "        return (correct / len(test_data), total_loss) \n",
    "\n",
    "    \n",
    "    def shuffle(self, training_data, training_labels):\n",
    "        idxs = np.array([i for i in range(len(training_data))]) \n",
    "        idx = np.random.shuffle(idxs)\n",
    "        print(\"shuffled idx is \", idxs) \n",
    "        training_data = training_data[idxs]\n",
    "        training_labels = training_labels[idxs]\n",
    "        return (training_data, training_labels)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        to_print = \"\"\n",
    "        for layer in self.network:\n",
    "            for node in layer:\n",
    "                to_print += str(node) + \" \"\n",
    "\n",
    "                to_print += \"\\n\"\n",
    "                to_print += \"downstream nodes: \" + str(node.downstream_nodes) + \"\\n\"\n",
    "                # to_print += \"downstream weights: \" + str(node.downstream_weights) + \"\\n\"\n",
    "                to_print += \"upstream idxs: \" + str(node.upstream_idxs) + \"\\n\"\n",
    "                to_print += \"upstream nodes: \" + str(node.upstream_nodes) + \"\\n\"\n",
    "                to_print += \"value: \" + str(node.value) + \"\\n\"\n",
    "                to_print += \"delta: \" + str(node.delta) + \"\\n\"\n",
    "                if (node.is_output_node):\n",
    "                    to_print += \"output class is \" + str(node.j) + \"\\n\"\n",
    "                to_print += \"-----------------\" \n",
    "                to_print += \"\\n\"\n",
    "        return to_print\n",
    "            \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([Node: 1, Node: 2, Node: 3], dtype=object), array([Node: 4, Node: 5, Node: 6], dtype=object), array([Output Node: 7, Output Node: 8, Output Node: 9], dtype=object)]\n",
      "network created\n"
     ]
    }
   ],
   "source": [
    "my_nn = Neural_Network(10, 3, [3], 3) \n",
    "my_nn.make_network()\n",
    "# print(my_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nn.forward_propagation(np.array([1, 2, 3])) \n",
    "my_nn.back_propagation(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffled idx is  [2 1 0]\n",
      "[[7 8 9]\n",
      " [4 5 6]\n",
      " [1 2 3]] [3 2 1]\n"
     ]
    }
   ],
   "source": [
    "def get_input(input_path, output_path):\n",
    "    x = np.load(input_path)\n",
    "    y = np.load(output_path)\n",
    "\n",
    "    y = y.astype('float')\n",
    "    x = x.astype('float')\n",
    "\n",
    "    #normalize x:\n",
    "    x = 2*(0.5 - x/255)\n",
    "    return x,y\n",
    "x = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "y = np.array([1,2,3])\n",
    "x, y = my_nn.shuffle(x,y)\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1024)\n",
      "(10000,)\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = get_input(\"../data/part b/x_train.npy\", \"../data/part b/y_train.npy\") \n",
    "X_test, Y_test = get_input(\"../data/part b/x_test.npy\", \"../data/part b/y_test.npy\" )\n",
    "print(X_train.shape) \n",
    "print(Y_train.shape)\n",
    "Y_train = Y_train - 1 \n",
    "Y_test = Y_test - 1 \n",
    "\n",
    "label_encoder = OneHotEncoder(sparse_output = False)\n",
    "label_encoder.fit(np.expand_dims(Y_train, axis = -1))\n",
    "\n",
    "y_train_onehot = label_encoder.transform(np.expand_dims(Y_train, axis = -1))\n",
    "y_test_onehot = label_encoder.transform(np.expand_dims(Y_test, axis = -1))\n",
    "\n",
    "print(y_train_onehot) \n",
    "print(y_test_onehot) \n",
    "# print(X_train)\n",
    "# print(Y_train) \n",
    "# print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([Node: 1, Node: 2, Node: 3, ..., Node: 1022, Node: 1023, Node: 1024],\n",
      "      dtype=object), array([Node: 1025, Node: 1026, Node: 1027, Node: 1028, Node: 1029,\n",
      "       Node: 1030, Node: 1031, Node: 1032, Node: 1033, Node: 1034],\n",
      "      dtype=object), array([Output Node: 1035, Output Node: 1036, Output Node: 1037,\n",
      "       Output Node: 1038, Output Node: 1039], dtype=object)]\n",
      "network created\n",
      "shuffled idx is  [ 929 8879 3661 ... 9319 6200 8260]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\surbh\\Desktop\\aaveg\\sem5\\COL774\\COL774_A3\\neural_networks\\nn.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m nn\u001b[39m.\u001b[39mmake_network()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# print(nn)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m nn\u001b[39m.\u001b[39mtrain(X_train, Y_train, \u001b[39m0.01\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\surbh\\Desktop\\aaveg\\sem5\\COL774\\COL774_A3\\neural_networks\\nn.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m value_list \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39mfor\u001b[39;00m eg, label \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(minibatch_data, minibatch_labels):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate(eg, label) \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m         network_delta, network_values \u001b[39m=\u001b[39m [],  [] \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m         \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork:\n",
      "\u001b[1;32mc:\\Users\\surbh\\Desktop\\aaveg\\sem5\\COL774\\COL774_A3\\neural_networks\\nn.ipynb Cell 10\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpropagate\u001b[39m(\u001b[39mself\u001b[39m, eg, label):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_propagation(eg) \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mback_propagation(label)\n",
      "\u001b[1;32mc:\\Users\\surbh\\Desktop\\aaveg\\sem5\\COL774\\COL774_A3\\neural_networks\\nn.ipynb Cell 10\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork) \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m , \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork[i]:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m         node\u001b[39m.\u001b[39mcompute_delta()\n",
      "\u001b[1;32mc:\\Users\\surbh\\Desktop\\aaveg\\sem5\\COL774\\COL774_A3\\neural_networks\\nn.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m multiply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownstream_nodes)):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39msum\u001b[39m \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownstream_nodes[i]\u001b[39m.\u001b[39mdelta \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownstream_weights[i]) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelta \u001b[39m=\u001b[39m multiply \u001b[39m*\u001b[39m \u001b[39msum\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn = Neural_Network(32, 1024, [10], 5) \n",
    "nn.make_network()\n",
    "# print(nn)\n",
    "nn.train(X_train, Y_train, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.59247996\n",
      "Iteration 2, loss = 1.48101393\n",
      "Iteration 3, loss = 1.30370545\n",
      "Iteration 4, loss = 1.17780136\n",
      "Iteration 5, loss = 1.10203257\n",
      "Iteration 6, loss = 1.04988275\n",
      "Iteration 7, loss = 1.00934160\n",
      "Iteration 8, loss = 0.97588285\n",
      "Iteration 9, loss = 0.94766919\n",
      "Iteration 10, loss = 0.92369537\n",
      "Iteration 11, loss = 0.90321915\n",
      "Iteration 12, loss = 0.88562936\n",
      "Iteration 13, loss = 0.87042135\n",
      "Iteration 14, loss = 0.85718538\n",
      "Iteration 15, loss = 0.84559192\n",
      "Iteration 16, loss = 0.83537623\n",
      "Iteration 17, loss = 0.82632452\n",
      "Iteration 18, loss = 0.81826292\n",
      "Iteration 19, loss = 0.81104888\n",
      "Iteration 20, loss = 0.80456471\n",
      "Iteration 21, loss = 0.79871259\n",
      "Iteration 22, loss = 0.79341075\n",
      "Iteration 23, loss = 0.78859037\n",
      "Iteration 24, loss = 0.78419325\n",
      "Iteration 25, loss = 0.78016987\n",
      "Iteration 26, loss = 0.77647781\n",
      "Iteration 27, loss = 0.77308058\n",
      "Iteration 28, loss = 0.76994657\n",
      "Iteration 29, loss = 0.76704826\n",
      "Iteration 30, loss = 0.76436160\n",
      "Iteration 31, loss = 0.76186545\n",
      "Iteration 32, loss = 0.75954117\n",
      "Iteration 33, loss = 0.75737226\n",
      "Iteration 34, loss = 0.75534406\n",
      "Iteration 35, loss = 0.75344353\n",
      "Iteration 36, loss = 0.75165903\n",
      "Iteration 37, loss = 0.74998014\n",
      "Iteration 38, loss = 0.74839752\n",
      "Iteration 39, loss = 0.74690278\n",
      "Iteration 40, loss = 0.74548835\n",
      "Iteration 41, loss = 0.74414742\n",
      "Iteration 42, loss = 0.74287383\n",
      "Iteration 43, loss = 0.74166199\n",
      "Iteration 44, loss = 0.74050684\n",
      "Iteration 45, loss = 0.73940377\n",
      "Iteration 46, loss = 0.73834857\n",
      "Iteration 47, loss = 0.73733742\n",
      "Iteration 48, loss = 0.73636680\n",
      "Iteration 49, loss = 0.73543348\n",
      "Iteration 50, loss = 0.73453451\n",
      "Iteration 51, loss = 0.73366716\n",
      "Iteration 52, loss = 0.73282890\n",
      "Iteration 53, loss = 0.73201739\n",
      "Iteration 54, loss = 0.73123048\n",
      "Iteration 55, loss = 0.73046614\n",
      "Iteration 56, loss = 0.72972249\n",
      "Iteration 57, loss = 0.72899779\n",
      "Iteration 58, loss = 0.72829038\n",
      "Iteration 59, loss = 0.72759876\n",
      "Iteration 60, loss = 0.72692148\n",
      "Iteration 61, loss = 0.72625721\n",
      "Iteration 62, loss = 0.72560473\n",
      "Iteration 63, loss = 0.72496288\n",
      "Iteration 64, loss = 0.72433062\n",
      "Iteration 65, loss = 0.72370699\n",
      "Iteration 66, loss = 0.72309113\n",
      "Iteration 67, loss = 0.72248225\n",
      "Iteration 68, loss = 0.72187967\n",
      "Iteration 69, loss = 0.72128279\n",
      "Iteration 70, loss = 0.72069109\n",
      "Iteration 71, loss = 0.72010415\n",
      "Iteration 72, loss = 0.71952158\n",
      "Iteration 73, loss = 0.71894310\n",
      "Iteration 74, loss = 0.71836845\n",
      "Iteration 75, loss = 0.71779745\n",
      "Iteration 76, loss = 0.71722994\n",
      "Iteration 77, loss = 0.71666580\n",
      "Iteration 78, loss = 0.71610495\n",
      "Iteration 79, loss = 0.71554730\n",
      "Iteration 80, loss = 0.71499282\n",
      "Iteration 81, loss = 0.71444144\n",
      "Iteration 82, loss = 0.71389314\n",
      "Iteration 83, loss = 0.71334787\n",
      "Iteration 84, loss = 0.71280560\n",
      "Iteration 85, loss = 0.71226631\n",
      "Iteration 86, loss = 0.71172993\n",
      "Iteration 87, loss = 0.71119645\n",
      "Iteration 88, loss = 0.71066580\n",
      "Iteration 89, loss = 0.71013794\n",
      "Iteration 90, loss = 0.70961281\n",
      "Iteration 91, loss = 0.70909035\n",
      "Iteration 92, loss = 0.70857051\n",
      "Iteration 93, loss = 0.70805321\n",
      "Iteration 94, loss = 0.70753838\n",
      "Iteration 95, loss = 0.70702595\n",
      "Iteration 96, loss = 0.70651585\n",
      "Iteration 97, loss = 0.70600800\n",
      "Iteration 98, loss = 0.70550231\n",
      "Iteration 99, loss = 0.70499872\n",
      "Iteration 100, loss = 0.70449713\n",
      "Iteration 101, loss = 0.70399746\n",
      "Iteration 102, loss = 0.70349964\n",
      "Iteration 103, loss = 0.70300356\n",
      "Iteration 104, loss = 0.70250916\n",
      "Iteration 105, loss = 0.70201635\n",
      "Iteration 106, loss = 0.70152503\n",
      "Iteration 107, loss = 0.70103513\n",
      "Iteration 108, loss = 0.70054656\n",
      "Iteration 109, loss = 0.70005923\n",
      "Iteration 110, loss = 0.69957306\n",
      "Iteration 111, loss = 0.69908796\n",
      "Iteration 112, loss = 0.69860384\n",
      "Iteration 113, loss = 0.69812063\n",
      "Iteration 114, loss = 0.69763823\n",
      "Iteration 115, loss = 0.69715655\n",
      "Iteration 116, loss = 0.69667553\n",
      "Iteration 117, loss = 0.69619507\n",
      "Iteration 118, loss = 0.69571509\n",
      "Iteration 119, loss = 0.69523551\n",
      "Iteration 120, loss = 0.69475626\n",
      "Iteration 121, loss = 0.69427726\n",
      "Iteration 122, loss = 0.69379843\n",
      "Iteration 123, loss = 0.69331971\n",
      "Iteration 124, loss = 0.69284103\n",
      "Iteration 125, loss = 0.69236233\n",
      "Iteration 126, loss = 0.69188355\n",
      "Iteration 127, loss = 0.69140463\n",
      "Iteration 128, loss = 0.69092553\n",
      "Iteration 129, loss = 0.69044620\n",
      "Iteration 130, loss = 0.68996662\n",
      "Iteration 131, loss = 0.68948676\n",
      "Iteration 132, loss = 0.68900658\n",
      "Iteration 133, loss = 0.68852609\n",
      "Iteration 134, loss = 0.68804528\n",
      "Iteration 135, loss = 0.68756414\n",
      "Iteration 136, loss = 0.68708271\n",
      "Iteration 137, loss = 0.68660099\n",
      "Iteration 138, loss = 0.68611902\n",
      "Iteration 139, loss = 0.68563685\n",
      "Iteration 140, loss = 0.68515452\n",
      "Iteration 141, loss = 0.68467211\n",
      "Iteration 142, loss = 0.68418969\n",
      "Iteration 143, loss = 0.68370733\n",
      "Iteration 144, loss = 0.68322514\n",
      "Iteration 145, loss = 0.68274322\n",
      "Iteration 146, loss = 0.68226169\n",
      "Iteration 147, loss = 0.68178066\n",
      "Iteration 148, loss = 0.68130027\n",
      "Iteration 149, loss = 0.68082065\n",
      "Iteration 150, loss = 0.68034196\n",
      "Iteration 151, loss = 0.67986434\n",
      "Iteration 152, loss = 0.67938796\n",
      "Iteration 153, loss = 0.67891296\n",
      "Iteration 154, loss = 0.67843953\n",
      "Iteration 155, loss = 0.67796781\n",
      "Iteration 156, loss = 0.67749797\n",
      "Iteration 157, loss = 0.67703019\n",
      "Iteration 158, loss = 0.67656462\n",
      "Iteration 159, loss = 0.67610141\n",
      "Iteration 160, loss = 0.67564073\n",
      "Iteration 161, loss = 0.67518271\n",
      "Iteration 162, loss = 0.67472751\n",
      "Iteration 163, loss = 0.67427525\n",
      "Iteration 164, loss = 0.67382606\n",
      "Iteration 165, loss = 0.67338006\n",
      "Iteration 166, loss = 0.67293735\n",
      "Iteration 167, loss = 0.67249804\n",
      "Iteration 168, loss = 0.67206221\n",
      "Iteration 169, loss = 0.67162994\n",
      "Iteration 170, loss = 0.67120130\n",
      "Iteration 171, loss = 0.67077635\n",
      "Iteration 172, loss = 0.67035514\n",
      "Iteration 173, loss = 0.66993771\n",
      "Iteration 174, loss = 0.66952408\n",
      "Iteration 175, loss = 0.66911429\n",
      "Iteration 176, loss = 0.66870833\n",
      "Iteration 177, loss = 0.66830623\n",
      "Iteration 178, loss = 0.66790797\n",
      "Iteration 179, loss = 0.66751355\n",
      "Iteration 180, loss = 0.66712296\n",
      "Iteration 181, loss = 0.66673617\n",
      "Iteration 182, loss = 0.66635315\n",
      "Iteration 183, loss = 0.66597388\n",
      "Iteration 184, loss = 0.66559831\n",
      "Iteration 185, loss = 0.66522642\n",
      "Iteration 186, loss = 0.66485815\n",
      "Iteration 187, loss = 0.66449346\n",
      "Iteration 188, loss = 0.66413230\n",
      "Iteration 189, loss = 0.66377463\n",
      "Iteration 190, loss = 0.66342039\n",
      "Iteration 191, loss = 0.66306952\n",
      "Iteration 192, loss = 0.66272198\n",
      "Iteration 193, loss = 0.66237771\n",
      "Iteration 194, loss = 0.66203665\n",
      "Iteration 195, loss = 0.66169875\n",
      "Iteration 196, loss = 0.66136395\n",
      "Iteration 197, loss = 0.66103219\n",
      "Iteration 198, loss = 0.66070343\n",
      "Iteration 199, loss = 0.66037760\n",
      "Iteration 200, loss = 0.66005466\n",
      "Iteration 201, loss = 0.65973454\n",
      "Iteration 202, loss = 0.65941720\n",
      "Iteration 203, loss = 0.65910259\n",
      "Iteration 204, loss = 0.65879065\n",
      "Iteration 205, loss = 0.65848134\n",
      "Iteration 206, loss = 0.65817461\n",
      "Iteration 207, loss = 0.65787040\n",
      "Iteration 208, loss = 0.65756868\n",
      "Iteration 209, loss = 0.65726939\n",
      "Iteration 210, loss = 0.65697250\n",
      "Iteration 211, loss = 0.65667796\n",
      "Iteration 212, loss = 0.65638573\n",
      "Iteration 213, loss = 0.65609577\n",
      "Iteration 214, loss = 0.65580804\n",
      "Iteration 215, loss = 0.65552249\n",
      "Iteration 216, loss = 0.65523910\n",
      "Iteration 217, loss = 0.65495782\n",
      "Iteration 218, loss = 0.65467862\n",
      "Iteration 219, loss = 0.65440146\n",
      "Iteration 220, loss = 0.65412632\n",
      "Iteration 221, loss = 0.65385315\n",
      "Iteration 222, loss = 0.65358193\n",
      "Iteration 223, loss = 0.65331262\n",
      "Iteration 224, loss = 0.65304520\n",
      "Iteration 225, loss = 0.65277963\n",
      "Iteration 226, loss = 0.65251589\n",
      "Iteration 227, loss = 0.65225394\n",
      "Iteration 228, loss = 0.65199377\n",
      "Iteration 229, loss = 0.65173533\n",
      "Iteration 230, loss = 0.65147862\n",
      "Iteration 231, loss = 0.65122359\n",
      "Iteration 232, loss = 0.65097024\n",
      "Iteration 233, loss = 0.65071852\n",
      "Iteration 234, loss = 0.65046843\n",
      "Iteration 235, loss = 0.65021993\n",
      "Iteration 236, loss = 0.64997300\n",
      "Iteration 237, loss = 0.64972763\n",
      "Iteration 238, loss = 0.64948379\n",
      "Iteration 239, loss = 0.64924146\n",
      "Iteration 240, loss = 0.64900061\n",
      "Iteration 241, loss = 0.64876124\n",
      "Iteration 242, loss = 0.64852331\n",
      "Iteration 243, loss = 0.64828681\n",
      "Iteration 244, loss = 0.64805173\n",
      "Iteration 245, loss = 0.64781803\n",
      "Iteration 246, loss = 0.64758571\n",
      "Iteration 247, loss = 0.64735475\n",
      "Iteration 248, loss = 0.64712513\n",
      "Iteration 249, loss = 0.64689683\n",
      "Iteration 250, loss = 0.64666984\n",
      "Iteration 251, loss = 0.64644413\n",
      "Iteration 252, loss = 0.64621970\n",
      "Iteration 253, loss = 0.64599653\n",
      "Iteration 254, loss = 0.64577460\n",
      "Iteration 255, loss = 0.64555389\n",
      "Iteration 256, loss = 0.64533440\n",
      "Iteration 257, loss = 0.64511611\n",
      "Iteration 258, loss = 0.64489900\n",
      "Iteration 259, loss = 0.64468305\n",
      "Iteration 260, loss = 0.64446827\n",
      "Iteration 261, loss = 0.64425462\n",
      "Iteration 262, loss = 0.64404211\n",
      "Iteration 263, loss = 0.64383071\n",
      "Iteration 264, loss = 0.64362041\n",
      "Iteration 265, loss = 0.64341120\n",
      "Iteration 266, loss = 0.64320307\n",
      "Iteration 267, loss = 0.64299600\n",
      "Iteration 268, loss = 0.64278999\n",
      "Iteration 269, loss = 0.64258501\n",
      "Iteration 270, loss = 0.64238107\n",
      "Iteration 271, loss = 0.64217814\n",
      "Iteration 272, loss = 0.64197622\n",
      "Iteration 273, loss = 0.64177529\n",
      "Iteration 274, loss = 0.64157535\n",
      "Iteration 275, loss = 0.64137638\n",
      "Iteration 276, loss = 0.64117837\n",
      "Iteration 277, loss = 0.64098131\n",
      "Iteration 278, loss = 0.64078520\n",
      "Iteration 279, loss = 0.64059001\n",
      "Iteration 280, loss = 0.64039575\n",
      "Iteration 281, loss = 0.64020240\n",
      "Iteration 282, loss = 0.64000995\n",
      "Iteration 283, loss = 0.63981839\n",
      "Iteration 284, loss = 0.63962771\n",
      "Iteration 285, loss = 0.63943791\n",
      "Iteration 286, loss = 0.63924897\n",
      "Iteration 287, loss = 0.63906088\n",
      "Iteration 288, loss = 0.63887364\n",
      "Iteration 289, loss = 0.63868724\n",
      "Iteration 290, loss = 0.63850167\n",
      "Iteration 291, loss = 0.63831691\n",
      "Iteration 292, loss = 0.63813297\n",
      "Iteration 293, loss = 0.63794983\n",
      "Iteration 294, loss = 0.63776748\n",
      "Iteration 295, loss = 0.63758592\n",
      "Iteration 296, loss = 0.63740514\n",
      "Iteration 297, loss = 0.63722513\n",
      "Iteration 298, loss = 0.63704588\n",
      "Iteration 299, loss = 0.63686739\n",
      "Iteration 300, loss = 0.63668964\n",
      "Iteration 301, loss = 0.63651263\n",
      "Iteration 302, loss = 0.63633636\n",
      "Iteration 303, loss = 0.63616081\n",
      "Iteration 304, loss = 0.63598598\n",
      "Iteration 305, loss = 0.63581186\n",
      "Iteration 306, loss = 0.63563845\n",
      "Iteration 307, loss = 0.63546573\n",
      "Iteration 308, loss = 0.63529370\n",
      "Iteration 309, loss = 0.63512236\n",
      "Iteration 310, loss = 0.63495169\n",
      "Iteration 311, loss = 0.63478170\n",
      "Iteration 312, loss = 0.63461236\n",
      "Iteration 313, loss = 0.63444369\n",
      "Iteration 314, loss = 0.63427567\n",
      "Iteration 315, loss = 0.63410829\n",
      "Iteration 316, loss = 0.63394155\n",
      "Iteration 317, loss = 0.63377544\n",
      "Iteration 318, loss = 0.63360996\n",
      "Iteration 319, loss = 0.63344510\n",
      "Iteration 320, loss = 0.63328086\n",
      "Iteration 321, loss = 0.63311723\n",
      "Iteration 322, loss = 0.63295420\n",
      "Iteration 323, loss = 0.63279177\n",
      "Iteration 324, loss = 0.63262993\n",
      "Iteration 325, loss = 0.63246868\n",
      "Iteration 326, loss = 0.63230801\n",
      "Iteration 327, loss = 0.63214792\n",
      "Iteration 328, loss = 0.63198840\n",
      "Iteration 329, loss = 0.63182944\n",
      "Iteration 330, loss = 0.63167105\n",
      "Iteration 331, loss = 0.63151321\n",
      "Iteration 332, loss = 0.63135592\n",
      "Iteration 333, loss = 0.63119918\n",
      "Iteration 334, loss = 0.63104298\n",
      "Iteration 335, loss = 0.63088731\n",
      "Iteration 336, loss = 0.63073218\n",
      "Iteration 337, loss = 0.63057758\n",
      "Iteration 338, loss = 0.63042349\n",
      "Iteration 339, loss = 0.63026993\n",
      "Iteration 340, loss = 0.63011688\n",
      "Iteration 341, loss = 0.62996433\n",
      "Iteration 342, loss = 0.62981229\n",
      "Iteration 343, loss = 0.62966075\n",
      "Iteration 344, loss = 0.62950971\n",
      "Iteration 345, loss = 0.62935916\n",
      "Iteration 346, loss = 0.62920909\n",
      "Iteration 347, loss = 0.62905951\n",
      "Iteration 348, loss = 0.62891041\n",
      "Iteration 349, loss = 0.62876178\n",
      "Iteration 350, loss = 0.62861362\n",
      "Iteration 351, loss = 0.62846593\n",
      "Iteration 352, loss = 0.62831870\n",
      "Iteration 353, loss = 0.62817194\n",
      "Iteration 354, loss = 0.62802562\n",
      "Iteration 355, loss = 0.62787976\n",
      "Iteration 356, loss = 0.62773435\n",
      "Iteration 357, loss = 0.62758938\n",
      "Iteration 358, loss = 0.62744485\n",
      "Iteration 359, loss = 0.62730076\n",
      "Iteration 360, loss = 0.62715710\n",
      "Iteration 361, loss = 0.62701387\n",
      "Iteration 362, loss = 0.62687107\n",
      "Iteration 363, loss = 0.62672869\n",
      "Iteration 364, loss = 0.62658673\n",
      "Iteration 365, loss = 0.62644519\n",
      "Iteration 366, loss = 0.62630406\n",
      "Iteration 367, loss = 0.62616334\n",
      "Iteration 368, loss = 0.62602303\n",
      "Iteration 369, loss = 0.62588312\n",
      "Iteration 370, loss = 0.62574361\n",
      "Iteration 371, loss = 0.62560450\n",
      "Iteration 372, loss = 0.62546578\n",
      "Iteration 373, loss = 0.62532745\n",
      "Iteration 374, loss = 0.62518952\n",
      "Iteration 375, loss = 0.62505196\n",
      "Iteration 376, loss = 0.62491479\n",
      "Iteration 377, loss = 0.62477800\n",
      "Iteration 378, loss = 0.62464158\n",
      "Iteration 379, loss = 0.62450554\n",
      "Iteration 380, loss = 0.62436987\n",
      "Iteration 381, loss = 0.62423457\n",
      "Iteration 382, loss = 0.62409963\n",
      "Iteration 383, loss = 0.62396505\n",
      "Iteration 384, loss = 0.62383084\n",
      "Iteration 385, loss = 0.62369698\n",
      "Iteration 386, loss = 0.62356347\n",
      "Iteration 387, loss = 0.62343032\n",
      "Iteration 388, loss = 0.62329752\n",
      "Iteration 389, loss = 0.62316506\n",
      "Iteration 390, loss = 0.62303295\n",
      "Iteration 391, loss = 0.62290119\n",
      "Iteration 392, loss = 0.62276976\n",
      "Iteration 393, loss = 0.62263866\n",
      "Iteration 394, loss = 0.62250791\n",
      "Iteration 395, loss = 0.62237748\n",
      "Iteration 396, loss = 0.62224739\n",
      "Iteration 397, loss = 0.62211762\n",
      "Iteration 398, loss = 0.62198818\n",
      "Iteration 399, loss = 0.62185906\n",
      "Iteration 400, loss = 0.62173026\n",
      "Iteration 401, loss = 0.62160179\n",
      "Iteration 402, loss = 0.62147362\n",
      "Iteration 403, loss = 0.62134578\n",
      "Iteration 404, loss = 0.62121824\n",
      "Iteration 405, loss = 0.62109102\n",
      "Iteration 406, loss = 0.62096410\n",
      "Iteration 407, loss = 0.62083749\n",
      "Iteration 408, loss = 0.62071118\n",
      "Iteration 409, loss = 0.62058517\n",
      "Iteration 410, loss = 0.62045947\n",
      "Iteration 411, loss = 0.62033406\n",
      "Iteration 412, loss = 0.62020895\n",
      "Iteration 413, loss = 0.62008413\n",
      "Iteration 414, loss = 0.61995961\n",
      "Iteration 415, loss = 0.61983537\n",
      "Iteration 416, loss = 0.61971142\n",
      "Iteration 417, loss = 0.61958776\n",
      "Iteration 418, loss = 0.61946439\n",
      "Iteration 419, loss = 0.61934129\n",
      "Iteration 420, loss = 0.61921848\n",
      "Iteration 421, loss = 0.61909595\n",
      "Iteration 422, loss = 0.61897369\n",
      "Iteration 423, loss = 0.61885171\n",
      "Iteration 424, loss = 0.61873000\n",
      "Iteration 425, loss = 0.61860857\n",
      "Iteration 426, loss = 0.61848740\n",
      "Iteration 427, loss = 0.61836651\n",
      "Iteration 428, loss = 0.61824588\n",
      "Iteration 429, loss = 0.61812552\n",
      "Iteration 430, loss = 0.61800542\n",
      "Iteration 431, loss = 0.61788558\n",
      "Iteration 432, loss = 0.61776600\n",
      "Iteration 433, loss = 0.61764668\n",
      "Iteration 434, loss = 0.61752762\n",
      "Iteration 435, loss = 0.61740881\n",
      "Iteration 436, loss = 0.61729026\n",
      "Iteration 437, loss = 0.61717196\n",
      "Iteration 438, loss = 0.61705392\n",
      "Iteration 439, loss = 0.61693612\n",
      "Iteration 440, loss = 0.61681857\n",
      "Iteration 441, loss = 0.61670126\n",
      "Iteration 442, loss = 0.61658420\n",
      "Iteration 443, loss = 0.61646739\n",
      "Iteration 444, loss = 0.61635082\n",
      "Iteration 445, loss = 0.61623448\n",
      "Iteration 446, loss = 0.61611839\n",
      "Iteration 447, loss = 0.61600253\n",
      "Iteration 448, loss = 0.61588692\n",
      "Iteration 449, loss = 0.61577153\n",
      "Iteration 450, loss = 0.61565638\n",
      "Iteration 451, loss = 0.61554147\n",
      "Iteration 452, loss = 0.61542678\n",
      "Iteration 453, loss = 0.61531232\n",
      "Iteration 454, loss = 0.61519809\n",
      "Iteration 455, loss = 0.61508409\n",
      "Iteration 456, loss = 0.61497032\n",
      "Iteration 457, loss = 0.61485677\n",
      "Iteration 458, loss = 0.61474344\n",
      "Iteration 459, loss = 0.61463033\n",
      "Iteration 460, loss = 0.61451745\n",
      "Iteration 461, loss = 0.61440478\n",
      "Iteration 462, loss = 0.61429234\n",
      "Iteration 463, loss = 0.61418011\n",
      "Iteration 464, loss = 0.61406809\n",
      "Iteration 465, loss = 0.61395629\n",
      "Iteration 466, loss = 0.61384471\n",
      "Iteration 467, loss = 0.61373333\n",
      "Iteration 468, loss = 0.61362217\n",
      "Iteration 469, loss = 0.61351121\n",
      "Iteration 470, loss = 0.61340047\n",
      "Iteration 471, loss = 0.61328993\n",
      "Iteration 472, loss = 0.61317960\n",
      "Iteration 473, loss = 0.61306948\n",
      "Iteration 474, loss = 0.61295955\n",
      "Iteration 475, loss = 0.61284984\n",
      "Iteration 476, loss = 0.61274032\n",
      "Iteration 477, loss = 0.61263100\n",
      "Iteration 478, loss = 0.61252189\n",
      "Iteration 479, loss = 0.61241297\n",
      "Iteration 480, loss = 0.61230425\n",
      "Iteration 481, loss = 0.61219573\n",
      "Iteration 482, loss = 0.61208740\n",
      "Iteration 483, loss = 0.61197927\n",
      "Iteration 484, loss = 0.61187133\n",
      "Iteration 485, loss = 0.61176358\n",
      "Iteration 486, loss = 0.61165603\n",
      "Iteration 487, loss = 0.61154866\n",
      "Iteration 488, loss = 0.61144149\n",
      "Iteration 489, loss = 0.61133450\n",
      "Iteration 490, loss = 0.61122770\n",
      "Iteration 491, loss = 0.61112109\n",
      "Iteration 492, loss = 0.61101466\n",
      "Iteration 493, loss = 0.61090842\n",
      "Iteration 494, loss = 0.61080236\n",
      "Iteration 495, loss = 0.61069648\n",
      "Iteration 496, loss = 0.61059078\n",
      "Iteration 497, loss = 0.61048527\n",
      "Iteration 498, loss = 0.61037993\n",
      "Iteration 499, loss = 0.61027478\n",
      "Iteration 500, loss = 0.61016980\n",
      "Iteration 501, loss = 0.61006500\n",
      "Iteration 502, loss = 0.60996038\n",
      "Iteration 503, loss = 0.60985593\n",
      "Iteration 504, loss = 0.60975165\n",
      "Iteration 505, loss = 0.60964755\n",
      "Iteration 506, loss = 0.60954363\n",
      "Iteration 507, loss = 0.60943987\n",
      "Iteration 508, loss = 0.60933628\n",
      "Iteration 509, loss = 0.60923287\n",
      "Iteration 510, loss = 0.60912962\n",
      "Iteration 511, loss = 0.60902655\n",
      "Iteration 512, loss = 0.60892364\n",
      "Iteration 513, loss = 0.60882090\n",
      "Iteration 514, loss = 0.60871832\n",
      "Iteration 515, loss = 0.60861591\n",
      "Iteration 516, loss = 0.60851366\n",
      "Iteration 517, loss = 0.60841158\n",
      "Iteration 518, loss = 0.60830966\n",
      "Iteration 519, loss = 0.60820790\n",
      "Iteration 520, loss = 0.60810631\n",
      "Iteration 521, loss = 0.60800487\n",
      "Iteration 522, loss = 0.60790360\n",
      "Iteration 523, loss = 0.60780248\n",
      "Iteration 524, loss = 0.60770152\n",
      "Iteration 525, loss = 0.60760072\n",
      "Iteration 526, loss = 0.60750008\n",
      "Iteration 527, loss = 0.60739959\n",
      "Iteration 528, loss = 0.60729926\n",
      "Iteration 529, loss = 0.60719908\n",
      "Iteration 530, loss = 0.60709905\n",
      "Iteration 531, loss = 0.60699918\n",
      "Iteration 532, loss = 0.60689946\n",
      "Iteration 533, loss = 0.60679990\n",
      "Iteration 534, loss = 0.60670048\n",
      "Iteration 535, loss = 0.60660122\n",
      "Iteration 536, loss = 0.60650210\n",
      "Iteration 537, loss = 0.60640314\n",
      "Iteration 538, loss = 0.60630432\n",
      "Iteration 539, loss = 0.60620565\n",
      "Iteration 540, loss = 0.60610712\n",
      "Iteration 541, loss = 0.60600875\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "test accuracy is 0.698\n",
      "test accuracy is 0.7353\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = MLPClassifier([5], activation=\"logistic\", solver=\"sgd\", batch_size=32, learning_rate=\"constant\" , learning_rate_init=0.03, \n",
    "                    shuffle=False,  momentum=0 , max_iter=1000, verbose=True) \n",
    "clf.fit(X_train, Y_train)\n",
    "test_accuracy = clf.score(X_test, Y_test) \n",
    "train_accuracy = clf.score(X_train, Y_train) \n",
    "print(f\"test accuracy is {test_accuracy}\")\n",
    "print(f\"test accuracy is {train_accuracy}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weight_update(self, delta_list, value_list, learning_rate):\n",
    "\n",
    "\n",
    "        # for i in range(len(self.network) - 1):\n",
    "        #     for l  in range(len(self.network[i])):\n",
    "        #         node = self.network[i][l] \n",
    "        #         for j in range(len(node.downstream_nodes)):\n",
    "        #             # node.downstream_weights[j] += (learning_rate * delta_list[i][j] * value_list[i][j])/self.M \n",
    "        #             update = 0 \n",
    "        #             for k in range(len(delta_list)):\n",
    "        #                 # assert(value_list[k][i][l] != 0)\n",
    "        #                 # assert(delta_list[k][i + 1][j] != 0) \n",
    "                        \n",
    "        #                 update += (delta_list[k][i + 1][j] * value_list[k][i][l])\n",
    "                    \n",
    "        #             update *= learning_rate/self.M\n",
    "        #             update *= learning_rate\n",
    "        #             node.downstream_weights[j] += update \n",
    "        #             # assert(update != 0)\n",
    "        # # print(\"len of delta list was \", len(delta_list))\n",
    "        # # print(\"weight update done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.training_data, self.training_labels =  self.shuffle(training_data, training_labels) \n",
    "#         prev_weights =[]\n",
    "#         weights = []\n",
    "#         for layer in self.network:\n",
    "#             for node in layer:\n",
    "#                 for i in range(len(node.downstream_weights)):\n",
    "#                             weights.append(node.downstream_weights[i])\n",
    "          \n",
    "#         while (True):\n",
    "           \n",
    "#             for i in range(0, len(training_data), self.M):\n",
    "#                 minibatch_data = training_data[i : i + self.M] \n",
    "#                 minibatch_labels = training_labels[i : i + self.M]\n",
    "#                 delta_list = []\n",
    "#                 value_list = []\n",
    "#                 for eg, label in zip(minibatch_data, minibatch_labels):\n",
    "                    \n",
    "#                         self.propagate(eg, label) \n",
    "#                         network_delta, network_values = [],  [] \n",
    "#                         for layer in self.network:\n",
    "#                             layer_deltas, layer_values = [],  []\n",
    "#                             for node in layer:\n",
    "#                                 # layer_deltas = np.append(layer_deltas, node.delta) \n",
    "#                                 layer_deltas.append(node.delta)\n",
    "#                                 # layer_values = np.append(layer_values, node.value)\n",
    "#                                 layer_values.append(node.value)  \n",
    "#                             # print(\"shape of layer_deltas is \", layer_deltas.shape)\n",
    "#                             # network_delta = np.append(network_delta, layer_deltas)\n",
    "#                             network_delta.append(layer_deltas)\n",
    "#                             # network_values = np.append(network_values, layer_values) \n",
    "#                             network_values.append(layer_values)\n",
    "#                         # print(\"shape of network_delta is \", network_delta.shape)\n",
    "#                         # input() \n",
    "#                         # delta_list = np.append(delta_list, network_delta)\n",
    "#                         delta_list.append(network_delta)\n",
    "#                         # value_list = np.append(value_list, network_values)\n",
    "#                         value_list.append(network_values)\n",
    "#                 # print(\"shape of delta_list is \", delta_list.shape)\n",
    "#                 # print(\"shape of value_list is \", value_list.shape) \n",
    "#                 # prev_weights = weights \n",
    "#                 # weights = [] \n",
    "#                 self.weight_update(delta_list, value_list, learning_rate)  \n",
    "#                 # for layer in self.network:\n",
    "#                     # for node in layer:\n",
    "#                         # for i in range(len(node.downstream_weights)):\n",
    "#                             # weights.append(node.downstream_weights[i])\n",
    "#                 # diff = self.get_weight_diff(weights, prev_weights)\n",
    "#                 # print(\"weights updated by \", diff)\n",
    "                \n",
    "#             computed_accuracy, loss = self.compute_accuracy_and_loss(X_train, Y_train) \n",
    "#             # loss = self.compute_loss(training_data, training_labels)\n",
    "#             print(\"accuracy is \", computed_accuracy)\n",
    "#             print(\"loss is \", loss)\n",
    "           \n",
    "#         print(\"training done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
