{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_input, n_neuron):\n",
    "        self.n_input = n_input\n",
    "        self.n_neuron = n_neuron\n",
    "        self.std_dev = np.sqrt(1 / n_input) # Xavier initialization\n",
    "        self.W = np.random.normal(0,self.std_dev,(n_neuron, n_input)) # weight matrix \n",
    "        self.b = np.zeros(n_neuron) \n",
    "        self.z = np.zeros(n_neuron) # z = Wx + b\n",
    "        self.a = np.zeros(n_neuron) # a = f(z) \n",
    "        self.grad_z = np.zeros(n_neuron) \n",
    "        self.grad_a = np.zeros(n_neuron) \n",
    "        self.grad_W = np.zeros((n_neuron, n_input)) \n",
    "        self.grad_b = np.zeros(n_neuron) \n",
    "        \n",
    "        self.next_layer = None\n",
    "        self.prev_layer = None\n",
    "        \n",
    "        self.grad_z_sum = self.grad_z  # will reset to 0 after each minibatch\n",
    "        self.grad_a_sum = self.grad_a\n",
    "        self.grad_W_sum = self.grad_W\n",
    "        self.grad_b_sum = self.grad_b \n",
    "\n",
    "        # if self.prev_layer is none then it is the input layer\n",
    "        # if self.next_layer is none then it is the output layer \n",
    "    \n",
    "    def set_prev_layer(self, layer): # set next layer of the previous layer also \n",
    "        layer.next_layer = self \n",
    "        self.prev_layer = layer \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z = self.W @ x + self.b  \n",
    "        self.a = self.activation(self.z) \n",
    "    \n",
    "    def activation(self, z ):\n",
    "        # logistic sigmoid\n",
    "        # z = min( 500, max(z, -500) )  \n",
    "        # # to prevent overflow \n",
    "        return 1 / (1 + np.exp(-z))  \n",
    "    \n",
    "    def activation_derivative(self, z):\n",
    "        # derivative of logistic sigmoid\n",
    "        val = self.activation(z) \n",
    "        return val * (1 - val)  \n",
    "\n",
    "    def compute_grad_a(self):\n",
    "        if self.next_layer is not None:\n",
    "            self.grad_a = self.next_layer.W.T @ self.next_layer.grad_z  \n",
    "            self.grad_a_sum += self.grad_a \n",
    "        else: # output layer\n",
    "            pass \n",
    "\n",
    "    def compute_grad_z(self):\n",
    "        self.grad_z = self.activation_derivative(self.z) * self.grad_a  # element-wise multiplication \n",
    "        self.grad_z_sum += self.grad_z \n",
    "\n",
    "    def compute_grad_W(self):\n",
    "        if (self.prev_layer is not None):\n",
    "            self.grad_W = np.outer(self.grad_z, self.prev_layer.a)  \n",
    "            self.grad_W_sum += self.grad_W \n",
    "        else: # input layer\n",
    "            pass \n",
    "\n",
    "    def compute_grad_b(self):\n",
    "        self.grad_b = self.grad_z \n",
    "        self.grad_b_sum += self.grad_b \n",
    "    \n",
    "    def backward(self):\n",
    "        self.compute_grad_a()\n",
    "        self.compute_grad_z()\n",
    "        self.compute_grad_W()\n",
    "        self.compute_grad_b() \n",
    "\n",
    "    def weight_update(self, learning_rate, batch_size):\n",
    "        # update parameters \n",
    "        self.W -= learning_rate * self.grad_W_sum / batch_size\n",
    "        self.b -= learning_rate * self.grad_b_sum / batch_size\n",
    "        self.a -= learning_rate * self.grad_a_sum / batch_size \n",
    "        self.z -= learning_rate * self.grad_z_sum / batch_size \n",
    "\n",
    "        self.grad_z_sum = np.zeros(self.n_neuron) \n",
    "        self.grad_a_sum = np.zeros(self.n_neuron) \n",
    "        self.grad_W_sum = np.zeros((self.n_neuron, self.n_input)) \n",
    "        self.grad_b_sum = np.zeros(self.n_neuron)  \n",
    "\n",
    "class InputLayer:\n",
    "    def __init__(self, n_input):\n",
    "        self.n_input = n_input\n",
    "        self.n_neuron = n_input\n",
    "        self.a = np.zeros(n_input) \n",
    "        self.next_layer = None\n",
    "        self.prev_layer = None \n",
    "    \n",
    "    def set_prev_layer(self, layer): # set next layer of the previous layer also \n",
    "        layer.next_layer = self \n",
    "        self.prev_layer = layer \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.a = x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 71)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:71\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(f\"epoch {epoch} completed\")\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class OutputLayer:\n",
    "    def __init__(self, n_input, n_classes):\n",
    "        self.n_input = n_input\n",
    "        self.n_classes = n_classes\n",
    "        self.std_dev = np.sqrt(1 / n_input) # Xavier initialization\n",
    "        self.W = np.random.normal(0,self.std_dev,(n_classes, n_input)) # weight matrix \n",
    "        self.b = np.zeros(n_classes) \n",
    "        self.z = np.zeros(n_classes) # z = Wx + b\n",
    "        self.grad_z = np.zeros(n_classes) \n",
    "        self.grad_W = np.zeros((n_classes, n_input)) \n",
    "        self.grad_b = np.zeros(n_classes) \n",
    "\n",
    "        self.grad_z_sum = self.grad_z  # will reset to 0 after each minibatch\n",
    "        self.grad_W_sum = self.grad_W\n",
    "        self.grad_b_sum = self.grad_b\n",
    "        \n",
    "        self.batch_loss = 0 \n",
    "        self.phi = np.zeros(n_classes) # softmax(z) \n",
    "        self.next_layer = None\n",
    "        self.prev_layer = None\n",
    "        # if self.prev_layer is none then it is the input layer\n",
    "        # if self.next_layer is none then it is the output layer \n",
    "    \n",
    "    def set_prev_layer(self, layer): # set next layer of the previous layer also \n",
    "        layer.next_layer = self \n",
    "        self.prev_layer = layer \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z = self.W @ x + self.b  \n",
    "        self.phi = self.softmax(self.z) \n",
    "    \n",
    "    def softmax(self, z):\n",
    "        return np.exp(z) / np.sum(np.exp(z)) \n",
    "\n",
    "    def compute_loss(self, y):\n",
    "        self.batch_loss -= np.log(self.phi[y])  \n",
    "    \n",
    "    def compute_grad_z(self, y):\n",
    "        self.grad_z = self.phi \n",
    "        self.grad_z[y] -= 1\n",
    "        self.grad_z_sum += self.grad_z \n",
    "    \n",
    "    def compute_grad_W(self):\n",
    "        if (self.prev_layer is not None):\n",
    "            self.grad_W = np.outer(self.grad_z, self.prev_layer.a) \n",
    "            self.grad_W_sum += self.grad_W \n",
    "        else: # input layer\n",
    "            pass \n",
    "    \n",
    "    def compute_grad_b(self):\n",
    "        self.grad_b = self.grad_z\n",
    "        self.grad_b_sum += self.grad_b\n",
    "    \n",
    "    def backward(self, y):\n",
    "        self.compute_grad_z(y) \n",
    "        self.compute_grad_W()\n",
    "        self.compute_grad_b() \n",
    "    \n",
    "    def weight_update(self, learning_rate, batch_size):\n",
    "        # update parameters \n",
    "        self.W -= learning_rate * self.grad_W_sum / batch_size\n",
    "        self.b -= learning_rate * self.grad_b_sum / batch_size\n",
    "        self.z -= learning_rate * self.grad_z_sum / batch_size \n",
    "        self.loss = self.batch_loss\n",
    "\n",
    "        self.grad_W_sum = np.zeros((self.n_classes, self.n_input)) \n",
    "        self.grad_b_sum = np.zeros(self.n_classes)\n",
    "        self.grad_z_sum = np.zeros(self.n_classes) \n",
    "        self.batch_loss = 0 \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self, minibatch_size, no_of_features, hidden_layers, no_of_classes):\n",
    "        self.M = minibatch_size \n",
    "        self.n = no_of_features\n",
    "        self.hidden_layers = hidden_layers \n",
    "        self.K = no_of_classes \n",
    "        self.training_data = None # should be already processed\n",
    "        self.training_labels = None # should be already processed\n",
    "        self.network = np.array([]) # array of layers \n",
    "        \n",
    "    def make_network(self):\n",
    "        input_layer = InputLayer(self.n) \n",
    "        self.network = np.append(self.network, input_layer) \n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            new_layer = Layer(self.network[-1].n_neuron, self.hidden_layers[i]) \n",
    "            new_layer.set_prev_layer(self.network[-1]) \n",
    "            self.network = np.append(self.network, new_layer) \n",
    "        \n",
    "        output_layer = OutputLayer(self.network[-1].n_neuron, self.K) \n",
    "        output_layer.set_prev_layer(self.network[-1]) \n",
    "        self.network = np.append(self.network, output_layer) \n",
    "        print(f\"printing network: {self.network}\") \n",
    "\n",
    "    def forward_prop(self, x):\n",
    "        self.network[0].forward(x) \n",
    "        for i in range(1, len(self.network)):\n",
    "            self.network[i].forward(self.network[i-1].a) \n",
    "    \n",
    "    def backward_prop(self, y):\n",
    "        self.network[-1].compute_loss(y) \n",
    "        self.network[-1].backward(y) \n",
    "        for i in range(len(self.network)-2, 0, -1):\n",
    "            self.network[i].backward()\n",
    "    \n",
    "    def weight_update(self, learning_rate):\n",
    "        for i in range(1, len(self.network)):\n",
    "            self.network[i].weight_update(learning_rate, self.M) \n",
    "        \n",
    "    \n",
    "    def shuffle(self, training_data, training_labels):\n",
    "        idxs = np.array([i for i in range(len(training_data))]) \n",
    "        idx = np.random.shuffle(idxs)\n",
    "        # print(\"shuffled idx is \", idxs) \n",
    "        training_data = training_data[idxs]\n",
    "        training_labels = training_labels[idxs]\n",
    "        return (training_data, training_labels)\n",
    "\n",
    "    def train(self, training_eg, labels, learning_rate):\n",
    "        self.training_eg, self.training_labels = self.shuffle(training_eg, labels)  \n",
    "        # self.validation_split() \n",
    "        batches = len(self.training_eg) // self.M \n",
    "        self.training_eg = self.training_eg[:batches * self.M] \n",
    "        self.training_labels = self.training_labels[:batches * self.M] \n",
    "        max_epochs = 1000\n",
    "        epoch, cnt = 0, 0  \n",
    "        tolerance, max_n_of_iter= 0.0001, 5 \n",
    "        loss, prev_loss = 0,0\n",
    "        n_of_iter = 0 \n",
    "        while(n_of_iter < max_n_of_iter and epoch < max_epochs):\n",
    "            for i in range(1, len(self.training_eg) + 1):\n",
    "                self.forward_prop(self.training_eg[i - 1]) \n",
    "                self.backward_prop(int(self.training_labels[i - 1])) \n",
    "                cnt += 1 \n",
    "                if (cnt % self.M == 0):\n",
    "                    self.weight_update(learning_rate)  \n",
    "                    cnt = 0 \n",
    "            print(f\"epoch {epoch} completed\") \n",
    "            loss = self.network[-1].loss / self.M \n",
    "            print(f\"loss is {loss}\") \n",
    "            epoch += 1 \n",
    "            if (abs(loss - prev_loss) < tolerance):\n",
    "                n_of_iter += 1\n",
    "            else : \n",
    "                n_of_iter = 0 \n",
    "\n",
    "    def predict(self, x):\n",
    "        self.forward_prop(x) \n",
    "        return np.argmax(self.network[-1].phi) \n",
    "\n",
    "    def compute_predictions(self, egs):\n",
    "        correct = 0 \n",
    "        predictions = [] \n",
    "        for i in range(len(egs)): predictions.append(self.predict(egs[i])) \n",
    "        return predictions\n",
    "\n",
    "    def validation_split(self):\n",
    "        validation_split_percent = 0.25 \n",
    "        split = int(validation_split_percent * len(self.training_data)) \n",
    "        validation_eg = self.training_data[:split]\n",
    "        self.training_eg = self.training_data[split:] \n",
    "        self.validation_eg = validation_eg \n",
    "        self.training_labels = self.training_labels[split:] \n",
    "        self.validation_labels = self.training_labels[:split] \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(input_path, output_path):\n",
    "    x = np.load(input_path)\n",
    "    y = np.load(output_path)\n",
    "\n",
    "    y = y.astype('float')\n",
    "    x = x.astype('float')\n",
    "\n",
    "    #normalize x:\n",
    "    x = 2*(0.5 - x/255)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_input(\"../data/part b/x_train.npy\", \"../data/part b/y_train.npy\") \n",
    "X_test, Y_test = get_input(\"../data/part b/x_test.npy\", \"../data/part b/y_test.npy\" )\n",
    "print(X_test.shape) \n",
    "print(Y_test.shape)\n",
    "Y_train = Y_train - 1 \n",
    "Y_test = Y_test - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimenting for part b \n",
    "# hidden_layers = np.array([[1], [5], [10], [50], [100]])\n",
    "hidden_layers = [] \n",
    "nn_list = [] \n",
    "for hidden_layer in hidden_layers:\n",
    "    nn = Neural_Network(32, 1024 , hidden_layer, 5) \n",
    "    nn.make_network() \n",
    "    nn.train(X_train, Y_train, 0.01) \n",
    "    train_pred = nn.compute_predictions(X_train) \n",
    "    test_pred = nn.compute_predictions(X_test) \n",
    "    report_test = classification_report( Y_test, test_pred)  \n",
    "    report_train = classification_report(Y_train, train_pred) \n",
    "    to_print = f\"hidden_layer is {hidden_layer}\\n\"\n",
    "    to_print += \"printing train report\\n\"\n",
    "    to_print += \"\\n\" + report_train + \"\\n\"\n",
    "    to_print += \"printing test report\\n\"\n",
    "    to_print += \"\\n\" + report_test + \"\\n\"\n",
    "    print() \n",
    "    print()  \n",
    "    with open(\"logfile\", \"a\") as file:\n",
    "        file.write(to_print)\n",
    "    nn_list.append(nn) \n",
    "    # print(f\"hidden layer is {hidden_layer}\")\n",
    "    # print(report)\n",
    "    # print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot by reading from logfile\n",
    "def read_logfile():\n",
    "    hidden_layer_sizes = [] \n",
    "    train_recall, test_recall = [[] for i in range(5)], [[] for i in range(5)] \n",
    "    train_precision, test_precision = [[] for i in range(5)], [[] for i in range(5)] \n",
    "    train_f1, test_f1 = [[] for i in range(5)], [[] for i in range(5)] \n",
    "    train_acc, test_acc= [], [] \n",
    "    in_train, in_test = False, False \n",
    "    with open(\"logfile\", \"r\") as file:\n",
    "        report = \"\" \n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line = line.lstrip() \n",
    "            if (line.startswith(\"hidden_layer\")):\n",
    "                size = re.findall(r'\\d+', line) \n",
    "                hidden_layer_sizes.append(int(size[0])) \n",
    "            elif (line.startswith(\"printing train report\")):\n",
    "                in_train = True\n",
    "                in_test = False \n",
    "            elif (line.startswith(\"printing test report\")):\n",
    "                in_test = True \n",
    "                in_train = False\n",
    "\n",
    "            elif (line.startswith(\"accuracy\")):\n",
    "                if (in_train):\n",
    "                    acc = re.findall(r'\\d+\\.\\d+', line)[0]\n",
    "                    train_acc.append(float(acc))  \n",
    "                elif (in_test):\n",
    "                    acc = re.findall(r'\\d+\\.\\d+', line)[0] \n",
    "                    test_acc.append(float(acc)) \n",
    "\n",
    "            elif (line.startswith(\"macro avg\") or line.startswith(\"weighted avg\")):\n",
    "                continue \n",
    "            \n",
    "            else:\n",
    "                vals = re.findall(r'\\d+\\.\\d+', line)\n",
    "                if (len(vals) == 0): continue\n",
    "                label = int(float(vals[0]))\n",
    "                if (in_train):\n",
    "                    train_recall[label].append(float(vals[2])) \n",
    "                    train_precision[label].append(float(vals[1])) \n",
    "                    train_f1[label].append(float(vals[3])) \n",
    "                elif (in_test):\n",
    "                    test_recall[label].append(float(vals[2])) \n",
    "                    test_precision[label].append(float(vals[1])) \n",
    "                    test_f1[label].append(float(vals[3])) \n",
    "    return (hidden_layer_sizes, train_recall, train_precision, train_f1, test_recall, test_precision, test_f1, train_acc, test_acc)\n",
    "    \n",
    "\n",
    "def plot_accuracies(y_lines, x, labels, x_label, y_label, name):\n",
    "\n",
    "    fig,axis = plt.subplots()\n",
    "    for y_line,label  in zip(y_lines, labels):\n",
    "        axis.plot(x, y_line, label = label) \n",
    "    axis.set_xlabel(x_label)\n",
    "    axis.set_ylabel(y_label)\n",
    "    axis.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{name}.pdf\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers, train_recall, train_precision, train_f1, test_recall, test_precision, test_f1, train_acc, test_acc = read_logfile() \n",
    "\n",
    "# plotting train and test accuracies \n",
    "plot_accuracies([train_acc, test_acc], hidden_layers, [\"train accuracy\", \"test accuracy\"], \"hidden layer size\", \"accuracy\", \"partb_accuracies\") \n",
    "train_f1 = np.array(train_f1) \n",
    "train_f1_avg = np.mean(train_f1, axis = 0) \n",
    "test_f1 = np.array(test_f1)\n",
    "test_f1_avg = np.mean(test_f1, axis = 0)\n",
    "print(train_f1_avg)\n",
    "print(train_f1) \n",
    "plot_accuracies([train_f1_avg, test_f1_avg], hidden_layers, [\"train f1\", \"test f1\"], \"hidden layer size\", \"f1 score\", \"partb_f1\")\n",
    "\n",
    "# for i in range(1, 6):\n",
    "#     for j in range(len(hidden_layers)):\n",
    "#         print(f\"for hidden layer {hidden_layers[j]}\") \n",
    "#         print(f\"class {i} train precision is {train_precision[i-1]}\") \n",
    "#         print(f\"class {i} test precision is {test_precision[i-1]}\")\n",
    "#         print(f\"class {i} train recall is {train_recall[i-1]}\")\n",
    "#         print(f\"class {i} test recall is {test_recall[i-1]}\")\n",
    "#         print(f\"class {i} train f1 is {train_f1[i-1]}\")\n",
    "#         print(f\"class {i} test f1 is {test_f1[i-1]}\")\n",
    "#         print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "We see that f1 score is a good measure of generalization accuracy, since plots of test f1 score and test accuracy follow the same trend. Same holds for test data. \n",
    "\n",
    "In general, increasing the hidden layer size increases the test accuracy and test f1 score. \n",
    "\n",
    "We also observe that increasing the hidden layer size doesn't necessariliy increase f1 score. This could be due to overfitting of the model and loss of generalization accuracy with overly complex model . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimenting for part b \n",
    "hidden_layers = [[512], [512, 256], [512,256, 128], [512, 256, 128, 64]] \n",
    "c_nn_list = [] \n",
    "for hidden_layer in hidden_layers:\n",
    "    nn = Neural_Network(32, 1024 , hidden_layer, 5) \n",
    "    nn.make_network() \n",
    "    nn.train(X_train, Y_train, 0.01) \n",
    "    train_pred = nn.compute_predictions(X_train) \n",
    "    test_pred = nn.compute_predictions(X_test) \n",
    "    report_test = classification_report( Y_test, test_pred)  \n",
    "    report_train = classification_report(Y_train, train_pred) \n",
    "    to_print = f\"hidden_layer is {hidden_layer}\\n\"\n",
    "    to_print += \"printing train report\\n\"\n",
    "    to_print += \"\\n\" + report_train + \"\\n\"\n",
    "    to_print += \"printing test report\\n\"\n",
    "    to_print += \"\\n\" + report_test + \"\\n\"\n",
    "    print() \n",
    "    print()  \n",
    "    with open(\"logfile_part_c\", \"a\") as file:\n",
    "        file.write(to_print)\n",
    "    c_nn_list.append(nn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveNeuralNetwork(Neural_Network):\n",
    "    def __init__(self, minibatch_size, no_of_features, hidden_layers, no_of_classes):\n",
    "        super().__init__(minibatch_size, no_of_features, hidden_layers, no_of_classes) \n",
    "        self.seed = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 500 \n",
    "print(np.exp(x)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
