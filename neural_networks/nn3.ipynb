{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_input, n_neuron):\n",
    "        self.n_input = n_input\n",
    "        self.n_neuron = n_neuron\n",
    "        self.std_dev = np.sqrt(1 / n_input) # Xavier initialization\n",
    "        self.W = np.random.normal(0,self.std_dev,(n_neuron, n_input)) # weight matrix \n",
    "        self.b = np.zeros(n_neuron) \n",
    "        self.z = np.zeros(n_neuron) # z = Wx + b, here init shape is for 1 eg only, but doesnt matter\n",
    "        self.a = np.zeros(n_neuron) # a = f(z) , same as above \n",
    "        self.grad_z = np.zeros(n_neuron) \n",
    "        self.grad_a = np.zeros(n_neuron) \n",
    "        self.grad_W = np.zeros((n_neuron, n_input)) \n",
    "        self.grad_b = np.zeros(n_neuron) \n",
    "        \n",
    "        self.next_layer = None\n",
    "        self.prev_layer = None\n",
    "        \n",
    "        self.grad_z_sum = self.grad_z  # will reset to 0 after each minibatch\n",
    "        self.grad_a_sum = self.grad_a\n",
    "        self.grad_W_sum = self.grad_W\n",
    "        self.grad_b_sum = self.grad_b \n",
    "\n",
    "        # if self.prev_layer is none then it is the input layer\n",
    "        # if self.next_layer is none then it is the output layer \n",
    "    \n",
    "    def set_prev_layer(self, layer): # set next layer of the previous layer also \n",
    "        layer.next_layer = self \n",
    "        self.prev_layer = layer \n",
    "\n",
    "    def forward(self, x):\n",
    "        temp = self.b.reshape(-1, 1) \n",
    "        self.z = self.W @ x + temp   \n",
    "        self.a = self.activation(self.z) \n",
    "    \n",
    "    def activation(self, z):\n",
    "        # logistic sigmoid\n",
    "        return 1 / (1 + np.exp(-z)) \n",
    "    \n",
    "    def activation_derivative(self, z):\n",
    "        # derivative of logistic sigmoid\n",
    "        val = self.activation(z) \n",
    "        return val * (1 - val)  \n",
    "\n",
    "    def compute_grad_a(self):\n",
    "        if self.next_layer is not None:\n",
    "            self.grad_a = self.next_layer.W.T @ self.next_layer.grad_z  \n",
    "            # self.grad_a_sum = np.sum(self.grad_a, axis=1) \n",
    "        else: # output layer\n",
    "            pass \n",
    "\n",
    "    def compute_grad_z(self):\n",
    "        self.grad_z = self.activation_derivative(self.z) * self.grad_a  # element-wise multiplication \n",
    "        # self.grad_z_sum = np.sum(self.grad_z, axis=1)  \n",
    "\n",
    "    def compute_grad_W(self):\n",
    "        if (self.prev_layer is not None):\n",
    "            # self.grad_W = np.outer(self.grad_z, self.prev_layer.a)  \n",
    "            temp1 = self.grad_z.T \n",
    "            temp2 = self.prev_layer.a.T \n",
    "            self.grad_W_sum = np.sum(np.einsum('ij,ik->ijk', temp1, temp2), axis=0)   \n",
    "        else: # input layer\n",
    "            pass \n",
    "\n",
    "    def compute_grad_b(self):\n",
    "        self.grad_b = self.grad_z \n",
    "        self.grad_b_sum = np.sum(self.grad_b, axis=1)   \n",
    "    \n",
    "    def backward(self):\n",
    "        self.compute_grad_a()\n",
    "        self.compute_grad_z()\n",
    "        self.compute_grad_W()\n",
    "        self.compute_grad_b() \n",
    "\n",
    "    def weight_update(self, learning_rate, batch_size):\n",
    "        # update parameters \n",
    "        # print(self.grad_W_sum.shape, self.grad_b_sum.shape, self.W.shape, self.b.shape)\n",
    "        # print(self.grad_z.shape, self.prev_layer.a.shape) \n",
    "        self.W -= learning_rate * self.grad_W_sum / batch_size\n",
    "        self.b -= learning_rate * self.grad_b_sum / batch_size\n",
    "        # self.a -= learning_rate * self.grad_a_sum / batch_size \n",
    "        # self.z -= learning_rate * self.grad_z_sum / batch_size \n",
    "\n",
    "        # self.grad_z_sum = np.zeros(self.n_neuron) \n",
    "        # self.grad_a_sum = np.zeros(self.n_neuron) \n",
    "        self.grad_W_sum = np.zeros((self.n_neuron, self.n_input)) \n",
    "        self.grad_b_sum = np.zeros(self.n_neuron)  \n",
    "\n",
    "class InputLayer:\n",
    "    def __init__(self, n_input):\n",
    "        self.n_input = n_input\n",
    "        self.n_neuron = n_input\n",
    "        self.a = np.zeros(n_input) \n",
    "        self.next_layer = None\n",
    "        self.prev_layer = None \n",
    "    \n",
    "    def set_prev_layer(self, layer): # set next layer of the previous layer also \n",
    "        layer.next_layer = self \n",
    "        self.prev_layer = layer \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.a = x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer:\n",
    "    def __init__(self, n_input, n_classes):\n",
    "        self.n_input = n_input\n",
    "        self.n_classes = n_classes\n",
    "        self.std_dev = np.sqrt(1 / n_input) # Xavier initialization\n",
    "        self.W = np.random.normal(0,self.std_dev,(n_classes, n_input)) # weight matrix \n",
    "        self.b = np.zeros(n_classes) \n",
    "        self.z = np.zeros(n_classes) # z = Wx + b\n",
    "        self.grad_z = np.zeros(n_classes) \n",
    "        self.grad_W = np.zeros((n_classes, n_input)) \n",
    "        self.grad_b = np.zeros(n_classes) \n",
    "\n",
    "        self.grad_z_sum = self.grad_z  # will reset to 0 after each minibatch\n",
    "        self.grad_W_sum = self.grad_W\n",
    "        self.grad_b_sum = self.grad_b\n",
    "        \n",
    "        self.batch_loss = 0 \n",
    "        self.phi = np.zeros(n_classes) # softmax(z) \n",
    "        self.next_layer = None\n",
    "        self.prev_layer = None\n",
    "        # if self.prev_layer is none then it is the input layer\n",
    "        # if self.next_layer is none then it is the output layer \n",
    "    \n",
    "    def set_prev_layer(self, layer): # set next layer of the previous layer also \n",
    "        layer.next_layer = self \n",
    "        self.prev_layer = layer \n",
    "\n",
    "    def forward(self, x):\n",
    "        temp = self.b.reshape(-1, 1) \n",
    "        self.z = self.W @ x + temp\n",
    "        # self.phi = self.softmax(self.z) \n",
    "        self.phi = np.apply_along_axis(self.softmax, axis=0 , arr=self.z) \n",
    "    \n",
    "    def softmax(self, z):\n",
    "        return np.exp(z) / np.sum(np.exp(z)) \n",
    "\n",
    "    def compute_loss(self, y):\n",
    "        self.batch_loss = 0 \n",
    "        # print(self.phi.shape)\n",
    "        for i in range(y.shape[0]):\n",
    "            self.batch_loss -= np.log(self.phi[int(y[i]) , i])  \n",
    "        # self.batch_loss -= np.log(self.phi[y])   \n",
    "    \n",
    "\n",
    "    def compute_grad_z(self, y):\n",
    "        def temp(index):\n",
    "            self.phi[int(y[index]) , index] -= 1\n",
    "            return 1   \n",
    "\n",
    "        self.grad_z = self.phi \n",
    "        # self.grad_z[y] -= 1 \n",
    "        vectorize_temp = np.vectorize(temp)\n",
    "        vectorize_temp(np.arange(y.shape[0]))   \n",
    "        # self.grad_z_sum += self.grad_z \n",
    "    \n",
    "    def compute_grad_W(self):\n",
    "        if (self.prev_layer is not None):\n",
    "            # self.grad_W = np.outer(self.grad_z, self.prev_layer.a)  \n",
    "            temp1 = self.grad_z.T \n",
    "            temp2 = self.prev_layer.a.T \n",
    "            self.grad_W_sum = np.sum(np.einsum('ij,ik->ijk', temp1, temp2), axis=0)   \n",
    "        else: # input layer\n",
    "            pass \n",
    "\n",
    "    def compute_grad_b(self):\n",
    "        self.grad_b = self.grad_z \n",
    "        self.grad_b_sum = np.sum(self.grad_b, axis=1)   \n",
    "    \n",
    "    def backward(self, y):\n",
    "        self.compute_grad_z(y) \n",
    "        self.compute_grad_W()\n",
    "        self.compute_grad_b() \n",
    "    \n",
    "    def weight_update(self, learning_rate, batch_size):\n",
    "        # update parameters \n",
    "        # print(self.grad_W_sum.shape, self.grad_b_sum.shape, self.W.shape, self.b.shape)\n",
    "        self.W -= learning_rate * self.grad_W_sum / batch_size\n",
    "        self.b -= learning_rate * self.grad_b_sum / batch_size\n",
    "        # self.z -= learning_rate * self.grad_z_sum / batch_size \n",
    "        self.loss = self.batch_loss\n",
    "\n",
    "        self.grad_W_sum = np.zeros((self.n_classes, self.n_input)) \n",
    "        self.grad_b_sum = np.zeros(self.n_classes)\n",
    "        # self.grad_z_sum = np.zeros(self.n_classes) \n",
    "        self.batch_loss = 0 \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    def __init__(self, minibatch_size, no_of_features, hidden_layers, no_of_classes):\n",
    "        self.M = minibatch_size \n",
    "        self.n = no_of_features\n",
    "        self.hidden_layers = hidden_layers \n",
    "        self.K = no_of_classes \n",
    "        self.training_data = None # should be already processed\n",
    "        self.training_labels = None # should be already processed\n",
    "        self.network = np.array([]) # array of layers \n",
    "        \n",
    "    def make_network(self):\n",
    "        input_layer = InputLayer(self.n) \n",
    "        self.network = np.append(self.network, input_layer) \n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            new_layer = Layer(self.network[-1].n_neuron, self.hidden_layers[i]) \n",
    "            new_layer.set_prev_layer(self.network[-1]) \n",
    "            self.network = np.append(self.network, new_layer) \n",
    "        \n",
    "        output_layer = OutputLayer(self.network[-1].n_neuron, self.K) \n",
    "        output_layer.set_prev_layer(self.network[-1]) \n",
    "        self.network = np.append(self.network, output_layer) \n",
    "        print(f\"printing network: {self.network}\") \n",
    "\n",
    "    def forward_prop(self, x):\n",
    "        self.network[0].forward(x) \n",
    "        for i in range(1, len(self.network)):\n",
    "            self.network[i].forward(self.network[i-1].a) \n",
    "    \n",
    "    def backward_prop(self, y):\n",
    "        self.network[-1].compute_loss(y) \n",
    "        self.network[-1].backward(y) \n",
    "        for i in range(len(self.network)-2, 0, -1):\n",
    "            self.network[i].backward()\n",
    "    \n",
    "    def weight_update(self, learning_rate):\n",
    "        for i in range(1, len(self.network)):\n",
    "            self.network[i].weight_update(learning_rate, self.M) \n",
    "        \n",
    "    \n",
    "    def shuffle(self, training_data, training_labels):\n",
    "        idxs = np.array([i for i in range(len(training_data))]) \n",
    "        idx = np.random.shuffle(idxs)\n",
    "        # print(\"shuffled idx is \", idxs) \n",
    "        training_data = training_data[idxs]\n",
    "        training_labels = training_labels[idxs]\n",
    "        return (training_data, training_labels)\n",
    "\n",
    "    def train(self, training_eg, labels, learning_rate):\n",
    "        self.training_eg, self.training_labels = self.shuffle(training_eg, labels)  \n",
    "        # self.validation_split() \n",
    "        batches = len(self.training_eg) // self.M \n",
    "        self.training_eg = self.training_eg[:batches * self.M] \n",
    "        self.training_labels = self.training_labels[:batches * self.M] \n",
    "        max_epochs = 1000\n",
    "        epoch, cnt = 0, 0  \n",
    "        tolerance, max_n_of_iter= 0.0001, 5 \n",
    "        loss, prev_loss = 0,0\n",
    "        n_of_iter = 0 \n",
    "\n",
    "        # split the training data into batches\n",
    "        batches_list = np.split(self.training_eg, batches) \n",
    "        labels_list = np.split(self.training_labels, batches) \n",
    "        for i in range(len(batches_list)): batches_list[i] = batches_list[i].T \n",
    "        \n",
    "        while(n_of_iter < max_n_of_iter and epoch < max_epochs):\n",
    "            \n",
    "            for batch, labels in zip(batches_list, labels_list):\n",
    "                self.forward_prop(batch) \n",
    "                self.backward_prop(labels) \n",
    "                self.weight_update(learning_rate) \n",
    "            print(f\"epoch {epoch} completed\") \n",
    "            loss = self.network[-1].loss / self.M \n",
    "            print(f\"loss is {loss}\") \n",
    "            epoch += 1 \n",
    "            if (abs(loss - prev_loss) < tolerance):\n",
    "                n_of_iter += 1\n",
    "            else : \n",
    "                n_of_iter = 0 \n",
    "\n",
    "    def predict(self, x):\n",
    "        x = x.T \n",
    "        self.forward_prop(x)\n",
    "        def temp(z) : return np.argmax(z) \n",
    "        # vectorize_temp = np.vectorize(temp) \n",
    "        predictions = np.apply_along_axis(temp, axis=0, arr=self.network[-1].phi)   \n",
    "        return predictions\n",
    "        # return np.argmax(self.network[-1].phi) \n",
    "\n",
    "\n",
    "    def compute_predictions(self, x):\n",
    "        correct = 0 \n",
    "        # predictions = [] \n",
    "        # for i in range(len(egs)): predictions.append(self.predict(egs[i]))  \n",
    "\n",
    "        x = x.T \n",
    "        self.forward_prop(x)\n",
    "        def temp(z) : return np.argmax(z) \n",
    "        predictions = np.apply_along_axis(temp, axis=0, arr=self.network[-1].phi)   \n",
    "        return predictions\n",
    "        \n",
    "    \n",
    "\n",
    "    def validation_split(self):\n",
    "        validation_split_percent = 0.25 \n",
    "        split = int(validation_split_percent * len(self.training_data)) \n",
    "        validation_eg = self.training_data[:split]\n",
    "        self.training_eg = self.training_data[split:] \n",
    "        self.validation_eg = validation_eg \n",
    "        self.training_labels = self.training_labels[split:] \n",
    "        self.validation_labels = self.training_labels[:split] \n",
    "        return  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(input_path, output_path):\n",
    "    x = np.load(input_path)\n",
    "    y = np.load(output_path)\n",
    "\n",
    "    y = y.astype('float')\n",
    "    x = x.astype('float')\n",
    "\n",
    "    #normalize x:\n",
    "    x = 2*(0.5 - x/255)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1024)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = get_input(\"../data/part b/x_train.npy\", \"../data/part b/y_train.npy\") \n",
    "X_test, Y_test = get_input(\"../data/part b/x_test.npy\", \"../data/part b/y_test.npy\" )\n",
    "print(X_test.shape) \n",
    "print(Y_test.shape)\n",
    "Y_train = Y_train - 1 \n",
    "Y_test = Y_test - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing network: [<__main__.InputLayer object at 0x000002A80FBEF250>\n",
      " <__main__.Layer object at 0x000002A80FBF1490>\n",
      " <__main__.OutputLayer object at 0x000002A80FA09B90>]\n",
      "epoch 0 completed\n",
      "loss is 1.6053221628812984\n",
      "epoch 1 completed\n",
      "loss is 1.5869457768164505\n",
      "epoch 2 completed\n",
      "loss is 1.5477479746706866\n",
      "epoch 3 completed\n",
      "loss is 1.506939523349672\n",
      "epoch 4 completed\n",
      "loss is 1.469797224698475\n",
      "epoch 5 completed\n",
      "loss is 1.436546764984929\n",
      "epoch 6 completed\n",
      "loss is 1.4067504325499232\n",
      "epoch 7 completed\n",
      "loss is 1.380026490118672\n",
      "epoch 8 completed\n",
      "loss is 1.356036468198142\n",
      "epoch 9 completed\n",
      "loss is 1.334454682815077\n",
      "epoch 10 completed\n",
      "loss is 1.3149698296133994\n",
      "epoch 11 completed\n",
      "loss is 1.2972951508652888\n",
      "epoch 12 completed\n",
      "loss is 1.2811757907929966\n",
      "epoch 13 completed\n",
      "loss is 1.2663913297084404\n",
      "epoch 14 completed\n",
      "loss is 1.2527546510963976\n",
      "epoch 15 completed\n",
      "loss is 1.240108746524663\n",
      "epoch 16 completed\n",
      "loss is 1.2283226934969111\n",
      "epoch 17 completed\n",
      "loss is 1.217287567781828\n",
      "epoch 18 completed\n",
      "loss is 1.2069126852173548\n",
      "epoch 19 completed\n",
      "loss is 1.1971223340590198\n",
      "epoch 20 completed\n",
      "loss is 1.1878530279802029\n",
      "epoch 21 completed\n",
      "loss is 1.1790512453699489\n",
      "epoch 22 completed\n",
      "loss is 1.170671594258905\n",
      "epoch 23 completed\n",
      "loss is 1.1626753357671082\n",
      "epoch 24 completed\n",
      "loss is 1.1550292019654913\n",
      "epoch 25 completed\n",
      "loss is 1.1477044510132222\n",
      "epoch 26 completed\n",
      "loss is 1.1406761106972638\n",
      "epoch 27 completed\n",
      "loss is 1.1339223697924852\n",
      "epoch 28 completed\n",
      "loss is 1.1274240843842966\n",
      "epoch 29 completed\n",
      "loss is 1.1211643731686158\n",
      "epoch 30 completed\n",
      "loss is 1.1151282816426786\n",
      "epoch 31 completed\n",
      "loss is 1.1093024999967935\n",
      "epoch 32 completed\n",
      "loss is 1.1036751234493873\n",
      "epoch 33 completed\n",
      "loss is 1.0982354468201334\n",
      "epoch 34 completed\n",
      "loss is 1.0929737874227248\n",
      "epoch 35 completed\n",
      "loss is 1.0878813320083414\n",
      "epoch 36 completed\n",
      "loss is 1.0829500046325289\n",
      "epoch 37 completed\n",
      "loss is 1.0781723530723544\n",
      "epoch 38 completed\n",
      "loss is 1.073541451892813\n",
      "epoch 39 completed\n",
      "loss is 1.0690508205385154\n",
      "epoch 40 completed\n",
      "loss is 1.064694354977625\n",
      "epoch 41 completed\n",
      "loss is 1.0604662715015283\n",
      "epoch 42 completed\n",
      "loss is 1.05636106132324\n",
      "epoch 43 completed\n",
      "loss is 1.052373454645507\n",
      "epoch 44 completed\n",
      "loss is 1.048498392902205\n",
      "epoch 45 completed\n",
      "loss is 1.0447310079231882\n",
      "epoch 46 completed\n",
      "loss is 1.0410666068370806\n",
      "epoch 47 completed\n",
      "loss is 1.037500661608885\n",
      "epoch 48 completed\n",
      "loss is 1.0340288022072153\n",
      "epoch 49 completed\n",
      "loss is 1.0306468125054584\n",
      "epoch 50 completed\n",
      "loss is 1.027350628137438\n",
      "epoch 51 completed\n",
      "loss is 1.0241363356462636\n",
      "epoch 52 completed\n",
      "loss is 1.0210001723805082\n",
      "epoch 53 completed\n",
      "loss is 1.0179385267009284\n",
      "epoch 54 completed\n",
      "loss is 1.0149479381607138\n",
      "epoch 55 completed\n",
      "loss is 1.0120250974109741\n",
      "epoch 56 completed\n",
      "loss is 1.0091668456597906\n",
      "epoch 57 completed\n",
      "loss is 1.0063701735775616\n",
      "epoch 58 completed\n",
      "loss is 1.003632219593988\n",
      "epoch 59 completed\n",
      "loss is 1.0009502675737008\n",
      "epoch 60 completed\n",
      "loss is 0.9983217438894312\n",
      "epoch 61 completed\n",
      "loss is 0.9957442139349457\n",
      "epoch 62 completed\n",
      "loss is 0.9932153781360822\n",
      "epoch 63 completed\n",
      "loss is 0.9907330675283255\n",
      "epoch 64 completed\n",
      "loss is 0.9882952389746142\n",
      "epoch 65 completed\n",
      "loss is 0.9858999700985028\n",
      "epoch 66 completed\n",
      "loss is 0.983545454006323\n",
      "epoch 67 completed\n",
      "loss is 0.9812299938683275\n",
      "epoch 68 completed\n",
      "loss is 0.9789519974236368\n",
      "epoch 69 completed\n",
      "loss is 0.9767099714676543\n",
      "epoch 70 completed\n",
      "loss is 0.9745025163739052\n",
      "epoch 71 completed\n",
      "loss is 0.9723283206953071\n",
      "epoch 72 completed\n",
      "loss is 0.9701861558830117\n",
      "epoch 73 completed\n",
      "loss is 0.9680748711542966\n",
      "epoch 74 completed\n",
      "loss is 0.9659933885347465\n",
      "epoch 75 completed\n",
      "loss is 0.9639406980942054\n",
      "epoch 76 completed\n",
      "loss is 0.9619158533907629\n",
      "epoch 77 completed\n",
      "loss is 0.9599179671324202\n",
      "epoch 78 completed\n",
      "loss is 0.9579462070620444\n",
      "epoch 79 completed\n",
      "loss is 0.9559997920677444\n",
      "epoch 80 completed\n",
      "loss is 0.9540779885179131\n",
      "epoch 81 completed\n",
      "loss is 0.9521801068177443\n",
      "epoch 82 completed\n",
      "loss is 0.9503054981821224\n",
      "epoch 83 completed\n",
      "loss is 0.9484535516182482\n",
      "epoch 84 completed\n",
      "loss is 0.946623691110217\n",
      "epoch 85 completed\n",
      "loss is 0.9448153729969431\n",
      "epoch 86 completed\n",
      "loss is 0.9430280835342602\n",
      "epoch 87 completed\n",
      "loss is 0.9412613366317003\n",
      "epoch 88 completed\n",
      "loss is 0.9395146717543393\n",
      "epoch 89 completed\n",
      "loss is 0.9377876519800833\n",
      "epoch 90 completed\n",
      "loss is 0.9360798622029387\n",
      "epoch 91 completed\n",
      "loss is 0.9343909074730259\n",
      "epoch 92 completed\n",
      "loss is 0.932720411464415\n",
      "epoch 93 completed\n",
      "loss is 0.9310680150621965\n",
      "epoch 94 completed\n",
      "loss is 0.9294333750606157\n",
      "epoch 95 completed\n",
      "loss is 0.9278161629644806\n",
      "epoch 96 completed\n",
      "loss is 0.9262160638864912\n",
      "epoch 97 completed\n",
      "loss is 0.9246327755335423\n",
      "epoch 98 completed\n",
      "loss is 0.9230660072754769\n",
      "epoch 99 completed\n",
      "loss is 0.9215154792901616\n",
      "epoch 100 completed\n",
      "loss is 0.9199809217791406\n",
      "epoch 101 completed\n",
      "loss is 0.9184620742485117\n",
      "epoch 102 completed\n",
      "loss is 0.9169586848500096\n",
      "epoch 103 completed\n",
      "loss is 0.9154705097776223\n",
      "epoch 104 completed\n",
      "loss is 0.9139973127153793\n",
      "epoch 105 completed\n",
      "loss is 0.9125388643322756\n",
      "epoch 106 completed\n",
      "loss is 0.9110949418205304\n",
      "epoch 107 completed\n",
      "loss is 0.909665328473696\n",
      "epoch 108 completed\n",
      "loss is 0.9082498133013454\n",
      "epoch 109 completed\n",
      "loss is 0.9068481906773143\n",
      "epoch 110 completed\n",
      "loss is 0.9054602600186948\n",
      "epoch 111 completed\n",
      "loss is 0.9040858254929706\n",
      "epoch 112 completed\n",
      "loss is 0.9027246957508952\n",
      "epoch 113 completed\n",
      "loss is 0.9013766836828652\n",
      "epoch 114 completed\n",
      "loss is 0.9000416061967392\n",
      "epoch 115 completed\n",
      "loss is 0.8987192840151934\n",
      "epoch 116 completed\n",
      "loss is 0.8974095414908584\n",
      "epoch 117 completed\n",
      "loss is 0.8961122064376043\n",
      "epoch 118 completed\n",
      "loss is 0.894827109976502\n",
      "epoch 119 completed\n",
      "loss is 0.8935540863950754\n",
      "epoch 120 completed\n",
      "loss is 0.8922929730185907\n",
      "epoch 121 completed\n",
      "loss is 0.8910436100922293\n",
      "epoch 122 completed\n",
      "loss is 0.8898058406730901\n",
      "epoch 123 completed\n",
      "loss is 0.8885795105310499\n",
      "epoch 124 completed\n",
      "loss is 0.8873644680576113\n",
      "epoch 125 completed\n",
      "loss is 0.8861605641819265\n",
      "epoch 126 completed\n",
      "loss is 0.8849676522932775\n",
      "epoch 127 completed\n",
      "loss is 0.8837855881693436\n",
      "epoch 128 completed\n",
      "loss is 0.8826142299096645\n",
      "epoch 129 completed\n",
      "loss is 0.8814534378737529\n",
      "epoch 130 completed\n",
      "loss is 0.8803030746233713\n",
      "epoch 131 completed\n",
      "loss is 0.879163004868533\n",
      "epoch 132 completed\n",
      "loss is 0.8780330954168339\n",
      "epoch 133 completed\n",
      "loss is 0.8769132151257613\n",
      "epoch 134 completed\n",
      "loss is 0.8758032348576616\n",
      "epoch 135 completed\n",
      "loss is 0.8747030274370932\n",
      "epoch 136 completed\n",
      "loss is 0.8736124676103008\n",
      "epoch 137 completed\n",
      "loss is 0.872531432006604\n",
      "epoch 138 completed\n",
      "loss is 0.8714597991014931\n",
      "epoch 139 completed\n",
      "loss is 0.8703974491812572\n",
      "epoch 140 completed\n",
      "loss is 0.8693442643090044\n",
      "epoch 141 completed\n",
      "loss is 0.8683001282919233\n",
      "epoch 142 completed\n",
      "loss is 0.867264926649677\n",
      "epoch 143 completed\n",
      "loss is 0.8662385465838279\n",
      "epoch 144 completed\n",
      "loss is 0.8652208769481999\n",
      "epoch 145 completed\n",
      "loss is 0.8642118082200961\n",
      "epoch 146 completed\n",
      "loss is 0.8632112324723157\n",
      "epoch 147 completed\n",
      "loss is 0.8622190433458979\n",
      "epoch 148 completed\n",
      "loss is 0.8612351360235525\n",
      "epoch 149 completed\n",
      "loss is 0.8602594072037252\n",
      "epoch 150 completed\n",
      "loss is 0.8592917550752666\n",
      "epoch 151 completed\n",
      "loss is 0.8583320792926645\n",
      "epoch 152 completed\n",
      "loss is 0.8573802809518193\n",
      "epoch 153 completed\n",
      "loss is 0.856436262566331\n",
      "epoch 154 completed\n",
      "loss is 0.8554999280442784\n",
      "epoch 155 completed\n",
      "loss is 0.8545711826654748\n",
      "epoch 156 completed\n",
      "loss is 0.8536499330591761\n",
      "epoch 157 completed\n",
      "loss is 0.8527360871822395\n",
      "epoch 158 completed\n",
      "loss is 0.8518295542977046\n",
      "epoch 159 completed\n",
      "loss is 0.8509302449537941\n",
      "epoch 160 completed\n",
      "loss is 0.8500380709633268\n",
      "epoch 161 completed\n",
      "loss is 0.8491529453835208\n",
      "epoch 162 completed\n",
      "loss is 0.8482747824961868\n",
      "epoch 163 completed\n",
      "loss is 0.8474034977883026\n",
      "epoch 164 completed\n",
      "loss is 0.8465390079329559\n",
      "epoch 165 completed\n",
      "loss is 0.8456812307706503\n",
      "epoch 166 completed\n",
      "loss is 0.8448300852909647\n",
      "epoch 167 completed\n",
      "loss is 0.8439854916145625\n",
      "epoch 168 completed\n",
      "loss is 0.8431473709755348\n",
      "epoch 169 completed\n",
      "loss is 0.8423156457040785\n",
      "epoch 170 completed\n",
      "loss is 0.8414902392095011\n",
      "epoch 171 completed\n",
      "loss is 0.8406710759635337\n",
      "epoch 172 completed\n",
      "loss is 0.839858081483962\n",
      "epoch 173 completed\n",
      "loss is 0.8390511823185525\n",
      "epoch 174 completed\n",
      "loss is 0.8382503060292819\n",
      "epoch 175 completed\n",
      "loss is 0.837455381176842\n",
      "epoch 176 completed\n",
      "loss is 0.8366663373054355\n",
      "epoch 177 completed\n",
      "loss is 0.8358831049278387\n",
      "epoch 178 completed\n",
      "loss is 0.8351056155107366\n",
      "epoch 179 completed\n",
      "loss is 0.8343338014603114\n",
      "epoch 180 completed\n",
      "loss is 0.8335675961080893\n",
      "epoch 181 completed\n",
      "loss is 0.8328069336970285\n",
      "epoch 182 completed\n",
      "loss is 0.8320517493678559\n",
      "epoch 183 completed\n",
      "loss is 0.8313019791456298\n",
      "epoch 184 completed\n",
      "loss is 0.8305575599265341\n",
      "epoch 185 completed\n",
      "loss is 0.8298184294648985\n",
      "epoch 186 completed\n",
      "loss is 0.8290845263604283\n",
      "epoch 187 completed\n",
      "loss is 0.8283557900456516\n",
      "epoch 188 completed\n",
      "loss is 0.8276321607735727\n",
      "epoch 189 completed\n",
      "loss is 0.8269135796055205\n",
      "epoch 190 completed\n",
      "loss is 0.8261999883992054\n",
      "epoch 191 completed\n",
      "loss is 0.8254913297969539\n",
      "epoch 192 completed\n",
      "loss is 0.8247875472141462\n",
      "epoch 193 completed\n",
      "loss is 0.8240885848278232\n",
      "epoch 194 completed\n",
      "loss is 0.8233943875654818\n",
      "epoch 195 completed\n",
      "loss is 0.8227049010940429\n",
      "epoch 196 completed\n",
      "loss is 0.8220200718089974\n",
      "epoch 197 completed\n",
      "loss is 0.821339846823707\n",
      "epoch 198 completed\n",
      "loss is 0.820664173958888\n",
      "epoch 199 completed\n",
      "loss is 0.8199930017322452\n",
      "epoch 200 completed\n",
      "loss is 0.8193262793482684\n",
      "epoch 201 completed\n",
      "loss is 0.8186639566881886\n",
      "epoch 202 completed\n",
      "loss is 0.8180059843000853\n",
      "epoch 203 completed\n",
      "loss is 0.8173523133891427\n",
      "epoch 204 completed\n",
      "loss is 0.8167028958080643\n",
      "epoch 205 completed\n",
      "loss is 0.8160576840476212\n",
      "epoch 206 completed\n",
      "loss is 0.8154166312273565\n",
      "epoch 207 completed\n",
      "loss is 0.8147796910864237\n",
      "epoch 208 completed\n",
      "loss is 0.8141468179745697\n",
      "epoch 209 completed\n",
      "loss is 0.813517966843253\n",
      "epoch 210 completed\n",
      "loss is 0.8128930932369055\n",
      "epoch 211 completed\n",
      "loss is 0.8122721532843161\n",
      "epoch 212 completed\n",
      "loss is 0.8116551036901603\n",
      "epoch 213 completed\n",
      "loss is 0.8110419017266504\n",
      "epoch 214 completed\n",
      "loss is 0.810432505225322\n",
      "epoch 215 completed\n",
      "loss is 0.8098268725689435\n",
      "epoch 216 completed\n",
      "loss is 0.8092249626835538\n",
      "epoch 217 completed\n",
      "loss is 0.8086267350306245\n",
      "epoch 218 completed\n",
      "loss is 0.8080321495993424\n",
      "epoch 219 completed\n",
      "loss is 0.807441166899015\n",
      "epoch 220 completed\n",
      "loss is 0.8068537479515985\n",
      "epoch 221 completed\n",
      "loss is 0.8062698542843363\n",
      "epoch 222 completed\n",
      "loss is 0.8056894479225212\n",
      "epoch 223 completed\n",
      "loss is 0.8051124913823692\n",
      "epoch 224 completed\n",
      "loss is 0.8045389476640068\n",
      "epoch 225 completed\n",
      "loss is 0.8039687802445723\n",
      "epoch 226 completed\n",
      "loss is 0.803401953071425\n",
      "epoch 227 completed\n",
      "loss is 0.8028384305554653\n",
      "epoch 228 completed\n",
      "loss is 0.8022781775645625\n",
      "epoch 229 completed\n",
      "loss is 0.8017211594170846\n",
      "epoch 230 completed\n",
      "loss is 0.801167341875537\n",
      "epoch 231 completed\n",
      "loss is 0.8006166911403052\n",
      "epoch 232 completed\n",
      "loss is 0.8000691738434949\n",
      "epoch 233 completed\n",
      "loss is 0.7995247570428685\n",
      "epoch 234 completed\n",
      "loss is 0.798983408215892\n",
      "epoch 235 completed\n",
      "loss is 0.798445095253871\n",
      "epoch 236 completed\n",
      "loss is 0.7979097864561778\n",
      "epoch 237 completed\n",
      "loss is 0.7973774505245812\n",
      "epoch 238 completed\n",
      "loss is 0.7968480565576638\n",
      "epoch 239 completed\n",
      "loss is 0.7963215740453342\n",
      "epoch 240 completed\n",
      "loss is 0.7957979728634195\n",
      "epoch 241 completed\n",
      "loss is 0.7952772232683574\n",
      "epoch 242 completed\n",
      "loss is 0.7947592958919654\n",
      "epoch 243 completed\n",
      "loss is 0.7942441617362997\n",
      "epoch 244 completed\n",
      "loss is 0.7937317921685958\n",
      "epoch 245 completed\n",
      "loss is 0.7932221589162927\n",
      "epoch 246 completed\n",
      "loss is 0.792715234062133\n",
      "epoch 247 completed\n",
      "loss is 0.7922109900393474\n",
      "epoch 248 completed\n",
      "loss is 0.7917093996269144\n",
      "epoch 249 completed\n",
      "loss is 0.791210435944897\n",
      "epoch 250 completed\n",
      "loss is 0.7907140724498501\n",
      "epoch 251 completed\n",
      "loss is 0.7902202829303056\n",
      "epoch 252 completed\n",
      "loss is 0.7897290415023284\n",
      "epoch 253 completed\n",
      "loss is 0.789240322605142\n",
      "epoch 254 completed\n",
      "loss is 0.7887541009968239\n",
      "epoch 255 completed\n",
      "loss is 0.7882703517500681\n",
      "epoch 256 completed\n",
      "loss is 0.7877890502480148\n",
      "epoch 257 completed\n",
      "loss is 0.787310172180142\n",
      "epoch 258 completed\n",
      "loss is 0.7868336935382294\n",
      "epoch 259 completed\n",
      "loss is 0.7863595906123718\n",
      "epoch 260 completed\n",
      "loss is 0.7858878399870661\n",
      "epoch 261 completed\n",
      "loss is 0.7854184185373525\n",
      "epoch 262 completed\n",
      "loss is 0.7849513034250117\n",
      "epoch 263 completed\n",
      "loss is 0.7844864720948277\n",
      "epoch 264 completed\n",
      "loss is 0.7840239022708991\n",
      "epoch 265 completed\n",
      "loss is 0.7835635719530128\n",
      "epoch 266 completed\n",
      "loss is 0.7831054594130614\n",
      "epoch 267 completed\n",
      "loss is 0.7826495431915257\n",
      "epoch 268 completed\n",
      "loss is 0.7821958020940064\n",
      "epoch 269 completed\n",
      "loss is 0.781744215187794\n",
      "epoch 270 completed\n",
      "loss is 0.7812947617985071\n",
      "epoch 271 completed\n",
      "loss is 0.7808474215067667\n",
      "epoch 272 completed\n",
      "loss is 0.7804021741449251\n",
      "epoch 273 completed\n",
      "loss is 0.7799589997938375\n",
      "epoch 274 completed\n",
      "loss is 0.7795178787796773\n",
      "epoch 275 completed\n",
      "loss is 0.7790787916708063\n",
      "epoch 276 completed\n",
      "loss is 0.7786417192746775\n",
      "epoch 277 completed\n",
      "loss is 0.7782066426347883\n",
      "epoch 278 completed\n",
      "loss is 0.7777735430276724\n",
      "epoch 279 completed\n",
      "loss is 0.7773424019599317\n",
      "epoch 280 completed\n",
      "loss is 0.7769132011653169\n",
      "epoch 281 completed\n",
      "loss is 0.7764859226018375\n",
      "epoch 282 completed\n",
      "loss is 0.7760605484489164\n",
      "epoch 283 completed\n",
      "loss is 0.7756370611045819\n",
      "epoch 284 completed\n",
      "loss is 0.7752154431826977\n",
      "epoch 285 completed\n",
      "loss is 0.7747956775102286\n",
      "epoch 286 completed\n",
      "loss is 0.7743777471245394\n",
      "epoch 287 completed\n",
      "loss is 0.773961635270736\n",
      "epoch 288 completed\n",
      "loss is 0.7735473253990331\n",
      "epoch 289 completed\n",
      "loss is 0.7731348011621663\n",
      "epoch 290 completed\n",
      "loss is 0.7727240464128248\n",
      "epoch 291 completed\n",
      "loss is 0.7723150452011274\n",
      "epoch 292 completed\n",
      "loss is 0.771907781772124\n",
      "epoch 293 completed\n",
      "loss is 0.7715022405633334\n",
      "epoch 294 completed\n",
      "loss is 0.7710984062023074\n",
      "epoch 295 completed\n",
      "loss is 0.7706962635042328\n",
      "epoch 296 completed\n",
      "loss is 0.7702957974695521\n",
      "epoch 297 completed\n",
      "loss is 0.7698969932816261\n",
      "epoch 298 completed\n",
      "loss is 0.7694998363044157\n",
      "epoch 299 completed\n",
      "loss is 0.7691043120801971\n",
      "epoch 300 completed\n",
      "loss is 0.7687104063273024\n",
      "epoch 301 completed\n",
      "loss is 0.7683181049378911\n",
      "epoch 302 completed\n",
      "loss is 0.7679273939757467\n",
      "epoch 303 completed\n",
      "loss is 0.7675382596740983\n",
      "epoch 304 completed\n",
      "loss is 0.7671506884334696\n",
      "epoch 305 completed\n",
      "loss is 0.7667646668195558\n",
      "epoch 306 completed\n",
      "loss is 0.7663801815611242\n",
      "epoch 307 completed\n",
      "loss is 0.7659972195479395\n",
      "epoch 308 completed\n",
      "loss is 0.7656157678287149\n",
      "epoch 309 completed\n",
      "loss is 0.7652358136090843\n",
      "epoch 310 completed\n",
      "loss is 0.7648573442496046\n",
      "epoch 311 completed\n",
      "loss is 0.7644803472637769\n",
      "epoch 312 completed\n",
      "loss is 0.7641048103160928\n",
      "epoch 313 completed\n",
      "loss is 0.7637307212200992\n",
      "epoch 314 completed\n",
      "loss is 0.7633580679364962\n",
      "epoch 315 completed\n",
      "loss is 0.7629868385712457\n",
      "epoch 316 completed\n",
      "loss is 0.7626170213737122\n",
      "epoch 317 completed\n",
      "loss is 0.7622486047348133\n",
      "epoch 318 completed\n",
      "loss is 0.7618815771852091\n",
      "epoch 319 completed\n",
      "loss is 0.7615159273934916\n",
      "epoch 320 completed\n",
      "loss is 0.7611516441644148\n",
      "epoch 321 completed\n",
      "loss is 0.7607887164371333\n",
      "epoch 322 completed\n",
      "loss is 0.7604271332834659\n",
      "epoch 323 completed\n",
      "loss is 0.7600668839061809\n",
      "epoch 324 completed\n",
      "loss is 0.7597079576372937\n",
      "epoch 325 completed\n",
      "loss is 0.7593503439363942\n",
      "epoch 326 completed\n",
      "loss is 0.7589940323889904\n",
      "epoch 327 completed\n",
      "loss is 0.7586390127048637\n",
      "epoch 328 completed\n",
      "loss is 0.7582852747164567\n",
      "epoch 329 completed\n",
      "loss is 0.7579328083772727\n",
      "epoch 330 completed\n",
      "loss is 0.7575816037602905\n",
      "epoch 331 completed\n",
      "loss is 0.7572316510564011\n",
      "epoch 332 completed\n",
      "loss is 0.7568829405728694\n",
      "epoch 333 completed\n",
      "loss is 0.7565354627318018\n",
      "epoch 334 completed\n",
      "loss is 0.7561892080686382\n",
      "epoch 335 completed\n",
      "loss is 0.7558441672306687\n",
      "epoch 336 completed\n",
      "loss is 0.7555003309755478\n",
      "epoch 337 completed\n",
      "loss is 0.7551576901698517\n",
      "epoch 338 completed\n",
      "loss is 0.7548162357876288\n",
      "epoch 339 completed\n",
      "loss is 0.7544759589089882\n",
      "epoch 340 completed\n",
      "loss is 0.7541368507186859\n",
      "epoch 341 completed\n",
      "loss is 0.7537989025047437\n",
      "epoch 342 completed\n",
      "loss is 0.7534621056570779\n",
      "epoch 343 completed\n",
      "loss is 0.753126451666138\n",
      "epoch 344 completed\n",
      "loss is 0.7527919321215758\n",
      "epoch 345 completed\n",
      "loss is 0.7524585387109176\n",
      "epoch 346 completed\n",
      "loss is 0.7521262632182607\n",
      "epoch 347 completed\n",
      "loss is 0.7517950975229801\n",
      "epoch 348 completed\n",
      "loss is 0.7514650335984573\n",
      "epoch 349 completed\n",
      "loss is 0.7511360635108155\n",
      "epoch 350 completed\n",
      "loss is 0.7508081794176751\n",
      "epoch 351 completed\n",
      "loss is 0.7504813735669329\n",
      "epoch 352 completed\n",
      "loss is 0.7501556382955389\n",
      "epoch 353 completed\n",
      "loss is 0.7498309660282991\n",
      "epoch 354 completed\n",
      "loss is 0.7495073492766946\n",
      "epoch 355 completed\n",
      "loss is 0.7491847806377074\n",
      "epoch 356 completed\n",
      "loss is 0.7488632527926654\n",
      "epoch 357 completed\n",
      "loss is 0.7485427585061055\n",
      "epoch 358 completed\n",
      "loss is 0.7482232906246425\n",
      "epoch 359 completed\n",
      "loss is 0.7479048420758556\n",
      "epoch 360 completed\n",
      "loss is 0.7475874058671946\n",
      "epoch 361 completed\n",
      "loss is 0.7472709750848892\n",
      "epoch 362 completed\n",
      "loss is 0.7469555428928822\n",
      "epoch 363 completed\n",
      "loss is 0.7466411025317682\n",
      "epoch 364 completed\n",
      "loss is 0.7463276473177488\n",
      "epoch 365 completed\n",
      "loss is 0.7460151706416029\n",
      "epoch 366 completed\n",
      "loss is 0.7457036659676728\n",
      "epoch 367 completed\n",
      "loss is 0.7453931268328505\n",
      "epoch 368 completed\n",
      "loss is 0.7450835468455934\n",
      "epoch 369 completed\n",
      "loss is 0.7447749196849371\n",
      "epoch 370 completed\n",
      "loss is 0.7444672390995383\n",
      "epoch 371 completed\n",
      "loss is 0.7441604989067124\n",
      "epoch 372 completed\n",
      "loss is 0.7438546929914941\n",
      "epoch 373 completed\n",
      "loss is 0.7435498153057114\n",
      "epoch 374 completed\n",
      "loss is 0.7432458598670607\n",
      "epoch 375 completed\n",
      "loss is 0.7429428207582095\n",
      "epoch 376 completed\n",
      "loss is 0.7426406921258991\n",
      "epoch 377 completed\n",
      "loss is 0.7423394681800622\n",
      "epoch 378 completed\n",
      "loss is 0.7420391431929534\n",
      "epoch 379 completed\n",
      "loss is 0.7417397114982912\n",
      "epoch 380 completed\n",
      "loss is 0.7414411674904111\n",
      "epoch 381 completed\n",
      "loss is 0.7411435056234283\n",
      "epoch 382 completed\n",
      "loss is 0.7408467204104177\n",
      "epoch 383 completed\n",
      "loss is 0.7405508064225933\n",
      "epoch 384 completed\n",
      "loss is 0.7402557582885111\n",
      "epoch 385 completed\n",
      "loss is 0.7399615706932758\n",
      "epoch 386 completed\n",
      "loss is 0.7396682383777579\n",
      "epoch 387 completed\n",
      "loss is 0.7393757561378271\n",
      "epoch 388 completed\n",
      "loss is 0.7390841188235905\n",
      "epoch 389 completed\n",
      "loss is 0.7387933213386395\n",
      "epoch 390 completed\n",
      "loss is 0.738503358639318\n",
      "epoch 391 completed\n",
      "loss is 0.738214225733985\n",
      "epoch 392 completed\n",
      "loss is 0.7379259176823053\n",
      "epoch 393 completed\n",
      "loss is 0.7376384295945292\n",
      "epoch 394 completed\n",
      "loss is 0.7373517566308022\n",
      "epoch 395 completed\n",
      "loss is 0.7370658940004715\n",
      "epoch 396 completed\n",
      "loss is 0.7367808369614046\n",
      "epoch 397 completed\n",
      "loss is 0.7364965808193223\n",
      "epoch 398 completed\n",
      "loss is 0.7362131209271358\n",
      "epoch 399 completed\n",
      "loss is 0.7359304526842947\n",
      "epoch 400 completed\n",
      "loss is 0.7356485715361423\n",
      "epoch 401 completed\n",
      "loss is 0.7353674729732892\n",
      "epoch 402 completed\n",
      "loss is 0.7350871525309781\n",
      "epoch 403 completed\n",
      "loss is 0.734807605788479\n",
      "epoch 404 completed\n",
      "loss is 0.7345288283684716\n",
      "epoch 405 completed\n",
      "loss is 0.7342508159364548\n",
      "epoch 406 completed\n",
      "loss is 0.7339735642001535\n",
      "epoch 407 completed\n",
      "loss is 0.7336970689089383\n",
      "epoch 408 completed\n",
      "loss is 0.7334213258532502\n",
      "epoch 409 completed\n",
      "loss is 0.7331463308640375\n",
      "epoch 410 completed\n",
      "loss is 0.7328720798122\n",
      "epoch 411 completed\n",
      "loss is 0.7325985686080394\n",
      "epoch 412 completed\n",
      "loss is 0.7323257932007156\n",
      "epoch 413 completed\n",
      "loss is 0.7320537495777201\n",
      "epoch 414 completed\n",
      "loss is 0.7317824337643438\n",
      "epoch 415 completed\n",
      "loss is 0.7315118418231688\n",
      "epoch 416 completed\n",
      "loss is 0.7312419698535443\n",
      "epoch 417 completed\n",
      "loss is 0.7309728139911003\n",
      "epoch 418 completed\n",
      "loss is 0.7307043704072386\n",
      "epoch 419 completed\n",
      "loss is 0.7304366353086563\n",
      "epoch 420 completed\n",
      "loss is 0.7301696049368559\n",
      "epoch 421 completed\n",
      "loss is 0.7299032755676782\n",
      "epoch 422 completed\n",
      "loss is 0.7296376435108315\n",
      "epoch 423 completed\n",
      "loss is 0.7293727051094334\n",
      "epoch 424 completed\n",
      "loss is 0.7291084567395634\n",
      "epoch 425 completed\n",
      "loss is 0.728844894809804\n",
      "epoch 426 completed\n",
      "loss is 0.7285820157608127\n",
      "epoch 427 completed\n",
      "loss is 0.7283198160648849\n",
      "epoch 428 completed\n",
      "loss is 0.7280582922255306\n",
      "epoch 429 completed\n",
      "loss is 0.7277974407770463\n",
      "epoch 430 completed\n",
      "loss is 0.7275372582841101\n",
      "epoch 431 completed\n",
      "loss is 0.7272777413413691\n",
      "epoch 432 completed\n",
      "loss is 0.7270188865730395\n",
      "epoch 433 completed\n",
      "loss is 0.7267606906325123\n",
      "epoch 434 completed\n",
      "loss is 0.7265031502019604\n",
      "epoch 435 completed\n",
      "loss is 0.7262462619919604\n",
      "epoch 436 completed\n",
      "loss is 0.7259900227411095\n",
      "epoch 437 completed\n",
      "loss is 0.7257344292156561\n",
      "epoch 438 completed\n",
      "loss is 0.7254794782091352\n",
      "epoch 439 completed\n",
      "loss is 0.7252251665420035\n",
      "epoch 440 completed\n",
      "loss is 0.7249714910612858\n",
      "epoch 441 completed\n",
      "loss is 0.7247184486402277\n",
      "epoch 442 completed\n",
      "loss is 0.7244660361779499\n",
      "epoch 443 completed\n",
      "loss is 0.7242142505991052\n",
      "epoch 444 completed\n",
      "loss is 0.7239630888535498\n",
      "epoch 445 completed\n",
      "loss is 0.7237125479160101\n",
      "epoch 446 completed\n",
      "loss is 0.7234626247857591\n",
      "epoch 447 completed\n",
      "loss is 0.7232133164863037\n",
      "epoch 448 completed\n",
      "loss is 0.7229646200650631\n",
      "epoch 449 completed\n",
      "loss is 0.7227165325930635\n",
      "epoch 450 completed\n",
      "loss is 0.7224690511646302\n",
      "epoch 451 completed\n",
      "loss is 0.7222221728970944\n",
      "epoch 452 completed\n",
      "loss is 0.7219758949304893\n",
      "epoch 453 completed\n",
      "loss is 0.721730214427269\n",
      "epoch 454 completed\n",
      "loss is 0.7214851285720179\n",
      "epoch 455 completed\n",
      "loss is 0.7212406345711647\n",
      "epoch 456 completed\n",
      "loss is 0.7209967296527138\n",
      "epoch 457 completed\n",
      "loss is 0.7207534110659668\n",
      "epoch 458 completed\n",
      "loss is 0.7205106760812552\n",
      "epoch 459 completed\n",
      "loss is 0.7202685219896768\n",
      "epoch 460 completed\n",
      "loss is 0.7200269461028342\n",
      "epoch 461 completed\n",
      "loss is 0.7197859457525769\n",
      "epoch 462 completed\n",
      "loss is 0.7195455182907526\n",
      "epoch 463 completed\n",
      "loss is 0.7193056610889601\n",
      "epoch 464 completed\n",
      "loss is 0.7190663715382969\n",
      "epoch 465 completed\n",
      "loss is 0.7188276470491259\n",
      "epoch 466 completed\n",
      "loss is 0.7185894850508351\n",
      "epoch 467 completed\n",
      "loss is 0.7183518829916056\n",
      "epoch 468 completed\n",
      "loss is 0.7181148383381843\n",
      "epoch 469 completed\n",
      "loss is 0.7178783485756491\n",
      "epoch 470 completed\n",
      "loss is 0.7176424112072006\n",
      "epoch 471 completed\n",
      "loss is 0.7174070237539294\n",
      "epoch 472 completed\n",
      "loss is 0.7171721837546087\n",
      "epoch 473 completed\n",
      "loss is 0.7169378887654801\n",
      "epoch 474 completed\n",
      "loss is 0.7167041363600432\n",
      "epoch 475 completed\n",
      "loss is 0.7164709241288534\n",
      "epoch 476 completed\n",
      "loss is 0.7162382496793158\n",
      "epoch 477 completed\n",
      "loss is 0.7160061106354925\n",
      "epoch 478 completed\n",
      "loss is 0.7157745046378924\n",
      "epoch 479 completed\n",
      "loss is 0.7155434293432987\n",
      "epoch 480 completed\n",
      "loss is 0.715312882424562\n",
      "epoch 481 completed\n",
      "loss is 0.7150828615704209\n",
      "epoch 482 completed\n",
      "loss is 0.714853364485318\n",
      "epoch 483 completed\n",
      "loss is 0.7146243888892165\n",
      "epoch 484 completed\n",
      "loss is 0.7143959325174206\n",
      "epoch 485 completed\n",
      "loss is 0.714167993120407\n",
      "epoch 486 completed\n",
      "loss is 0.7139405684636456\n",
      "epoch 487 completed\n",
      "loss is 0.7137136563274298\n",
      "epoch 488 completed\n",
      "loss is 0.7134872545067145\n",
      "epoch 489 completed\n",
      "loss is 0.7132613608109463\n",
      "epoch 490 completed\n",
      "loss is 0.7130359730639007\n",
      "epoch 491 completed\n",
      "loss is 0.7128110891035282\n",
      "epoch 492 completed\n",
      "loss is 0.7125867067817927\n",
      "epoch 493 completed\n",
      "loss is 0.7123628239645181\n",
      "epoch 494 completed\n",
      "loss is 0.7121394385312358\n",
      "epoch 495 completed\n",
      "loss is 0.7119165483750354\n",
      "epoch 496 completed\n",
      "loss is 0.7116941514024165\n",
      "epoch 497 completed\n",
      "loss is 0.7114722455331445\n",
      "epoch 498 completed\n",
      "loss is 0.7112508287001105\n",
      "epoch 499 completed\n",
      "loss is 0.7110298988491802\n",
      "epoch 500 completed\n",
      "loss is 0.7108094539390696\n",
      "epoch 501 completed\n",
      "loss is 0.7105894919411997\n",
      "epoch 502 completed\n",
      "loss is 0.7103700108395656\n",
      "epoch 503 completed\n",
      "loss is 0.7101510086306\n",
      "epoch 504 completed\n",
      "loss is 0.7099324833230498\n",
      "epoch 505 completed\n",
      "loss is 0.7097144329378446\n",
      "epoch 506 completed\n",
      "loss is 0.709496855507971\n",
      "epoch 507 completed\n",
      "loss is 0.7092797490783449\n",
      "epoch 508 completed\n",
      "loss is 0.7090631117056992\n",
      "epoch 509 completed\n",
      "loss is 0.7088469414584536\n",
      "epoch 510 completed\n",
      "loss is 0.7086312364165988\n",
      "epoch 511 completed\n",
      "loss is 0.7084159946715851\n",
      "epoch 512 completed\n",
      "loss is 0.7082012143262004\n",
      "epoch 513 completed\n",
      "loss is 0.7079868934944634\n",
      "epoch 514 completed\n",
      "loss is 0.707773030301508\n",
      "epoch 515 completed\n",
      "loss is 0.7075596228834753\n",
      "epoch 516 completed\n",
      "loss is 0.7073466693874041\n",
      "epoch 517 completed\n",
      "loss is 0.7071341679711283\n",
      "epoch 518 completed\n",
      "loss is 0.7069221168031716\n",
      "epoch 519 completed\n",
      "loss is 0.7067105140626401\n",
      "epoch 520 completed\n",
      "loss is 0.7064993579391263\n",
      "epoch 521 completed\n",
      "loss is 0.7062886466326064\n",
      "epoch 522 completed\n",
      "loss is 0.7060783783533416\n",
      "epoch 523 completed\n",
      "loss is 0.7058685513217873\n",
      "epoch 524 completed\n",
      "loss is 0.705659163768484\n",
      "epoch 525 completed\n",
      "loss is 0.7054502139339781\n",
      "epoch 526 completed\n",
      "loss is 0.7052417000687209\n",
      "epoch 527 completed\n",
      "loss is 0.7050336204329837\n",
      "epoch 528 completed\n",
      "loss is 0.7048259732967566\n",
      "epoch 529 completed\n",
      "loss is 0.7046187569396734\n",
      "epoch 530 completed\n",
      "loss is 0.7044119696509201\n",
      "epoch 531 completed\n",
      "loss is 0.7042056097291421\n",
      "epoch 532 completed\n",
      "loss is 0.7039996754823679\n",
      "epoch 533 completed\n",
      "loss is 0.7037941652279238\n",
      "epoch 534 completed\n",
      "loss is 0.7035890772923529\n",
      "epoch 535 completed\n",
      "loss is 0.7033844100113279\n",
      "epoch 536 completed\n",
      "loss is 0.7031801617295794\n",
      "epoch 537 completed\n",
      "loss is 0.7029763308008145\n",
      "epoch 538 completed\n",
      "loss is 0.7027729155876374\n",
      "epoch 539 completed\n",
      "loss is 0.7025699144614769\n",
      "epoch 540 completed\n",
      "loss is 0.7023673258025072\n",
      "epoch 541 completed\n",
      "loss is 0.7021651479995779\n",
      "epoch 542 completed\n",
      "loss is 0.7019633794501385\n",
      "epoch 543 completed\n",
      "loss is 0.7017620185601686\n",
      "epoch 544 completed\n",
      "loss is 0.7015610637441041\n",
      "epoch 545 completed\n",
      "loss is 0.7013605134247698\n",
      "epoch 546 completed\n",
      "loss is 0.7011603660333098\n",
      "epoch 547 completed\n",
      "loss is 0.7009606200091193\n",
      "epoch 548 completed\n",
      "loss is 0.7007612737997785\n",
      "epoch 549 completed\n",
      "loss is 0.7005623258609878\n",
      "epoch 550 completed\n",
      "loss is 0.7003637746564992\n",
      "epoch 551 completed\n",
      "loss is 0.7001656186580564\n",
      "epoch 552 completed\n",
      "loss is 0.6999678563453284\n",
      "epoch 553 completed\n",
      "loss is 0.6997704862058469\n",
      "epoch 554 completed\n",
      "loss is 0.6995735067349502\n",
      "epoch 555 completed\n",
      "loss is 0.6993769164357151\n",
      "epoch 556 completed\n",
      "loss is 0.6991807138189021\n",
      "epoch 557 completed\n",
      "loss is 0.6989848974028946\n",
      "epoch 558 completed\n",
      "loss is 0.698789465713644\n",
      "epoch 559 completed\n",
      "loss is 0.6985944172846084\n",
      "epoch 560 completed\n",
      "loss is 0.6983997506566952\n",
      "epoch 561 completed\n",
      "loss is 0.698205464378211\n",
      "epoch 562 completed\n",
      "loss is 0.6980115570048004\n",
      "epoch 563 completed\n",
      "loss is 0.697818027099396\n",
      "epoch 564 completed\n",
      "loss is 0.697624873232165\n",
      "epoch 565 completed\n",
      "loss is 0.6974320939804469\n",
      "epoch 566 completed\n",
      "loss is 0.6972396879287183\n",
      "epoch 567 completed\n",
      "loss is 0.6970476536685277\n",
      "epoch 568 completed\n",
      "loss is 0.6968559897984532\n",
      "epoch 569 completed\n",
      "loss is 0.6966646949240392\n",
      "epoch 570 completed\n",
      "loss is 0.6964737676577657\n",
      "epoch 571 completed\n",
      "loss is 0.6962832066189831\n",
      "epoch 572 completed\n",
      "loss is 0.6960930104338748\n",
      "epoch 573 completed\n",
      "loss is 0.6959031777354062\n",
      "epoch 574 completed\n",
      "loss is 0.6957137071632745\n",
      "epoch 575 completed\n",
      "loss is 0.6955245973638622\n",
      "epoch 576 completed\n",
      "loss is 0.6953358469902013\n",
      "epoch 577 completed\n",
      "loss is 0.6951474547019141\n",
      "epoch 578 completed\n",
      "loss is 0.6949594191651796\n",
      "epoch 579 completed\n",
      "loss is 0.6947717390526785\n",
      "epoch 580 completed\n",
      "loss is 0.6945844130435659\n",
      "epoch 581 completed\n",
      "loss is 0.6943974398234096\n",
      "epoch 582 completed\n",
      "loss is 0.6942108180841585\n",
      "epoch 583 completed\n",
      "loss is 0.6940245465241024\n",
      "epoch 584 completed\n",
      "loss is 0.6938386238478202\n",
      "epoch 585 completed\n",
      "loss is 0.6936530487661482\n",
      "epoch 586 completed\n",
      "loss is 0.6934678199961392\n",
      "epoch 587 completed\n",
      "loss is 0.6932829362610153\n",
      "epoch 588 completed\n",
      "loss is 0.693098396290137\n",
      "epoch 589 completed\n",
      "loss is 0.6929141988189551\n",
      "epoch 590 completed\n",
      "loss is 0.6927303425889814\n",
      "epoch 591 completed\n",
      "loss is 0.6925468263477439\n",
      "epoch 592 completed\n",
      "loss is 0.692363648848754\n",
      "epoch 593 completed\n",
      "loss is 0.6921808088514612\n",
      "epoch 594 completed\n",
      "loss is 0.6919983051212247\n",
      "epoch 595 completed\n",
      "loss is 0.6918161364292735\n",
      "epoch 596 completed\n",
      "loss is 0.6916343015526694\n",
      "epoch 597 completed\n",
      "loss is 0.6914527992742723\n",
      "epoch 598 completed\n",
      "loss is 0.6912716283827058\n",
      "epoch 599 completed\n",
      "loss is 0.6910907876723189\n",
      "epoch 600 completed\n",
      "loss is 0.6909102759431588\n",
      "epoch 601 completed\n",
      "loss is 0.6907300920009277\n",
      "epoch 602 completed\n",
      "loss is 0.690550234656952\n",
      "epoch 603 completed\n",
      "loss is 0.6903707027281556\n",
      "epoch 604 completed\n",
      "loss is 0.6901914950370166\n",
      "epoch 605 completed\n",
      "loss is 0.6900126104115393\n",
      "epoch 606 completed\n",
      "loss is 0.6898340476852262\n",
      "epoch 607 completed\n",
      "loss is 0.6896558056970364\n",
      "epoch 608 completed\n",
      "loss is 0.68947788329136\n",
      "epoch 609 completed\n",
      "loss is 0.6893002793179909\n",
      "epoch 610 completed\n",
      "loss is 0.6891229926320853\n",
      "epoch 611 completed\n",
      "loss is 0.6889460220941357\n",
      "epoch 612 completed\n",
      "loss is 0.6887693665699437\n",
      "epoch 613 completed\n",
      "loss is 0.6885930249305859\n",
      "epoch 614 completed\n",
      "loss is 0.6884169960523874\n",
      "epoch 615 completed\n",
      "loss is 0.6882412788168868\n",
      "epoch 616 completed\n",
      "loss is 0.6880658721108103\n",
      "epoch 617 completed\n",
      "loss is 0.6878907748260458\n",
      "epoch 618 completed\n",
      "loss is 0.6877159858596112\n",
      "epoch 619 completed\n",
      "loss is 0.6875415041136219\n",
      "epoch 620 completed\n",
      "loss is 0.6873673284952683\n",
      "epoch 621 completed\n",
      "loss is 0.6871934579167852\n",
      "epoch 622 completed\n",
      "loss is 0.6870198912954253\n",
      "epoch 623 completed\n",
      "loss is 0.686846627553436\n",
      "epoch 624 completed\n",
      "loss is 0.6866736656180232\n",
      "epoch 625 completed\n",
      "loss is 0.6865010044213309\n",
      "epoch 626 completed\n",
      "loss is 0.6863286429004085\n",
      "epoch 627 completed\n",
      "loss is 0.6861565799971959\n",
      "epoch 628 completed\n",
      "loss is 0.6859848146584822\n",
      "epoch 629 completed\n",
      "loss is 0.6858133458358938\n",
      "epoch 630 completed\n",
      "loss is 0.6856421724858588\n",
      "epoch 631 completed\n",
      "loss is 0.6854712935695868\n",
      "epoch 632 completed\n",
      "loss is 0.6853007080530363\n",
      "epoch 633 completed\n",
      "loss is 0.6851304149069088\n",
      "epoch 634 completed\n",
      "loss is 0.6849604131065972\n",
      "epoch 635 completed\n",
      "loss is 0.6847907016321776\n",
      "epoch 636 completed\n",
      "loss is 0.6846212794683858\n",
      "epoch 637 completed\n",
      "loss is 0.6844521456045817\n",
      "epoch 638 completed\n",
      "loss is 0.6842832990347402\n",
      "epoch 639 completed\n",
      "loss is 0.6841147387574151\n",
      "epoch 640 completed\n",
      "loss is 0.683946463775718\n",
      "epoch 641 completed\n",
      "loss is 0.6837784730973027\n",
      "epoch 642 completed\n",
      "loss is 0.6836107657343293\n",
      "epoch 643 completed\n",
      "loss is 0.6834433407034533\n",
      "epoch 644 completed\n",
      "loss is 0.6832761970257941\n",
      "epoch 645 completed\n",
      "loss is 0.6831093337269141\n",
      "epoch 646 completed\n",
      "loss is 0.682942749836801\n",
      "epoch 647 completed\n",
      "loss is 0.6827764443898402\n",
      "epoch 648 completed\n",
      "loss is 0.6826104164247917\n",
      "epoch 649 completed\n",
      "loss is 0.6824446649847721\n",
      "epoch 650 completed\n",
      "loss is 0.6822791891172353\n",
      "epoch 651 completed\n",
      "loss is 0.6821139878739406\n",
      "epoch 652 completed\n",
      "loss is 0.6819490603109379\n",
      "epoch 653 completed\n",
      "loss is 0.6817844054885468\n",
      "epoch 654 completed\n",
      "loss is 0.6816200224713352\n",
      "epoch 655 completed\n",
      "loss is 0.681455910328093\n",
      "epoch 656 completed\n",
      "loss is 0.6812920681318202\n",
      "epoch 657 completed\n",
      "loss is 0.6811284949596967\n",
      "epoch 658 completed\n",
      "loss is 0.6809651898930675\n",
      "epoch 659 completed\n",
      "loss is 0.6808021520174183\n",
      "epoch 660 completed\n",
      "loss is 0.6806393804223644\n",
      "epoch 661 completed\n",
      "loss is 0.6804768742016176\n",
      "epoch 662 completed\n",
      "loss is 0.6803146324529721\n",
      "epoch 663 completed\n",
      "loss is 0.6801526542782873\n",
      "epoch 664 completed\n",
      "loss is 0.6799909387834626\n",
      "epoch 665 completed\n",
      "loss is 0.6798294850784219\n",
      "epoch 666 completed\n",
      "loss is 0.6796682922770919\n",
      "epoch 667 completed\n",
      "loss is 0.67950735949738\n",
      "epoch 668 completed\n",
      "loss is 0.6793466858611663\n",
      "epoch 669 completed\n",
      "loss is 0.6791862704942659\n",
      "epoch 670 completed\n",
      "loss is 0.6790261125264287\n",
      "epoch 671 completed\n",
      "loss is 0.6788662110913067\n",
      "epoch 672 completed\n",
      "loss is 0.6787065653264415\n",
      "epoch 673 completed\n",
      "loss is 0.6785471743732455\n",
      "epoch 674 completed\n",
      "loss is 0.6783880373769839\n",
      "epoch 675 completed\n",
      "loss is 0.6782291534867474\n",
      "epoch 676 completed\n",
      "loss is 0.678070521855451\n",
      "epoch 677 completed\n",
      "loss is 0.6779121416398037\n",
      "epoch 678 completed\n",
      "loss is 0.6777540120002843\n",
      "epoch 679 completed\n",
      "loss is 0.6775961321011396\n",
      "epoch 680 completed\n",
      "loss is 0.677438501110355\n",
      "epoch 681 completed\n",
      "loss is 0.6772811181996421\n",
      "epoch 682 completed\n",
      "loss is 0.6771239825444187\n",
      "epoch 683 completed\n",
      "loss is 0.6769670933237926\n",
      "epoch 684 completed\n",
      "loss is 0.6768104497205414\n",
      "epoch 685 completed\n",
      "loss is 0.6766540509210952\n",
      "epoch 686 completed\n",
      "loss is 0.6764978961155256\n",
      "epoch 687 completed\n",
      "loss is 0.6763419844975177\n",
      "epoch 688 completed\n",
      "loss is 0.676186315264364\n",
      "epoch 689 completed\n",
      "loss is 0.6760308876169412\n",
      "epoch 690 completed\n",
      "loss is 0.6758757007596964\n",
      "epoch 691 completed\n",
      "loss is 0.6757207539006217\n",
      "epoch 692 completed\n",
      "loss is 0.675566046251252\n",
      "epoch 693 completed\n",
      "loss is 0.6754115770266393\n",
      "epoch 694 completed\n",
      "loss is 0.6752573454453344\n",
      "epoch 695 completed\n",
      "loss is 0.675103350729376\n",
      "epoch 696 completed\n",
      "loss is 0.6749495921042733\n",
      "epoch 697 completed\n",
      "loss is 0.6747960687989831\n",
      "epoch 698 completed\n",
      "loss is 0.6746427800459014\n",
      "epoch 699 completed\n",
      "loss is 0.6744897250808497\n",
      "epoch 700 completed\n",
      "loss is 0.6743369031430501\n",
      "epoch 701 completed\n",
      "loss is 0.6741843134751111\n",
      "epoch 702 completed\n",
      "loss is 0.6740319553230195\n",
      "epoch 703 completed\n",
      "loss is 0.6738798279361137\n",
      "epoch 704 completed\n",
      "loss is 0.6737279305670797\n",
      "epoch 705 completed\n",
      "loss is 0.6735762624719258\n",
      "epoch 706 completed\n",
      "loss is 0.6734248229099719\n",
      "epoch 707 completed\n",
      "loss is 0.6732736111438327\n",
      "epoch 708 completed\n",
      "loss is 0.6731226264394015\n",
      "epoch 709 completed\n",
      "loss is 0.6729718680658391\n",
      "epoch 710 completed\n",
      "loss is 0.6728213352955529\n",
      "epoch 711 completed\n",
      "loss is 0.6726710274041849\n",
      "epoch 712 completed\n",
      "loss is 0.6725209436705957\n",
      "epoch 713 completed\n",
      "loss is 0.6723710833768537\n",
      "epoch 714 completed\n",
      "loss is 0.6722214458082152\n",
      "epoch 715 completed\n",
      "loss is 0.6720720302531091\n",
      "epoch 716 completed\n",
      "loss is 0.671922836003124\n",
      "epoch 717 completed\n",
      "loss is 0.6717738623529962\n",
      "epoch 718 completed\n",
      "loss is 0.6716251086005932\n",
      "epoch 719 completed\n",
      "loss is 0.6714765740468951\n",
      "epoch 720 completed\n",
      "loss is 0.6713282579959875\n",
      "epoch 721 completed\n",
      "loss is 0.6711801597550413\n",
      "epoch 722 completed\n",
      "loss is 0.6710322786343001\n",
      "epoch 723 completed\n",
      "loss is 0.6708846139470679\n",
      "epoch 724 completed\n",
      "loss is 0.6707371650096907\n",
      "epoch 725 completed\n",
      "loss is 0.6705899311415493\n",
      "epoch 726 completed\n",
      "loss is 0.6704429116650396\n",
      "epoch 727 completed\n",
      "loss is 0.6702961059055597\n",
      "epoch 728 completed\n",
      "loss is 0.6701495131914944\n",
      "epoch 729 completed\n",
      "loss is 0.6700031328542104\n",
      "epoch 730 completed\n",
      "loss is 0.6698569642280258\n",
      "epoch 731 completed\n",
      "loss is 0.6697110066502137\n",
      "epoch 732 completed\n",
      "loss is 0.6695652594609774\n",
      "epoch 733 completed\n",
      "loss is 0.6694197220034444\n",
      "epoch 734 completed\n",
      "loss is 0.6692743936236476\n",
      "epoch 735 completed\n",
      "loss is 0.669129273670508\n",
      "epoch 736 completed\n",
      "loss is 0.6689843614958371\n",
      "epoch 737 completed\n",
      "loss is 0.6688396564543055\n",
      "epoch 738 completed\n",
      "loss is 0.6686951579034394\n",
      "epoch 739 completed\n",
      "loss is 0.6685508652036025\n",
      "epoch 740 completed\n",
      "loss is 0.6684067777179953\n",
      "epoch 741 completed\n",
      "loss is 0.6682628948126226\n",
      "epoch 742 completed\n",
      "loss is 0.6681192158562953\n",
      "epoch 743 completed\n",
      "loss is 0.6679757402206138\n",
      "epoch 744 completed\n",
      "loss is 0.6678324672799502\n",
      "epoch 745 completed\n",
      "loss is 0.6676893964114414\n",
      "epoch 746 completed\n",
      "loss is 0.6675465269949753\n",
      "epoch 747 completed\n",
      "loss is 0.6674038584131744\n",
      "epoch 748 completed\n",
      "loss is 0.6672613900513894\n",
      "epoch 749 completed\n",
      "loss is 0.6671191212976804\n",
      "epoch 750 completed\n",
      "loss is 0.6669770515428066\n",
      "epoch 751 completed\n",
      "loss is 0.6668351801802175\n",
      "epoch 752 completed\n",
      "loss is 0.6666935066060345\n",
      "epoch 753 completed\n",
      "loss is 0.6665520302190442\n",
      "epoch 754 completed\n",
      "loss is 0.6664107504206828\n",
      "epoch 755 completed\n",
      "loss is 0.6662696666150205\n",
      "epoch 756 completed\n",
      "loss is 0.6661287782087575\n",
      "epoch 757 completed\n",
      "loss is 0.665988084611204\n",
      "epoch 758 completed\n",
      "loss is 0.6658475852342776\n",
      "epoch 759 completed\n",
      "loss is 0.6657072794924805\n",
      "epoch 760 completed\n",
      "loss is 0.6655671668028939\n",
      "epoch 761 completed\n",
      "loss is 0.6654272465851624\n",
      "epoch 762 completed\n",
      "loss is 0.665287518261491\n",
      "epoch 763 completed\n",
      "loss is 0.665147981256619\n",
      "epoch 764 completed\n",
      "loss is 0.6650086349978203\n",
      "epoch 765 completed\n",
      "loss is 0.6648694789148893\n",
      "epoch 766 completed\n",
      "loss is 0.6647305124401224\n",
      "epoch 767 completed\n",
      "loss is 0.6645917350083144\n",
      "epoch 768 completed\n",
      "loss is 0.6644531460567458\n",
      "epoch 769 completed\n",
      "loss is 0.6643147450251632\n",
      "epoch 770 completed\n",
      "loss is 0.6641765313557828\n",
      "epoch 771 completed\n",
      "loss is 0.6640385044932628\n",
      "epoch 772 completed\n",
      "loss is 0.6639006638847045\n",
      "epoch 773 completed\n",
      "loss is 0.6637630089796369\n",
      "epoch 774 completed\n",
      "loss is 0.6636255392300012\n",
      "epoch 775 completed\n",
      "loss is 0.6634882540901426\n",
      "epoch 776 completed\n",
      "loss is 0.663351153016805\n",
      "epoch 777 completed\n",
      "loss is 0.6632142354691117\n",
      "epoch 778 completed\n",
      "loss is 0.6630775009085602\n",
      "epoch 779 completed\n",
      "loss is 0.6629409487990009\n",
      "epoch 780 completed\n",
      "loss is 0.6628045786066447\n",
      "epoch 781 completed\n",
      "loss is 0.6626683898000294\n",
      "epoch 782 completed\n",
      "loss is 0.6625323818500342\n",
      "epoch 783 completed\n",
      "loss is 0.6623965542298456\n",
      "epoch 784 completed\n",
      "loss is 0.662260906414959\n",
      "epoch 785 completed\n",
      "loss is 0.6621254378831661\n",
      "epoch 786 completed\n",
      "loss is 0.6619901481145439\n",
      "epoch 787 completed\n",
      "loss is 0.6618550365914424\n",
      "epoch 788 completed\n",
      "loss is 0.6617201027984773\n",
      "epoch 789 completed\n",
      "loss is 0.661585346222519\n",
      "epoch 790 completed\n",
      "loss is 0.6614507663526781\n",
      "epoch 791 completed\n",
      "loss is 0.6613163626803013\n",
      "epoch 792 completed\n",
      "loss is 0.661182134698954\n",
      "epoch 793 completed\n",
      "loss is 0.6610480819044174\n",
      "epoch 794 completed\n",
      "loss is 0.6609142037946704\n",
      "epoch 795 completed\n",
      "loss is 0.6607804998698855\n",
      "epoch 796 completed\n",
      "loss is 0.6606469696324183\n",
      "epoch 797 completed\n",
      "loss is 0.6605136125867946\n",
      "epoch 798 completed\n",
      "loss is 0.6603804282397044\n",
      "epoch 799 completed\n",
      "loss is 0.6602474160999807\n",
      "epoch 800 completed\n",
      "loss is 0.6601145756786068\n",
      "epoch 801 completed\n",
      "loss is 0.659981906488692\n",
      "epoch 802 completed\n",
      "loss is 0.6598494080454704\n",
      "epoch 803 completed\n",
      "loss is 0.6597170798662864\n",
      "epoch 804 completed\n",
      "loss is 0.6595849214705823\n",
      "epoch 805 completed\n",
      "loss is 0.659452932379901\n",
      "epoch 806 completed\n",
      "loss is 0.659321112117861\n",
      "epoch 807 completed\n",
      "loss is 0.6591894602101541\n",
      "epoch 808 completed\n",
      "loss is 0.6590579761845382\n",
      "epoch 809 completed\n",
      "loss is 0.6589266595708281\n",
      "epoch 810 completed\n",
      "loss is 0.6587955099008693\n",
      "epoch 811 completed\n",
      "loss is 0.6586645267085576\n",
      "epoch 812 completed\n",
      "loss is 0.6585337095298016\n",
      "epoch 813 completed\n",
      "loss is 0.6584030579025324\n",
      "epoch 814 completed\n",
      "loss is 0.6582725713666859\n",
      "epoch 815 completed\n",
      "loss is 0.658142249464194\n",
      "epoch 816 completed\n",
      "loss is 0.6580120917389791\n",
      "epoch 817 completed\n",
      "loss is 0.6578820977369366\n",
      "epoch 818 completed\n",
      "loss is 0.6577522670059351\n",
      "epoch 819 completed\n",
      "loss is 0.6576225990958035\n",
      "epoch 820 completed\n",
      "loss is 0.6574930935583235\n",
      "epoch 821 completed\n",
      "loss is 0.6573637499472108\n",
      "epoch 822 completed\n",
      "loss is 0.6572345678181206\n",
      "epoch 823 completed\n",
      "loss is 0.6571055467286316\n",
      "epoch 824 completed\n",
      "loss is 0.6569766862382379\n",
      "epoch 825 completed\n",
      "loss is 0.656847985908333\n",
      "epoch 826 completed\n",
      "loss is 0.6567194453022235\n",
      "epoch 827 completed\n",
      "loss is 0.6565910639850856\n",
      "epoch 828 completed\n",
      "loss is 0.6564628415239828\n",
      "epoch 829 completed\n",
      "loss is 0.6563347774878491\n",
      "epoch 830 completed\n",
      "loss is 0.6562068714474851\n",
      "epoch 831 completed\n",
      "loss is 0.6560791229755379\n",
      "epoch 832 completed\n",
      "loss is 0.6559515316465043\n",
      "epoch 833 completed\n",
      "loss is 0.6558240970367174\n",
      "epoch 834 completed\n",
      "loss is 0.6556968187243305\n",
      "epoch 835 completed\n",
      "loss is 0.6555696962893249\n",
      "epoch 836 completed\n",
      "loss is 0.6554427293134887\n",
      "epoch 837 completed\n",
      "loss is 0.6553159173804152\n",
      "epoch 838 completed\n",
      "loss is 0.6551892600754838\n",
      "epoch 839 completed\n",
      "loss is 0.655062756985871\n",
      "epoch 840 completed\n",
      "loss is 0.6549364077005186\n",
      "epoch 841 completed\n",
      "loss is 0.6548102118101489\n",
      "epoch 842 completed\n",
      "loss is 0.6546841689072334\n",
      "epoch 843 completed\n",
      "loss is 0.6545582785860042\n",
      "epoch 844 completed\n",
      "loss is 0.6544325404424334\n",
      "epoch 845 completed\n",
      "loss is 0.6543069540742312\n",
      "epoch 846 completed\n",
      "loss is 0.6541815190808371\n",
      "epoch 847 completed\n",
      "loss is 0.6540562350634112\n",
      "epoch 848 completed\n",
      "loss is 0.6539311016248198\n",
      "epoch 849 completed\n",
      "loss is 0.6538061183696358\n",
      "epoch 850 completed\n",
      "loss is 0.653681284904133\n",
      "epoch 851 completed\n",
      "loss is 0.6535566008362673\n",
      "epoch 852 completed\n",
      "loss is 0.653432065775678\n",
      "epoch 853 completed\n",
      "loss is 0.6533076793336753\n",
      "epoch 854 completed\n",
      "loss is 0.6531834411232359\n",
      "epoch 855 completed\n",
      "loss is 0.6530593507589916\n",
      "epoch 856 completed\n",
      "loss is 0.6529354078572238\n",
      "epoch 857 completed\n",
      "loss is 0.6528116120358567\n",
      "epoch 858 completed\n",
      "loss is 0.652687962914447\n",
      "epoch 859 completed\n",
      "loss is 0.652564460114176\n",
      "epoch 860 completed\n",
      "loss is 0.6524411032578505\n",
      "epoch 861 completed\n",
      "loss is 0.6523178919698758\n",
      "epoch 862 completed\n",
      "loss is 0.6521948258762732\n",
      "epoch 863 completed\n",
      "loss is 0.6520719046046537\n",
      "epoch 864 completed\n",
      "loss is 0.6519491277842198\n",
      "epoch 865 completed\n",
      "loss is 0.6518264950457526\n",
      "epoch 866 completed\n",
      "loss is 0.6517040060216079\n",
      "epoch 867 completed\n",
      "loss is 0.6515816603457096\n",
      "epoch 868 completed\n",
      "loss is 0.6514594576535361\n",
      "epoch 869 completed\n",
      "loss is 0.6513373975821252\n",
      "epoch 870 completed\n",
      "loss is 0.6512154797700538\n",
      "epoch 871 completed\n",
      "loss is 0.6510937038574368\n",
      "epoch 872 completed\n",
      "loss is 0.6509720694859173\n",
      "epoch 873 completed\n",
      "loss is 0.650850576298669\n",
      "epoch 874 completed\n",
      "loss is 0.6507292239403755\n",
      "epoch 875 completed\n",
      "loss is 0.6506080120572328\n",
      "epoch 876 completed\n",
      "loss is 0.650486940296933\n",
      "epoch 877 completed\n",
      "loss is 0.6503660083086696\n",
      "epoch 878 completed\n",
      "loss is 0.6502452157431211\n",
      "epoch 879 completed\n",
      "loss is 0.6501245622524463\n",
      "epoch 880 completed\n",
      "loss is 0.6500040474902826\n",
      "epoch 881 completed\n",
      "loss is 0.6498836711117254\n",
      "epoch 882 completed\n",
      "loss is 0.6497634327733377\n",
      "epoch 883 completed\n",
      "loss is 0.6496433321331356\n",
      "epoch 884 completed\n",
      "loss is 0.6495233688505837\n",
      "epoch 885 completed\n",
      "loss is 0.6494035425865792\n",
      "epoch 886 completed\n",
      "loss is 0.6492838530034596\n",
      "epoch 887 completed\n",
      "loss is 0.6491642997649858\n",
      "epoch 888 completed\n",
      "loss is 0.6490448825363381\n",
      "epoch 889 completed\n",
      "loss is 0.6489256009841102\n",
      "epoch 890 completed\n",
      "loss is 0.6488064547763075\n",
      "epoch 891 completed\n",
      "loss is 0.6486874435823283\n",
      "epoch 892 completed\n",
      "loss is 0.6485685670729692\n",
      "epoch 893 completed\n",
      "loss is 0.6484498249204125\n",
      "epoch 894 completed\n",
      "loss is 0.6483312167982196\n",
      "epoch 895 completed\n",
      "loss is 0.6482127423813324\n",
      "epoch 896 completed\n",
      "loss is 0.6480944013460487\n",
      "epoch 897 completed\n",
      "loss is 0.6479761933700434\n",
      "epoch 898 completed\n",
      "loss is 0.6478581181323311\n",
      "epoch 899 completed\n",
      "loss is 0.6477401753132839\n",
      "epoch 900 completed\n",
      "loss is 0.6476223645946148\n",
      "epoch 901 completed\n",
      "loss is 0.6475046856593702\n",
      "epoch 902 completed\n",
      "loss is 0.6473871381919318\n",
      "epoch 903 completed\n",
      "loss is 0.647269721878002\n",
      "epoch 904 completed\n",
      "loss is 0.647152436404598\n",
      "epoch 905 completed\n",
      "loss is 0.6470352814600548\n",
      "epoch 906 completed\n",
      "loss is 0.6469182567340065\n",
      "epoch 907 completed\n",
      "loss is 0.6468013619173905\n",
      "epoch 908 completed\n",
      "loss is 0.6466845967024378\n",
      "epoch 909 completed\n",
      "loss is 0.6465679607826592\n",
      "epoch 910 completed\n",
      "loss is 0.6464514538528573\n",
      "epoch 911 completed\n",
      "loss is 0.6463350756091004\n",
      "epoch 912 completed\n",
      "loss is 0.6462188257487333\n",
      "epoch 913 completed\n",
      "loss is 0.6461027039703602\n",
      "epoch 914 completed\n",
      "loss is 0.6459867099738459\n",
      "epoch 915 completed\n",
      "loss is 0.6458708434603005\n",
      "epoch 916 completed\n",
      "loss is 0.6457551041320867\n",
      "epoch 917 completed\n",
      "loss is 0.6456394916927994\n",
      "epoch 918 completed\n",
      "loss is 0.6455240058472771\n",
      "epoch 919 completed\n",
      "loss is 0.6454086463015825\n",
      "epoch 920 completed\n",
      "loss is 0.6452934127629956\n",
      "epoch 921 completed\n",
      "loss is 0.6451783049400236\n",
      "epoch 922 completed\n",
      "loss is 0.6450633225423773\n",
      "epoch 923 completed\n",
      "loss is 0.6449484652809743\n",
      "epoch 924 completed\n",
      "loss is 0.6448337328679349\n",
      "epoch 925 completed\n",
      "loss is 0.644719125016571\n",
      "epoch 926 completed\n",
      "loss is 0.6446046414413854\n",
      "epoch 927 completed\n",
      "loss is 0.6444902818580629\n",
      "epoch 928 completed\n",
      "loss is 0.6443760459834684\n",
      "epoch 929 completed\n",
      "loss is 0.6442619335356355\n",
      "epoch 930 completed\n",
      "loss is 0.6441479442337676\n",
      "epoch 931 completed\n",
      "loss is 0.6440340777982303\n",
      "epoch 932 completed\n",
      "loss is 0.6439203339505413\n",
      "epoch 933 completed\n",
      "loss is 0.6438067124133734\n",
      "epoch 934 completed\n",
      "loss is 0.6436932129105396\n",
      "epoch 935 completed\n",
      "loss is 0.6435798351669967\n",
      "epoch 936 completed\n",
      "loss is 0.6434665789088395\n",
      "epoch 937 completed\n",
      "loss is 0.6433534438632816\n",
      "epoch 938 completed\n",
      "loss is 0.6432404297586742\n",
      "epoch 939 completed\n",
      "loss is 0.6431275363244726\n",
      "epoch 940 completed\n",
      "loss is 0.6430147632912601\n",
      "epoch 941 completed\n",
      "loss is 0.64290211039072\n",
      "epoch 942 completed\n",
      "loss is 0.6427895773556417\n",
      "epoch 943 completed\n",
      "loss is 0.6426771639199108\n",
      "epoch 944 completed\n",
      "loss is 0.6425648698185076\n",
      "epoch 945 completed\n",
      "loss is 0.6424526947874989\n",
      "epoch 946 completed\n",
      "loss is 0.6423406385640404\n",
      "epoch 947 completed\n",
      "loss is 0.6422287008863591\n",
      "epoch 948 completed\n",
      "loss is 0.6421168814937586\n",
      "epoch 949 completed\n",
      "loss is 0.6420051801266049\n",
      "epoch 950 completed\n",
      "loss is 0.6418935965263339\n",
      "epoch 951 completed\n",
      "loss is 0.6417821304354355\n",
      "epoch 952 completed\n",
      "loss is 0.6416707815974584\n",
      "epoch 953 completed\n",
      "loss is 0.6415595497569907\n",
      "epoch 954 completed\n",
      "loss is 0.6414484346596688\n",
      "epoch 955 completed\n",
      "loss is 0.6413374360521708\n",
      "epoch 956 completed\n",
      "loss is 0.6412265536822012\n",
      "epoch 957 completed\n",
      "loss is 0.6411157872984988\n",
      "epoch 958 completed\n",
      "loss is 0.641005136650824\n",
      "epoch 959 completed\n",
      "loss is 0.6408946014899591\n",
      "epoch 960 completed\n",
      "loss is 0.640784181567699\n",
      "epoch 961 completed\n",
      "loss is 0.6406738766368502\n",
      "epoch 962 completed\n",
      "loss is 0.640563686451221\n",
      "epoch 963 completed\n",
      "loss is 0.640453610765626\n",
      "epoch 964 completed\n",
      "loss is 0.6403436493358698\n",
      "epoch 965 completed\n",
      "loss is 0.6402338019187537\n",
      "epoch 966 completed\n",
      "loss is 0.6401240682720606\n",
      "epoch 967 completed\n",
      "loss is 0.6400144481545605\n",
      "epoch 968 completed\n",
      "loss is 0.6399049413259973\n",
      "epoch 969 completed\n",
      "loss is 0.6397955475470888\n",
      "epoch 970 completed\n",
      "loss is 0.6396862665795215\n",
      "epoch 971 completed\n",
      "loss is 0.6395770981859507\n",
      "epoch 972 completed\n",
      "loss is 0.6394680421299815\n",
      "epoch 973 completed\n",
      "loss is 0.6393590981761815\n",
      "epoch 974 completed\n",
      "loss is 0.6392502660900659\n",
      "epoch 975 completed\n",
      "loss is 0.6391415456381011\n",
      "epoch 976 completed\n",
      "loss is 0.6390329365876918\n",
      "epoch 977 completed\n",
      "loss is 0.6389244387071797\n",
      "epoch 978 completed\n",
      "loss is 0.6388160517658382\n",
      "epoch 979 completed\n",
      "loss is 0.6387077755338758\n",
      "epoch 980 completed\n",
      "loss is 0.638599609782417\n",
      "epoch 981 completed\n",
      "loss is 0.6384915542835155\n",
      "epoch 982 completed\n",
      "loss is 0.6383836088101356\n",
      "epoch 983 completed\n",
      "loss is 0.6382757731361556\n",
      "epoch 984 completed\n",
      "loss is 0.6381680470363571\n",
      "epoch 985 completed\n",
      "loss is 0.6380604302864329\n",
      "epoch 986 completed\n",
      "loss is 0.6379529226629731\n",
      "epoch 987 completed\n",
      "loss is 0.6378455239434571\n",
      "epoch 988 completed\n",
      "loss is 0.637738233906258\n",
      "epoch 989 completed\n",
      "loss is 0.6376310523306395\n",
      "epoch 990 completed\n",
      "loss is 0.6375239789967441\n",
      "epoch 991 completed\n",
      "loss is 0.6374170136855928\n",
      "epoch 992 completed\n",
      "loss is 0.6373101561790837\n",
      "epoch 993 completed\n",
      "loss is 0.6372034062599838\n",
      "epoch 994 completed\n",
      "loss is 0.6370967637119296\n",
      "epoch 995 completed\n",
      "loss is 0.6369902283194154\n",
      "epoch 996 completed\n",
      "loss is 0.6368837998677952\n",
      "epoch 997 completed\n",
      "loss is 0.6367774781432808\n",
      "epoch 998 completed\n",
      "loss is 0.6366712629329316\n",
      "epoch 999 completed\n",
      "loss is 0.6365651540246554\n",
      "\n",
      "\n",
      "printing network: [<__main__.InputLayer object at 0x000002A80FE0F310>\n",
      " <__main__.Layer object at 0x000002A80FBD2A90>\n",
      " <__main__.OutputLayer object at 0x000002A80FCC47D0>]\n",
      "epoch 0 completed\n",
      "loss is 1.6022250931135418\n",
      "epoch 1 completed\n",
      "loss is 1.596288660470216\n",
      "epoch 2 completed\n",
      "loss is 1.5831392571639413\n",
      "epoch 3 completed\n",
      "loss is 1.5580958743904758\n",
      "epoch 4 completed\n",
      "loss is 1.5230649382583317\n",
      "epoch 5 completed\n",
      "loss is 1.4786011816788796\n",
      "epoch 6 completed\n",
      "loss is 1.4290261782739\n",
      "epoch 7 completed\n",
      "loss is 1.3794667006443364\n",
      "epoch 8 completed\n",
      "loss is 1.3333431791055739\n",
      "epoch 9 completed\n",
      "loss is 1.2920749683402335\n",
      "epoch 10 completed\n",
      "loss is 1.2557605345109675\n",
      "epoch 11 completed\n",
      "loss is 1.2238618179263843\n",
      "epoch 12 completed\n",
      "loss is 1.1956338571470848\n",
      "epoch 13 completed\n",
      "loss is 1.1703433690286458\n",
      "epoch 14 completed\n",
      "loss is 1.1473619261485448\n",
      "epoch 15 completed\n",
      "loss is 1.1261926018199777\n",
      "epoch 16 completed\n",
      "loss is 1.1064634446070678\n",
      "epoch 17 completed\n",
      "loss is 1.0879065605244456\n",
      "epoch 18 completed\n",
      "loss is 1.0703335016944244\n",
      "epoch 19 completed\n",
      "loss is 1.0536125792043332\n",
      "epoch 20 completed\n",
      "loss is 1.037650398526505\n",
      "epoch 21 completed\n",
      "loss is 1.022377953949111\n",
      "epoch 22 completed\n",
      "loss is 1.0077406781124445\n",
      "epoch 23 completed\n",
      "loss is 0.9936915853242939\n",
      "epoch 24 completed\n",
      "loss is 0.9801868306210691\n",
      "epoch 25 completed\n",
      "loss is 0.9671835013116825\n",
      "epoch 26 completed\n",
      "loss is 0.954640017218385\n",
      "epoch 27 completed\n",
      "loss is 0.9425192014210948\n",
      "epoch 28 completed\n",
      "loss is 0.9307916301362692\n",
      "epoch 29 completed\n",
      "loss is 0.9194347449936153\n",
      "epoch 30 completed\n",
      "loss is 0.9084277493039511\n",
      "epoch 31 completed\n",
      "loss is 0.8977486464834081\n",
      "epoch 32 completed\n",
      "loss is 0.8873763355091064\n",
      "epoch 33 completed\n",
      "loss is 0.8772944442245115\n",
      "epoch 34 completed\n",
      "loss is 0.8674936134714119\n",
      "epoch 35 completed\n",
      "loss is 0.8579712960251806\n",
      "epoch 36 completed\n",
      "loss is 0.8487294622230456\n",
      "epoch 37 completed\n",
      "loss is 0.8397713180997166\n",
      "epoch 38 completed\n",
      "loss is 0.8310985233252259\n",
      "epoch 39 completed\n",
      "loss is 0.8227101909921987\n",
      "epoch 40 completed\n",
      "loss is 0.8146040108977705\n",
      "epoch 41 completed\n",
      "loss is 0.8067783790077148\n",
      "epoch 42 completed\n",
      "loss is 0.7992333322432847\n",
      "epoch 43 completed\n",
      "loss is 0.7919688750015432\n",
      "epoch 44 completed\n",
      "loss is 0.7849819999659677\n",
      "epoch 45 completed\n",
      "loss is 0.7782652667648498\n",
      "epoch 46 completed\n",
      "loss is 0.7718079749034457\n",
      "epoch 47 completed\n",
      "loss is 0.7655984257415109\n",
      "epoch 48 completed\n",
      "loss is 0.7596255700858464\n",
      "epoch 49 completed\n",
      "loss is 0.7538796165067251\n",
      "epoch 50 completed\n",
      "loss is 0.7483519952709309\n",
      "epoch 51 completed\n",
      "loss is 0.7430351149765104\n",
      "epoch 52 completed\n",
      "loss is 0.7379221369951717\n",
      "epoch 53 completed\n",
      "loss is 0.73300684084521\n",
      "epoch 54 completed\n",
      "loss is 0.7282835935375617\n",
      "epoch 55 completed\n",
      "loss is 0.7237474232330633\n",
      "epoch 56 completed\n",
      "loss is 0.719394200256105\n",
      "epoch 57 completed\n",
      "loss is 0.7152209313568774\n",
      "epoch 58 completed\n",
      "loss is 0.7112261659657718\n",
      "epoch 59 completed\n",
      "loss is 0.7074104829177988\n",
      "epoch 60 completed\n",
      "loss is 0.7037769521138383\n",
      "epoch 61 completed\n",
      "loss is 0.7003313213297861\n",
      "epoch 62 completed\n",
      "loss is 0.697081456478738\n",
      "epoch 63 completed\n",
      "loss is 0.6940353658717123\n",
      "epoch 64 completed\n",
      "loss is 0.6911973399516498\n",
      "epoch 65 completed\n",
      "loss is 0.688562920486338\n",
      "epoch 66 completed\n",
      "loss is 0.6861153759089079\n",
      "epoch 67 completed\n",
      "loss is 0.6838268926067821\n",
      "epoch 68 completed\n",
      "loss is 0.6816647335426389\n",
      "epoch 69 completed\n",
      "loss is 0.6795987035209052\n",
      "epoch 70 completed\n",
      "loss is 0.6776059434536885\n",
      "epoch 71 completed\n",
      "loss is 0.6756719716750889\n",
      "epoch 72 completed\n",
      "loss is 0.6737892606624222\n",
      "epoch 73 completed\n",
      "loss is 0.6719550118141485\n",
      "epoch 74 completed\n",
      "loss is 0.6701691358111759\n",
      "epoch 75 completed\n",
      "loss is 0.6684327861843019\n",
      "epoch 76 completed\n",
      "loss is 0.6667474364985561\n",
      "epoch 77 completed\n",
      "loss is 0.6651143694402059\n",
      "epoch 78 completed\n",
      "loss is 0.6635344383626754\n",
      "epoch 79 completed\n",
      "loss is 0.66200799235549\n",
      "epoch 80 completed\n",
      "loss is 0.6605348906227377\n",
      "epoch 81 completed\n",
      "loss is 0.6591145597502746\n",
      "epoch 82 completed\n",
      "loss is 0.6577460667544784\n",
      "epoch 83 completed\n",
      "loss is 0.6564281931891024\n",
      "epoch 84 completed\n",
      "loss is 0.655159503083244\n",
      "epoch 85 completed\n",
      "loss is 0.6539384017939844\n",
      "epoch 86 completed\n",
      "loss is 0.6527631851958522\n",
      "epoch 87 completed\n",
      "loss is 0.6516320797968392\n",
      "epoch 88 completed\n",
      "loss is 0.650543274870206\n",
      "epoch 89 completed\n",
      "loss is 0.6494949478276273\n",
      "epoch 90 completed\n",
      "loss is 0.6484852840113999\n",
      "epoch 91 completed\n",
      "loss is 0.6475124919531743\n",
      "epoch 92 completed\n",
      "loss is 0.6465748149897432\n",
      "epoch 93 completed\n",
      "loss is 0.645670539971367\n",
      "epoch 94 completed\n",
      "loss is 0.6447980036583703\n",
      "epoch 95 completed\n",
      "loss is 0.6439555972822051\n",
      "epoch 96 completed\n",
      "loss is 0.6431417696483631\n",
      "epoch 97 completed\n",
      "loss is 0.6423550290787764\n",
      "epoch 98 completed\n",
      "loss is 0.6415939444282325\n",
      "epoch 99 completed\n",
      "loss is 0.6408571453600538\n",
      "epoch 100 completed\n",
      "loss is 0.6401433220283189\n",
      "epoch 101 completed\n",
      "loss is 0.6394512242848815\n",
      "epoch 102 completed\n",
      "loss is 0.6387796605074451\n",
      "epoch 103 completed\n",
      "loss is 0.6381274961282358\n",
      "epoch 104 completed\n",
      "loss is 0.6374936519301153\n",
      "epoch 105 completed\n",
      "loss is 0.6368771021671589\n",
      "epoch 106 completed\n",
      "loss is 0.6362768725589963\n",
      "epoch 107 completed\n",
      "loss is 0.6356920382018773\n",
      "epoch 108 completed\n",
      "loss is 0.6351217214340563\n",
      "epoch 109 completed\n",
      "loss is 0.6345650896883762\n",
      "epoch 110 completed\n",
      "loss is 0.6340213533605592\n",
      "epoch 111 completed\n",
      "loss is 0.6334897637176524\n",
      "epoch 112 completed\n",
      "loss is 0.6329696108671854\n",
      "epoch 113 completed\n",
      "loss is 0.6324602218038659\n",
      "epoch 114 completed\n",
      "loss is 0.6319609585470827\n",
      "epoch 115 completed\n",
      "loss is 0.6314712163791044\n",
      "epoch 116 completed\n",
      "loss is 0.6309904221906978\n",
      "epoch 117 completed\n",
      "loss is 0.6305180329379657\n",
      "epoch 118 completed\n",
      "loss is 0.6300535342115553\n",
      "epoch 119 completed\n",
      "loss is 0.6295964389170258\n",
      "epoch 120 completed\n",
      "loss is 0.6291462860631055\n",
      "epoch 121 completed\n",
      "loss is 0.6287026396528316\n",
      "epoch 122 completed\n",
      "loss is 0.6282650876710995\n",
      "epoch 123 completed\n",
      "loss is 0.6278332411610462\n",
      "epoch 124 completed\n",
      "loss is 0.6274067333808069\n",
      "epoch 125 completed\n",
      "loss is 0.6269852190316003\n",
      "epoch 126 completed\n",
      "loss is 0.6265683735477714\n",
      "epoch 127 completed\n",
      "loss is 0.626155892439285\n",
      "epoch 128 completed\n",
      "loss is 0.6257474906772499\n",
      "epoch 129 completed\n",
      "loss is 0.6253429021133307\n",
      "epoch 130 completed\n",
      "loss is 0.6249418789242946\n",
      "epoch 131 completed\n",
      "loss is 0.6245441910735101\n",
      "epoch 132 completed\n",
      "loss is 0.6241496257818839\n",
      "epoch 133 completed\n",
      "loss is 0.6237579870014347\n",
      "epoch 134 completed\n",
      "loss is 0.6233690948855878\n",
      "epoch 135 completed\n",
      "loss is 0.6229827852511173\n",
      "epoch 136 completed\n",
      "loss is 0.6225989090275823\n",
      "epoch 137 completed\n",
      "loss is 0.6222173316910578\n",
      "epoch 138 completed\n",
      "loss is 0.62183793267989\n",
      "epoch 139 completed\n",
      "loss is 0.6214606047911228\n",
      "epoch 140 completed\n",
      "loss is 0.6210852535571445\n",
      "epoch 141 completed\n",
      "loss is 0.6207117966029793\n",
      "epoch 142 completed\n",
      "loss is 0.6203401629853743\n",
      "epoch 143 completed\n",
      "loss is 0.6199702925156545\n",
      "epoch 144 completed\n",
      "loss is 0.6196021350688532\n",
      "epoch 145 completed\n",
      "loss is 0.6192356498822713\n",
      "epoch 146 completed\n",
      "loss is 0.6188708048470155\n",
      "epoch 147 completed\n",
      "loss is 0.6185075757964253\n",
      "epoch 148 completed\n",
      "loss is 0.6181459457955797\n",
      "epoch 149 completed\n",
      "loss is 0.6177859044361564\n",
      "epoch 150 completed\n",
      "loss is 0.6174274471410222\n",
      "epoch 151 completed\n",
      "loss is 0.6170705744828068\n",
      "epoch 152 completed\n",
      "loss is 0.6167152915206071\n",
      "epoch 153 completed\n",
      "loss is 0.6163616071587125\n",
      "epoch 154 completed\n",
      "loss is 0.6160095335309294\n",
      "epoch 155 completed\n",
      "loss is 0.6156590854137257\n",
      "epoch 156 completed\n",
      "loss is 0.6153102796709826\n",
      "epoch 157 completed\n",
      "loss is 0.6149631347327058\n",
      "epoch 158 completed\n",
      "loss is 0.6146176701095357\n",
      "epoch 159 completed\n",
      "loss is 0.6142739059444661\n",
      "epoch 160 completed\n",
      "loss is 0.6139318626026208\n",
      "epoch 161 completed\n",
      "loss is 0.6135915602995423\n",
      "epoch 162 completed\n",
      "loss is 0.6132530187679287\n",
      "epoch 163 completed\n",
      "loss is 0.6129162569623864\n",
      "epoch 164 completed\n",
      "loss is 0.6125812928013513\n",
      "epoch 165 completed\n",
      "loss is 0.6122481429450077\n",
      "epoch 166 completed\n",
      "loss is 0.6119168226077292\n",
      "epoch 167 completed\n",
      "loss is 0.6115873454033269\n",
      "epoch 168 completed\n",
      "loss is 0.6112597232211865\n",
      "epoch 169 completed\n",
      "loss is 0.6109339661312204\n",
      "epoch 170 completed\n",
      "loss is 0.6106100823154618\n",
      "epoch 171 completed\n",
      "loss is 0.6102880780240499\n",
      "epoch 172 completed\n",
      "loss is 0.609967957553339\n",
      "epoch 173 completed\n",
      "loss is 0.6096497232438777\n",
      "epoch 174 completed\n",
      "loss is 0.6093333754960236\n",
      "epoch 175 completed\n",
      "loss is 0.6090189128010397\n",
      "epoch 176 completed\n",
      "loss is 0.6087063317856086\n",
      "epoch 177 completed\n",
      "loss is 0.6083956272677926\n",
      "epoch 178 completed\n",
      "loss is 0.6080867923225849\n",
      "epoch 179 completed\n",
      "loss is 0.6077798183553482\n",
      "epoch 180 completed\n",
      "loss is 0.6074746951815297\n",
      "epoch 181 completed\n",
      "loss is 0.6071714111112191\n",
      "epoch 182 completed\n",
      "loss is 0.6068699530372248\n",
      "epoch 183 completed\n",
      "loss is 0.6065703065254797\n",
      "epoch 184 completed\n",
      "loss is 0.6062724559067529\n",
      "epoch 185 completed\n",
      "loss is 0.6059763843686966\n",
      "epoch 186 completed\n",
      "loss is 0.6056820740474708\n",
      "epoch 187 completed\n",
      "loss is 0.6053895061182213\n",
      "epoch 188 completed\n",
      "loss is 0.6050986608838237\n",
      "epoch 189 completed\n",
      "loss is 0.6048095178613989\n",
      "epoch 190 completed\n",
      "loss is 0.6045220558662135\n",
      "epoch 191 completed\n",
      "loss is 0.6042362530925932\n",
      "epoch 192 completed\n",
      "loss is 0.6039520871916331\n",
      "epoch 193 completed\n",
      "loss is 0.6036695353454951\n",
      "epoch 194 completed\n",
      "loss is 0.6033885743381413\n",
      "epoch 195 completed\n",
      "loss is 0.6031091806224399\n",
      "epoch 196 completed\n",
      "loss is 0.602831330383575\n",
      "epoch 197 completed\n",
      "loss is 0.6025549995987703\n",
      "epoch 198 completed\n",
      "loss is 0.6022801640933257\n",
      "epoch 199 completed\n",
      "loss is 0.6020067995930425\n",
      "epoch 200 completed\n",
      "loss is 0.6017348817730825\n",
      "epoch 201 completed\n",
      "loss is 0.601464386303364\n",
      "epoch 202 completed\n",
      "loss is 0.6011952888905973\n",
      "epoch 203 completed\n",
      "loss is 0.6009275653170622\n",
      "epoch 204 completed\n",
      "loss is 0.6006611914762721\n",
      "epoch 205 completed\n",
      "loss is 0.600396143405623\n",
      "epoch 206 completed\n",
      "loss is 0.6001323973162005\n",
      "epoch 207 completed\n",
      "loss is 0.5998699296198384\n",
      "epoch 208 completed\n",
      "loss is 0.5996087169536023\n",
      "epoch 209 completed\n",
      "loss is 0.5993487362018106\n",
      "epoch 210 completed\n",
      "loss is 0.5990899645157456\n",
      "epoch 211 completed\n",
      "loss is 0.5988323793311717\n",
      "epoch 212 completed\n",
      "loss is 0.598575958383807\n",
      "epoch 213 completed\n",
      "loss is 0.5983206797228665\n",
      "epoch 214 completed\n",
      "loss is 0.5980665217227984\n",
      "epoch 215 completed\n",
      "loss is 0.5978134630933435\n",
      "epoch 216 completed\n",
      "loss is 0.5975614828880188\n",
      "epoch 217 completed\n",
      "loss is 0.5973105605111452\n",
      "epoch 218 completed\n",
      "loss is 0.5970606757235195\n",
      "epoch 219 completed\n",
      "loss is 0.5968118086468345\n",
      "epoch 220 completed\n",
      "loss is 0.5965639397669369\n",
      "epoch 221 completed\n",
      "loss is 0.5963170499360225\n",
      "epoch 222 completed\n",
      "loss is 0.5960711203738498\n",
      "epoch 223 completed\n",
      "loss is 0.5958261326680461\n",
      "epoch 224 completed\n",
      "loss is 0.5955820687736024\n",
      "epoch 225 completed\n",
      "loss is 0.5953389110116112\n",
      "epoch 226 completed\n",
      "loss is 0.595096642067318\n",
      "epoch 227 completed\n",
      "loss is 0.5948552449875653\n",
      "epoch 228 completed\n",
      "loss is 0.59461470317767\n",
      "epoch 229 completed\n",
      "loss is 0.594375000397801\n",
      "epoch 230 completed\n",
      "loss is 0.5941361207589113\n",
      "epoch 231 completed\n",
      "loss is 0.5938980487182624\n",
      "epoch 232 completed\n",
      "loss is 0.5936607690746045\n",
      "epoch 233 completed\n",
      "loss is 0.5934242669630397\n",
      "epoch 234 completed\n",
      "loss is 0.5931885278496128\n",
      "epoch 235 completed\n",
      "loss is 0.5929535375256657\n",
      "epoch 236 completed\n",
      "loss is 0.5927192821020014\n",
      "epoch 237 completed\n",
      "loss is 0.5924857480028634\n",
      "epoch 238 completed\n",
      "loss is 0.5922529219597865\n",
      "epoch 239 completed\n",
      "loss is 0.5920207910053313\n",
      "epoch 240 completed\n",
      "loss is 0.5917893424667297\n",
      "epoch 241 completed\n",
      "loss is 0.5915585639594637\n",
      "epoch 242 completed\n",
      "loss is 0.5913284433807996\n",
      "epoch 243 completed\n",
      "loss is 0.5910989689033034\n",
      "epoch 244 completed\n",
      "loss is 0.5908701289683367\n",
      "epoch 245 completed\n",
      "loss is 0.590641912279572\n",
      "epoch 246 completed\n",
      "loss is 0.5904143077965206\n",
      "epoch 247 completed\n",
      "loss is 0.5901873047281064\n",
      "epoch 248 completed\n",
      "loss is 0.5899608925262787\n",
      "epoch 249 completed\n",
      "loss is 0.5897350608796931\n",
      "epoch 250 completed\n",
      "loss is 0.5895097997074461\n",
      "epoch 251 completed\n",
      "loss is 0.5892850991529087\n",
      "epoch 252 completed\n",
      "loss is 0.589060949577626\n",
      "epoch 253 completed\n",
      "loss is 0.5888373415553187\n",
      "epoch 254 completed\n",
      "loss is 0.5886142658659861\n",
      "epoch 255 completed\n",
      "loss is 0.5883917134900981\n",
      "epoch 256 completed\n",
      "loss is 0.5881696756029219\n",
      "epoch 257 completed\n",
      "loss is 0.5879481435689412\n",
      "epoch 258 completed\n",
      "loss is 0.5877271089363953\n",
      "epoch 259 completed\n",
      "loss is 0.5875065634319536\n",
      "epoch 260 completed\n",
      "loss is 0.5872864989554947\n",
      "epoch 261 completed\n",
      "loss is 0.587066907575022\n",
      "epoch 262 completed\n",
      "loss is 0.5868477815217079\n",
      "epoch 263 completed\n",
      "loss is 0.5866291131850581\n",
      "epoch 264 completed\n",
      "loss is 0.5864108951082144\n",
      "epoch 265 completed\n",
      "loss is 0.586193119983384\n",
      "epoch 266 completed\n",
      "loss is 0.5859757806474011\n",
      "epoch 267 completed\n",
      "loss is 0.585758870077422\n",
      "epoch 268 completed\n",
      "loss is 0.5855423813867466\n",
      "epoch 269 completed\n",
      "loss is 0.5853263078207754\n",
      "epoch 270 completed\n",
      "loss is 0.5851106427530987\n",
      "epoch 271 completed\n",
      "loss is 0.5848953796817031\n",
      "epoch 272 completed\n",
      "loss is 0.5846805122253289\n",
      "epoch 273 completed\n",
      "loss is 0.5844660341199288\n",
      "epoch 274 completed\n",
      "loss is 0.5842519392152743\n",
      "epoch 275 completed\n",
      "loss is 0.5840382214716716\n",
      "epoch 276 completed\n",
      "loss is 0.5838248749568092\n",
      "epoch 277 completed\n",
      "loss is 0.5836118938427238\n",
      "epoch 278 completed\n",
      "loss is 0.5833992724028781\n",
      "epoch 279 completed\n",
      "loss is 0.583187005009368\n",
      "epoch 280 completed\n",
      "loss is 0.5829750861302291\n",
      "epoch 281 completed\n",
      "loss is 0.582763510326869\n",
      "epoch 282 completed\n",
      "loss is 0.582552272251595\n",
      "epoch 283 completed\n",
      "loss is 0.58234136664526\n",
      "epoch 284 completed\n",
      "loss is 0.5821307883350015\n",
      "epoch 285 completed\n",
      "loss is 0.581920532232091\n",
      "epoch 286 completed\n",
      "loss is 0.5817105933298662\n",
      "epoch 287 completed\n",
      "loss is 0.5815009667017833\n",
      "epoch 288 completed\n",
      "loss is 0.5812916474995294\n",
      "epoch 289 completed\n",
      "loss is 0.5810826309512436\n",
      "epoch 290 completed\n",
      "loss is 0.5808739123598197\n",
      "epoch 291 completed\n",
      "loss is 0.5806654871012911\n",
      "epoch 292 completed\n",
      "loss is 0.5804573506232842\n",
      "epoch 293 completed\n",
      "loss is 0.5802494984435608\n",
      "epoch 294 completed\n",
      "loss is 0.5800419261486247\n",
      "epoch 295 completed\n",
      "loss is 0.5798346293923993\n",
      "epoch 296 completed\n",
      "loss is 0.5796276038949705\n",
      "epoch 297 completed\n",
      "loss is 0.5794208454413896\n",
      "epoch 298 completed\n",
      "loss is 0.5792143498805367\n",
      "epoch 299 completed\n",
      "loss is 0.579008113124044\n",
      "epoch 300 completed\n",
      "loss is 0.5788021311452635\n",
      "epoch 301 completed\n",
      "loss is 0.5785963999782897\n",
      "epoch 302 completed\n",
      "loss is 0.5783909157170295\n",
      "epoch 303 completed\n",
      "loss is 0.5781856745143148\n",
      "epoch 304 completed\n",
      "loss is 0.5779806725810516\n",
      "epoch 305 completed\n",
      "loss is 0.577775906185415\n",
      "epoch 306 completed\n",
      "loss is 0.5775713716520752\n",
      "epoch 307 completed\n",
      "loss is 0.5773670653614602\n",
      "epoch 308 completed\n",
      "loss is 0.5771629837490385\n",
      "epoch 309 completed\n",
      "loss is 0.5769591233046495\n",
      "epoch 310 completed\n",
      "loss is 0.5767554805718405\n",
      "epoch 311 completed\n",
      "loss is 0.576552052147237\n",
      "epoch 312 completed\n",
      "loss is 0.5763488346799396\n",
      "epoch 313 completed\n",
      "loss is 0.5761458248709325\n",
      "epoch 314 completed\n",
      "loss is 0.5759430194725128\n",
      "epoch 315 completed\n",
      "loss is 0.5757404152877442\n",
      "epoch 316 completed\n",
      "loss is 0.5755380091699173\n",
      "epoch 317 completed\n",
      "loss is 0.5753357980220293\n",
      "epoch 318 completed\n",
      "loss is 0.575133778796276\n",
      "epoch 319 completed\n",
      "loss is 0.5749319484935566\n",
      "epoch 320 completed\n",
      "loss is 0.5747303041629892\n",
      "epoch 321 completed\n",
      "loss is 0.5745288429014401\n",
      "epoch 322 completed\n",
      "loss is 0.5743275618530548\n",
      "epoch 323 completed\n",
      "loss is 0.5741264582088133\n",
      "epoch 324 completed\n",
      "loss is 0.5739255292060752\n",
      "epoch 325 completed\n",
      "loss is 0.5737247721281495\n",
      "epoch 326 completed\n",
      "loss is 0.5735241843038649\n",
      "epoch 327 completed\n",
      "loss is 0.5733237631071516\n",
      "epoch 328 completed\n",
      "loss is 0.573123505956629\n",
      "epoch 329 completed\n",
      "loss is 0.5729234103152026\n",
      "epoch 330 completed\n",
      "loss is 0.5727234736896667\n",
      "epoch 331 completed\n",
      "loss is 0.5725236936303222\n",
      "epoch 332 completed\n",
      "loss is 0.5723240677305914\n",
      "epoch 333 completed\n",
      "loss is 0.5721245936266607\n",
      "epoch 334 completed\n",
      "loss is 0.5719252689971037\n",
      "epoch 335 completed\n",
      "loss is 0.5717260915625431\n",
      "epoch 336 completed\n",
      "loss is 0.5715270590853052\n",
      "epoch 337 completed\n",
      "loss is 0.5713281693690867\n",
      "epoch 338 completed\n",
      "loss is 0.5711294202586382\n",
      "epoch 339 completed\n",
      "loss is 0.5709308096394525\n",
      "epoch 340 completed\n",
      "loss is 0.5707323354374673\n",
      "epoch 341 completed\n",
      "loss is 0.5705339956187775\n",
      "epoch 342 completed\n",
      "loss is 0.5703357881893637\n",
      "epoch 343 completed\n",
      "loss is 0.57013771119483\n",
      "epoch 344 completed\n",
      "loss is 0.5699397627201591\n",
      "epoch 345 completed\n",
      "loss is 0.5697419408894725\n",
      "epoch 346 completed\n",
      "loss is 0.5695442438658166\n",
      "epoch 347 completed\n",
      "loss is 0.5693466698509537\n",
      "epoch 348 completed\n",
      "loss is 0.5691492170851694\n",
      "epoch 349 completed\n",
      "loss is 0.5689518838471075\n",
      "epoch 350 completed\n",
      "loss is 0.5687546684535938\n",
      "epoch 351 completed\n",
      "loss is 0.5685575692595044\n",
      "epoch 352 completed\n",
      "loss is 0.5683605846576301\n",
      "epoch 353 completed\n",
      "loss is 0.5681637130785702\n",
      "epoch 354 completed\n",
      "loss is 0.5679669529906286\n",
      "epoch 355 completed\n",
      "loss is 0.5677703028997395\n",
      "epoch 356 completed\n",
      "loss is 0.5675737613494021\n",
      "epoch 357 completed\n",
      "loss is 0.5673773269206319\n",
      "epoch 358 completed\n",
      "loss is 0.5671809982319344\n",
      "epoch 359 completed\n",
      "loss is 0.5669847739392889\n",
      "epoch 360 completed\n",
      "loss is 0.5667886527361501\n",
      "epoch 361 completed\n",
      "loss is 0.5665926333534668\n",
      "epoch 362 completed\n",
      "loss is 0.5663967145597202\n",
      "epoch 363 completed\n",
      "loss is 0.5662008951609673\n",
      "epoch 364 completed\n",
      "loss is 0.566005174000915\n",
      "epoch 365 completed\n",
      "loss is 0.5658095499609992\n",
      "epoch 366 completed\n",
      "loss is 0.5656140219604789\n",
      "epoch 367 completed\n",
      "loss is 0.5654185889565523\n",
      "epoch 368 completed\n",
      "loss is 0.5652232499444804\n",
      "epoch 369 completed\n",
      "loss is 0.5650280039577287\n",
      "epoch 370 completed\n",
      "loss is 0.5648328500681179\n",
      "epoch 371 completed\n",
      "loss is 0.5646377873859956\n",
      "epoch 372 completed\n",
      "loss is 0.5644428150604158\n",
      "epoch 373 completed\n",
      "loss is 0.564247932279327\n",
      "epoch 374 completed\n",
      "loss is 0.5640531382697807\n",
      "epoch 375 completed\n",
      "loss is 0.5638584322981425\n",
      "epoch 376 completed\n",
      "loss is 0.563663813670322\n",
      "epoch 377 completed\n",
      "loss is 0.5634692817320089\n",
      "epoch 378 completed\n",
      "loss is 0.5632748358689142\n",
      "epoch 379 completed\n",
      "loss is 0.5630804755070293\n",
      "epoch 380 completed\n",
      "loss is 0.5628862001128887\n",
      "epoch 381 completed\n",
      "loss is 0.5626920091938388\n",
      "epoch 382 completed\n",
      "loss is 0.5624979022983184\n",
      "epoch 383 completed\n",
      "loss is 0.5623038790161446\n",
      "epoch 384 completed\n",
      "loss is 0.562109938978799\n",
      "epoch 385 completed\n",
      "loss is 0.56191608185973\n",
      "epoch 386 completed\n",
      "loss is 0.5617223073746498\n",
      "epoch 387 completed\n",
      "loss is 0.561528615281842\n",
      "epoch 388 completed\n",
      "loss is 0.5613350053824726\n",
      "epoch 389 completed\n",
      "loss is 0.5611414775208953\n",
      "epoch 390 completed\n",
      "loss is 0.5609480315849701\n",
      "epoch 391 completed\n",
      "loss is 0.5607546675063824\n",
      "epoch 392 completed\n",
      "loss is 0.5605613852609537\n",
      "epoch 393 completed\n",
      "loss is 0.5603681848689621\n",
      "epoch 394 completed\n",
      "loss is 0.5601750663954613\n",
      "epoch 395 completed\n",
      "loss is 0.5599820299505971\n",
      "epoch 396 completed\n",
      "loss is 0.5597890756899211\n",
      "epoch 397 completed\n",
      "loss is 0.5595962038147045\n",
      "epoch 398 completed\n",
      "loss is 0.5594034145722533\n",
      "epoch 399 completed\n",
      "loss is 0.5592107082562119\n",
      "epoch 400 completed\n",
      "loss is 0.559018085206872\n",
      "epoch 401 completed\n",
      "loss is 0.5588255458114724\n",
      "epoch 402 completed\n",
      "loss is 0.5586330905044913\n",
      "epoch 403 completed\n",
      "loss is 0.5584407197679421\n",
      "epoch 404 completed\n",
      "loss is 0.5582484341316587\n",
      "epoch 405 completed\n",
      "loss is 0.5580562341735709\n",
      "epoch 406 completed\n",
      "loss is 0.5578641205199791\n",
      "epoch 407 completed\n",
      "loss is 0.5576720938458197\n",
      "epoch 408 completed\n",
      "loss is 0.5574801548749199\n",
      "epoch 409 completed\n",
      "loss is 0.557288304380248\n",
      "epoch 410 completed\n",
      "loss is 0.5570965431841556\n",
      "epoch 411 completed\n",
      "loss is 0.5569048721586012\n",
      "epoch 412 completed\n",
      "loss is 0.5567132922253838\n",
      "epoch 413 completed\n",
      "loss is 0.5565218043563354\n",
      "epoch 414 completed\n",
      "loss is 0.5563304095735384\n",
      "epoch 415 completed\n",
      "loss is 0.5561391089495029\n",
      "epoch 416 completed\n",
      "loss is 0.5559479036073508\n",
      "epoch 417 completed\n",
      "loss is 0.5557567947209747\n",
      "epoch 418 completed\n",
      "loss is 0.555565783515192\n",
      "epoch 419 completed\n",
      "loss is 0.5553748712658846\n",
      "epoch 420 completed\n",
      "loss is 0.5551840593001242\n",
      "epoch 421 completed\n",
      "loss is 0.5549933489962819\n",
      "epoch 422 completed\n",
      "loss is 0.5548027417841271\n",
      "epoch 423 completed\n",
      "loss is 0.5546122391449075\n",
      "epoch 424 completed\n",
      "loss is 0.5544218426114158\n",
      "epoch 425 completed\n",
      "loss is 0.554231553768043\n",
      "epoch 426 completed\n",
      "loss is 0.5540413742508082\n",
      "epoch 427 completed\n",
      "loss is 0.5538513057473834\n",
      "epoch 428 completed\n",
      "loss is 0.5536613499970832\n",
      "epoch 429 completed\n",
      "loss is 0.5534715087908576\n",
      "epoch 430 completed\n",
      "loss is 0.553281783971258\n",
      "epoch 431 completed\n",
      "loss is 0.5530921774323713\n",
      "epoch 432 completed\n",
      "loss is 0.5529026911197655\n",
      "epoch 433 completed\n",
      "loss is 0.5527133270303826\n",
      "epoch 434 completed\n",
      "loss is 0.552524087212438\n",
      "epoch 435 completed\n",
      "loss is 0.5523349737652872\n",
      "epoch 436 completed\n",
      "loss is 0.552145988839268\n",
      "epoch 437 completed\n",
      "loss is 0.5519571346355424\n",
      "epoch 438 completed\n",
      "loss is 0.551768413405888\n",
      "epoch 439 completed\n",
      "loss is 0.5515798274524931\n",
      "epoch 440 completed\n",
      "loss is 0.5513913791277203\n",
      "epoch 441 completed\n",
      "loss is 0.5512030708338401\n",
      "epoch 442 completed\n",
      "loss is 0.5510149050227582\n",
      "epoch 443 completed\n",
      "loss is 0.5508268841957052\n",
      "epoch 444 completed\n",
      "loss is 0.5506390109029109\n",
      "epoch 445 completed\n",
      "loss is 0.5504512877432571\n",
      "epoch 446 completed\n",
      "loss is 0.5502637173638931\n",
      "epoch 447 completed\n",
      "loss is 0.5500763024598476\n",
      "epoch 448 completed\n",
      "loss is 0.5498890457735988\n",
      "epoch 449 completed\n",
      "loss is 0.5497019500946236\n",
      "epoch 450 completed\n",
      "loss is 0.5495150182589333\n",
      "epoch 451 completed\n",
      "loss is 0.5493282531485639\n",
      "epoch 452 completed\n",
      "loss is 0.5491416576910672\n",
      "epoch 453 completed\n",
      "loss is 0.5489552348589443\n",
      "epoch 454 completed\n",
      "loss is 0.5487689876690833\n",
      "epoch 455 completed\n",
      "loss is 0.5485829191821547\n",
      "epoch 456 completed\n",
      "loss is 0.5483970325019798\n",
      "epoch 457 completed\n",
      "loss is 0.5482113307748824\n",
      "epoch 458 completed\n",
      "loss is 0.5480258171890126\n",
      "epoch 459 completed\n",
      "loss is 0.5478404949736294\n",
      "epoch 460 completed\n",
      "loss is 0.5476553673983804\n",
      "epoch 461 completed\n",
      "loss is 0.547470437772535\n",
      "epoch 462 completed\n",
      "loss is 0.5472857094442021\n",
      "epoch 463 completed\n",
      "loss is 0.5471011857995202\n",
      "epoch 464 completed\n",
      "loss is 0.5469168702618141\n",
      "epoch 465 completed\n",
      "loss is 0.5467327662907383\n",
      "epoch 466 completed\n",
      "loss is 0.546548877381373\n",
      "epoch 467 completed\n",
      "loss is 0.5463652070633241\n",
      "epoch 468 completed\n",
      "loss is 0.5461817588997662\n",
      "epoch 469 completed\n",
      "loss is 0.5459985364864821\n",
      "epoch 470 completed\n",
      "loss is 0.5458155434508624\n",
      "epoch 471 completed\n",
      "loss is 0.5456327834508922\n",
      "epoch 472 completed\n",
      "loss is 0.5454502601740996\n",
      "epoch 473 completed\n",
      "loss is 0.5452679773364933\n",
      "epoch 474 completed\n",
      "loss is 0.5450859386814589\n",
      "epoch 475 completed\n",
      "loss is 0.5449041479786475\n",
      "epoch 476 completed\n",
      "loss is 0.5447226090228334\n",
      "epoch 477 completed\n",
      "loss is 0.5445413256327467\n",
      "epoch 478 completed\n",
      "loss is 0.5443603016498835\n",
      "epoch 479 completed\n",
      "loss is 0.5441795409372969\n",
      "epoch 480 completed\n",
      "loss is 0.5439990473783625\n",
      "epoch 481 completed\n",
      "loss is 0.5438188248755276\n",
      "epoch 482 completed\n",
      "loss is 0.5436388773490294\n",
      "epoch 483 completed\n",
      "loss is 0.5434592087356116\n",
      "epoch 484 completed\n",
      "loss is 0.5432798229871957\n",
      "epoch 485 completed\n",
      "loss is 0.5431007240695572\n",
      "epoch 486 completed\n",
      "loss is 0.542921915960967\n",
      "epoch 487 completed\n",
      "loss is 0.5427434026508319\n",
      "epoch 488 completed\n",
      "loss is 0.5425651881382955\n",
      "epoch 489 completed\n",
      "loss is 0.5423872764308478\n",
      "epoch 490 completed\n",
      "loss is 0.542209671542901\n",
      "epoch 491 completed\n",
      "loss is 0.5420323774943654\n",
      "epoch 492 completed\n",
      "loss is 0.5418553983091976\n",
      "epoch 493 completed\n",
      "loss is 0.5416787380139486\n",
      "epoch 494 completed\n",
      "loss is 0.5415024006362954\n",
      "epoch 495 completed\n",
      "loss is 0.5413263902035622\n",
      "epoch 496 completed\n",
      "loss is 0.5411507107412304\n",
      "epoch 497 completed\n",
      "loss is 0.5409753662714493\n",
      "epoch 498 completed\n",
      "loss is 0.5408003608115254\n",
      "epoch 499 completed\n",
      "loss is 0.5406256983724165\n",
      "epoch 500 completed\n",
      "loss is 0.5404513829572197\n",
      "epoch 501 completed\n",
      "loss is 0.5402774185596443\n",
      "epoch 502 completed\n",
      "loss is 0.5401038091625072\n",
      "epoch 503 completed\n",
      "loss is 0.5399305587361932\n",
      "epoch 504 completed\n",
      "loss is 0.5397576712371424\n",
      "epoch 505 completed\n",
      "loss is 0.5395851506063304\n",
      "epoch 506 completed\n",
      "loss is 0.5394130007677386\n",
      "epoch 507 completed\n",
      "loss is 0.5392412256268552\n",
      "epoch 508 completed\n",
      "loss is 0.5390698290691509\n",
      "epoch 509 completed\n",
      "loss is 0.5388988149585807\n",
      "epoch 510 completed\n",
      "loss is 0.5387281871360887\n",
      "epoch 511 completed\n",
      "loss is 0.5385579494181209\n",
      "epoch 512 completed\n",
      "loss is 0.5383881055951453\n",
      "epoch 513 completed\n",
      "loss is 0.5382186594301925\n",
      "epoch 514 completed\n",
      "loss is 0.5380496146573991\n",
      "epoch 515 completed\n",
      "loss is 0.5378809749805799\n",
      "epoch 516 completed\n",
      "loss is 0.5377127440717956\n",
      "epoch 517 completed\n",
      "loss is 0.5375449255699661\n",
      "epoch 518 completed\n",
      "loss is 0.5373775230794697\n",
      "epoch 519 completed\n",
      "loss is 0.5372105401688\n",
      "epoch 520 completed\n",
      "loss is 0.5370439803692151\n",
      "epoch 521 completed\n",
      "loss is 0.5368778471734185\n",
      "epoch 522 completed\n",
      "loss is 0.5367121440342764\n",
      "epoch 523 completed\n",
      "loss is 0.5365468743635518\n",
      "epoch 524 completed\n",
      "loss is 0.5363820415306673\n",
      "epoch 525 completed\n",
      "loss is 0.5362176488614974\n",
      "epoch 526 completed\n",
      "loss is 0.5360536996371973\n",
      "epoch 527 completed\n",
      "loss is 0.535890197093061\n",
      "epoch 528 completed\n",
      "loss is 0.5357271444174109\n",
      "epoch 529 completed\n",
      "loss is 0.535564544750523\n",
      "epoch 530 completed\n",
      "loss is 0.5354024011836065\n",
      "epoch 531 completed\n",
      "loss is 0.5352407167577827\n",
      "epoch 532 completed\n",
      "loss is 0.535079494463151\n",
      "epoch 533 completed\n",
      "loss is 0.5349187372378648\n",
      "epoch 534 completed\n",
      "loss is 0.5347584479672561\n",
      "epoch 535 completed\n",
      "loss is 0.5345986294830108\n",
      "epoch 536 completed\n",
      "loss is 0.534439284562386\n",
      "epoch 537 completed\n",
      "loss is 0.5342804159274688\n",
      "epoch 538 completed\n",
      "loss is 0.5341220262444942\n",
      "epoch 539 completed\n",
      "loss is 0.5339641181231988\n",
      "epoch 540 completed\n",
      "loss is 0.53380669411623\n",
      "epoch 541 completed\n",
      "loss is 0.5336497567186184\n",
      "epoch 542 completed\n",
      "loss is 0.5334933083672727\n",
      "epoch 543 completed\n",
      "loss is 0.5333373514405655\n",
      "epoch 544 completed\n",
      "loss is 0.53318188825794\n",
      "epoch 545 completed\n",
      "loss is 0.5330269210795938\n",
      "epoch 546 completed\n",
      "loss is 0.5328724521062029\n",
      "epoch 547 completed\n",
      "loss is 0.53271848347872\n",
      "epoch 548 completed\n",
      "loss is 0.5325650172782102\n",
      "epoch 549 completed\n",
      "loss is 0.5324120555257578\n",
      "epoch 550 completed\n",
      "loss is 0.5322596001824296\n",
      "epoch 551 completed\n",
      "loss is 0.5321076531492911\n",
      "epoch 552 completed\n",
      "loss is 0.5319562162674897\n",
      "epoch 553 completed\n",
      "loss is 0.5318052913183942\n",
      "epoch 554 completed\n",
      "loss is 0.5316548800237865\n",
      "epoch 555 completed\n",
      "loss is 0.5315049840461294\n",
      "epoch 556 completed\n",
      "loss is 0.5313556049888782\n",
      "epoch 557 completed\n",
      "loss is 0.5312067443968619\n",
      "epoch 558 completed\n",
      "loss is 0.5310584037567224\n",
      "epoch 559 completed\n",
      "loss is 0.5309105844974038\n",
      "epoch 560 completed\n",
      "loss is 0.5307632879907198\n",
      "epoch 561 completed\n",
      "loss is 0.530616515551961\n",
      "epoch 562 completed\n",
      "loss is 0.5304702684405727\n",
      "epoch 563 completed\n",
      "loss is 0.530324547860879\n",
      "epoch 564 completed\n",
      "loss is 0.5301793549628802\n",
      "epoch 565 completed\n",
      "loss is 0.5300346908430819\n",
      "epoch 566 completed\n",
      "loss is 0.52989055654541\n",
      "epoch 567 completed\n",
      "loss is 0.5297469530621506\n",
      "epoch 568 completed\n",
      "loss is 0.5296038813349624\n",
      "epoch 569 completed\n",
      "loss is 0.5294613422559313\n",
      "epoch 570 completed\n",
      "loss is 0.529319336668677\n",
      "epoch 571 completed\n",
      "loss is 0.5291778653695134\n",
      "epoch 572 completed\n",
      "loss is 0.5290369291086537\n",
      "epoch 573 completed\n",
      "loss is 0.5288965285914576\n",
      "epoch 574 completed\n",
      "loss is 0.5287566644797222\n",
      "epoch 575 completed\n",
      "loss is 0.5286173373930411\n",
      "epoch 576 completed\n",
      "loss is 0.5284785479101537\n",
      "epoch 577 completed\n",
      "loss is 0.5283402965703913\n",
      "epoch 578 completed\n",
      "loss is 0.5282025838751133\n",
      "epoch 579 completed\n",
      "loss is 0.5280654102892085\n",
      "epoch 580 completed\n",
      "loss is 0.5279287762426176\n",
      "epoch 581 completed\n",
      "loss is 0.5277926821318908\n",
      "epoch 582 completed\n",
      "loss is 0.5276571283217688\n",
      "epoch 583 completed\n",
      "loss is 0.5275221151467978\n",
      "epoch 584 completed\n",
      "loss is 0.5273876429129646\n",
      "epoch 585 completed\n",
      "loss is 0.5272537118993573\n",
      "epoch 586 completed\n",
      "loss is 0.5271203223598453\n",
      "epoch 587 completed\n",
      "loss is 0.5269874745247737\n",
      "epoch 588 completed\n",
      "loss is 0.5268551686026799\n",
      "epoch 589 completed\n",
      "loss is 0.5267234047820193\n",
      "epoch 590 completed\n",
      "loss is 0.5265921832329021\n",
      "epoch 591 completed\n",
      "loss is 0.5264615041088422\n",
      "epoch 592 completed\n",
      "loss is 0.526331367548506\n",
      "epoch 593 completed\n",
      "loss is 0.5262017736774748\n",
      "epoch 594 completed\n",
      "loss is 0.5260727226099988\n",
      "epoch 595 completed\n",
      "loss is 0.5259442144507608\n",
      "epoch 596 completed\n",
      "loss is 0.5258162492966301\n",
      "epoch 597 completed\n",
      "loss is 0.525688827238415\n",
      "epoch 598 completed\n",
      "loss is 0.5255619483626094\n",
      "epoch 599 completed\n",
      "loss is 0.525435612753123\n",
      "epoch 600 completed\n",
      "loss is 0.5253098204930188\n",
      "epoch 601 completed\n",
      "loss is 0.5251845716662107\n",
      "epoch 602 completed\n",
      "loss is 0.5250598663591776\n",
      "epoch 603 completed\n",
      "loss is 0.524935704662633\n",
      "epoch 604 completed\n",
      "loss is 0.5248120866731891\n",
      "epoch 605 completed\n",
      "loss is 0.5246890124950151\n",
      "epoch 606 completed\n",
      "loss is 0.5245664822414313\n",
      "epoch 607 completed\n",
      "loss is 0.5244444960365358\n",
      "epoch 608 completed\n",
      "loss is 0.5243230540167624\n",
      "epoch 609 completed\n",
      "loss is 0.5242021563324373\n",
      "epoch 610 completed\n",
      "loss is 0.5240818031492881\n",
      "epoch 611 completed\n",
      "loss is 0.5239619946499536\n",
      "epoch 612 completed\n",
      "loss is 0.5238427310354281\n",
      "epoch 613 completed\n",
      "loss is 0.5237240125265137\n",
      "epoch 614 completed\n",
      "loss is 0.5236058393652007\n",
      "epoch 615 completed\n",
      "loss is 0.5234882118160517\n",
      "epoch 616 completed\n",
      "loss is 0.5233711301675299\n",
      "epoch 617 completed\n",
      "loss is 0.5232545947333002\n",
      "epoch 618 completed\n",
      "loss is 0.5231386058535006\n",
      "epoch 619 completed\n",
      "loss is 0.5230231638959709\n",
      "epoch 620 completed\n",
      "loss is 0.5229082692574473\n",
      "epoch 621 completed\n",
      "loss is 0.5227939223647327\n",
      "epoch 622 completed\n",
      "loss is 0.5226801236758122\n",
      "epoch 623 completed\n",
      "loss is 0.5225668736809455\n",
      "epoch 624 completed\n",
      "loss is 0.5224541729037212\n",
      "epoch 625 completed\n",
      "loss is 0.5223420219020747\n",
      "epoch 626 completed\n",
      "loss is 0.5222304212692682\n",
      "epoch 627 completed\n",
      "loss is 0.522119371634826\n",
      "epoch 628 completed\n",
      "loss is 0.5220088736654581\n",
      "epoch 629 completed\n",
      "loss is 0.5218989280659273\n",
      "epoch 630 completed\n",
      "loss is 0.5217895355798764\n",
      "epoch 631 completed\n",
      "loss is 0.5216806969906514\n",
      "epoch 632 completed\n",
      "loss is 0.5215724131220546\n",
      "epoch 633 completed\n",
      "loss is 0.5214646848390775\n",
      "epoch 634 completed\n",
      "loss is 0.5213575130486102\n",
      "epoch 635 completed\n",
      "loss is 0.5212508987001063\n",
      "epoch 636 completed\n",
      "loss is 0.5211448427862135\n",
      "epoch 637 completed\n",
      "loss is 0.5210393463433783\n",
      "epoch 638 completed\n",
      "loss is 0.52093441045242\n",
      "epoch 639 completed\n",
      "loss is 0.5208300362390686\n",
      "epoch 640 completed\n",
      "loss is 0.5207262248744747\n",
      "epoch 641 completed\n",
      "loss is 0.5206229775757045\n",
      "epoch 642 completed\n",
      "loss is 0.5205202956061712\n",
      "epoch 643 completed\n",
      "loss is 0.5204181802760812\n",
      "epoch 644 completed\n",
      "loss is 0.5203166329428184\n",
      "epoch 645 completed\n",
      "loss is 0.5202156550113297\n",
      "epoch 646 completed\n",
      "loss is 0.5201152479344588\n",
      "epoch 647 completed\n",
      "loss is 0.5200154132132785\n",
      "epoch 648 completed\n",
      "loss is 0.519916152397387\n",
      "epoch 649 completed\n",
      "loss is 0.5198174670851812\n",
      "epoch 650 completed\n",
      "loss is 0.5197193589241105\n",
      "epoch 651 completed\n",
      "loss is 0.5196218296109065\n",
      "epoch 652 completed\n",
      "loss is 0.5195248808917958\n",
      "epoch 653 completed\n",
      "loss is 0.5194285145626862\n",
      "epoch 654 completed\n",
      "loss is 0.5193327324693371\n",
      "epoch 655 completed\n",
      "loss is 0.5192375365075167\n",
      "epoch 656 completed\n",
      "loss is 0.5191429286231225\n",
      "epoch 657 completed\n",
      "loss is 0.5190489108122996\n",
      "epoch 658 completed\n",
      "loss is 0.5189554851215442\n",
      "epoch 659 completed\n",
      "loss is 0.518862653647779\n",
      "epoch 660 completed\n",
      "loss is 0.5187704185384139\n",
      "epoch 661 completed\n",
      "loss is 0.5186787819913996\n",
      "epoch 662 completed\n",
      "loss is 0.5185877462552557\n",
      "epoch 663 completed\n",
      "loss is 0.5184973136290926\n",
      "epoch 664 completed\n",
      "loss is 0.5184074864626133\n",
      "epoch 665 completed\n",
      "loss is 0.5183182671561002\n",
      "epoch 666 completed\n",
      "loss is 0.5182296581603913\n",
      "epoch 667 completed\n",
      "loss is 0.5181416619768463\n",
      "epoch 668 completed\n",
      "loss is 0.518054281157284\n",
      "epoch 669 completed\n",
      "loss is 0.5179675183039331\n",
      "epoch 670 completed\n",
      "loss is 0.5178813760693338\n",
      "epoch 671 completed\n",
      "loss is 0.5177958571562614\n",
      "epoch 672 completed\n",
      "loss is 0.5177109643176162\n",
      "epoch 673 completed\n",
      "loss is 0.5176267003563128\n",
      "epoch 674 completed\n",
      "loss is 0.5175430681251367\n",
      "epoch 675 completed\n",
      "loss is 0.5174600705266144\n",
      "epoch 676 completed\n",
      "loss is 0.5173777105128515\n",
      "epoch 677 completed\n",
      "loss is 0.5172959910853623\n",
      "epoch 678 completed\n",
      "loss is 0.5172149152948904\n",
      "epoch 679 completed\n",
      "loss is 0.517134486241219\n",
      "epoch 680 completed\n",
      "loss is 0.5170547070729368\n",
      "epoch 681 completed\n",
      "loss is 0.516975580987247\n",
      "epoch 682 completed\n",
      "loss is 0.516897111229707\n",
      "epoch 683 completed\n",
      "loss is 0.516819301093981\n",
      "epoch 684 completed\n",
      "loss is 0.5167421539215742\n",
      "epoch 685 completed\n",
      "loss is 0.5166656731015515\n",
      "epoch 686 completed\n",
      "loss is 0.516589862070224\n",
      "epoch 687 completed\n",
      "loss is 0.5165147243108558\n",
      "epoch 688 completed\n",
      "loss is 0.5164402633533118\n",
      "epoch 689 completed\n",
      "loss is 0.5163664827737123\n",
      "epoch 690 completed\n",
      "loss is 0.5162933861940748\n",
      "epoch 691 completed\n",
      "loss is 0.5162209772819026\n",
      "epoch 692 completed\n",
      "loss is 0.5161492597498046\n",
      "epoch 693 completed\n",
      "loss is 0.5160782373550514\n",
      "epoch 694 completed\n",
      "loss is 0.516007913899129\n",
      "epoch 695 completed\n",
      "loss is 0.5159382932272716\n",
      "epoch 696 completed\n",
      "loss is 0.5158693792279642\n",
      "epoch 697 completed\n",
      "loss is 0.5158011758324391\n",
      "epoch 698 completed\n",
      "loss is 0.5157336870141148\n",
      "epoch 699 completed\n",
      "loss is 0.5156669167880484\n",
      "epoch 700 completed\n",
      "loss is 0.5156008692103378\n",
      "epoch 701 completed\n",
      "loss is 0.5155355483775007\n",
      "epoch 702 completed\n",
      "loss is 0.5154709584258302\n",
      "epoch 703 completed\n",
      "loss is 0.5154071035307215\n",
      "epoch 704 completed\n",
      "loss is 0.5153439879059606\n",
      "epoch 705 completed\n",
      "loss is 0.515281615802999\n",
      "epoch 706 completed\n",
      "loss is 0.5152199915101694\n",
      "epoch 707 completed\n",
      "loss is 0.5151591193518992\n",
      "epoch 708 completed\n",
      "loss is 0.5150990036878643\n",
      "epoch 709 completed\n",
      "loss is 0.5150396489121186\n",
      "epoch 710 completed\n",
      "loss is 0.5149810594521922\n",
      "epoch 711 completed\n",
      "loss is 0.5149232397681318\n",
      "epoch 712 completed\n",
      "loss is 0.5148661943515376\n",
      "epoch 713 completed\n",
      "loss is 0.5148099277245161\n",
      "epoch 714 completed\n",
      "loss is 0.5147544444386175\n",
      "epoch 715 completed\n",
      "loss is 0.5146997490737387\n",
      "epoch 716 completed\n",
      "loss is 0.5146458462369543\n",
      "epoch 717 completed\n",
      "loss is 0.5145927405613246\n",
      "epoch 718 completed\n",
      "loss is 0.5145404367046428\n",
      "epoch 719 completed\n",
      "loss is 0.5144889393481531\n",
      "epoch 720 completed\n",
      "loss is 0.5144382531951927\n",
      "epoch 721 completed\n",
      "loss is 0.5143883829698064\n",
      "epoch 722 completed\n",
      "loss is 0.5143393334152951\n",
      "epoch 723 completed\n",
      "loss is 0.514291109292716\n",
      "epoch 724 completed\n",
      "loss is 0.5142437153793318\n",
      "epoch 725 completed\n",
      "loss is 0.5141971564669923\n",
      "epoch 726 completed\n",
      "loss is 0.5141514373604656\n",
      "epoch 727 completed\n",
      "loss is 0.5141065628757129\n",
      "epoch 728 completed\n",
      "loss is 0.5140625378380965\n",
      "epoch 729 completed\n",
      "loss is 0.5140193670805226\n",
      "epoch 730 completed\n",
      "loss is 0.5139770554415342\n",
      "epoch 731 completed\n",
      "loss is 0.5139356077633249\n",
      "epoch 732 completed\n",
      "loss is 0.5138950288896909\n",
      "epoch 733 completed\n",
      "loss is 0.5138553236639182\n",
      "epoch 734 completed\n",
      "loss is 0.5138164969265953\n",
      "epoch 735 completed\n",
      "loss is 0.5137785535133594\n",
      "epoch 736 completed\n",
      "loss is 0.5137414982525603\n",
      "epoch 737 completed\n",
      "loss is 0.5137053359628692\n",
      "epoch 738 completed\n",
      "loss is 0.5136700714507964\n",
      "epoch 739 completed\n",
      "loss is 0.5136357095081239\n",
      "epoch 740 completed\n",
      "loss is 0.5136022549092956\n",
      "epoch 741 completed\n",
      "loss is 0.5135697124086901\n",
      "epoch 742 completed\n",
      "loss is 0.5135380867378276\n",
      "epoch 743 completed\n",
      "loss is 0.5135073826024943\n",
      "epoch 744 completed\n",
      "loss is 0.5134776046797911\n",
      "epoch 745 completed\n",
      "loss is 0.5134487576150775\n",
      "epoch 746 completed\n",
      "loss is 0.5134208460188481\n",
      "epoch 747 completed\n",
      "loss is 0.5133938744635049\n",
      "epoch 748 completed\n",
      "loss is 0.5133678474800651\n",
      "epoch 749 completed\n",
      "loss is 0.5133427695547503\n",
      "epoch 750 completed\n",
      "loss is 0.5133186451254984\n",
      "epoch 751 completed\n",
      "loss is 0.5132954785783959\n",
      "epoch 752 completed\n",
      "loss is 0.5132732742439873\n",
      "epoch 753 completed\n",
      "loss is 0.5132520363935154\n",
      "epoch 754 completed\n",
      "loss is 0.513231769235061\n",
      "epoch 755 completed\n",
      "loss is 0.513212476909572\n",
      "epoch 756 completed\n",
      "loss is 0.5131941634868222\n",
      "epoch 757 completed\n",
      "loss is 0.5131768329612412\n",
      "epoch 758 completed\n",
      "loss is 0.51316048924768\n",
      "epoch 759 completed\n",
      "loss is 0.51314513617705\n",
      "epoch 760 completed\n",
      "loss is 0.5131307774918737\n",
      "epoch 761 completed\n",
      "loss is 0.5131174168417459\n",
      "epoch 762 completed\n",
      "loss is 0.5131050577786718\n",
      "epoch 763 completed\n",
      "loss is 0.5130937037523332\n",
      "epoch 764 completed\n",
      "loss is 0.5130833581052423\n",
      "epoch 765 completed\n",
      "loss is 0.5130740240677871\n",
      "epoch 766 completed\n",
      "loss is 0.5130657047531924\n",
      "epoch 767 completed\n",
      "loss is 0.5130584031523799\n",
      "epoch 768 completed\n",
      "loss is 0.5130521221287286\n",
      "epoch 769 completed\n",
      "loss is 0.5130468644127364\n",
      "epoch 770 completed\n",
      "loss is 0.513042632596589\n",
      "epoch 771 completed\n",
      "loss is 0.5130394291286415\n",
      "epoch 772 completed\n",
      "loss is 0.5130372563077935\n",
      "epoch 773 completed\n",
      "loss is 0.5130361162777854\n",
      "epoch 774 completed\n",
      "loss is 0.5130360110213934\n",
      "epoch 775 completed\n",
      "loss is 0.5130369423545579\n",
      "epoch 776 completed\n",
      "loss is 0.5130389119204054\n",
      "epoch 777 completed\n",
      "loss is 0.5130419211832109\n",
      "epoch 778 completed\n",
      "loss is 0.5130459714222597\n",
      "epoch 779 completed\n",
      "loss is 0.5130510637256512\n",
      "epoch 780 completed\n",
      "loss is 0.5130571989840236\n",
      "epoch 781 completed\n",
      "loss is 0.5130643778842247\n",
      "epoch 782 completed\n",
      "loss is 0.5130726009028886\n",
      "epoch 783 completed\n",
      "loss is 0.5130818682999998\n",
      "epoch 784 completed\n",
      "loss is 0.5130921801123632\n",
      "epoch 785 completed\n",
      "loss is 0.5131035361470488\n",
      "epoch 786 completed\n",
      "loss is 0.5131159359747949\n",
      "epoch 787 completed\n",
      "loss is 0.5131293789233616\n",
      "epoch 788 completed\n",
      "loss is 0.5131438640708722\n",
      "epoch 789 completed\n",
      "loss is 0.5131593902391257\n",
      "epoch 790 completed\n",
      "loss is 0.5131759559868975\n",
      "epoch 791 completed\n",
      "loss is 0.513193559603221\n",
      "epoch 792 completed\n",
      "loss is 0.5132121991007038\n",
      "epoch 793 completed\n",
      "loss is 0.5132318722088204\n",
      "epoch 794 completed\n",
      "loss is 0.5132525763672469\n",
      "epoch 795 completed\n",
      "loss is 0.5132743087192151\n",
      "epoch 796 completed\n",
      "loss is 0.5132970661049225\n",
      "epoch 797 completed\n",
      "loss is 0.5133208450549729\n",
      "epoch 798 completed\n",
      "loss is 0.5133456417838947\n",
      "epoch 799 completed\n",
      "loss is 0.5133714521837462\n",
      "epoch 800 completed\n",
      "loss is 0.513398271817771\n",
      "epoch 801 completed\n",
      "loss is 0.5134260959141842\n",
      "epoch 802 completed\n",
      "loss is 0.5134549193600731\n",
      "epoch 803 completed\n",
      "loss is 0.5134847366954018\n",
      "epoch 804 completed\n",
      "loss is 0.5135155421071667\n",
      "epoch 805 completed\n",
      "loss is 0.5135473294237191\n",
      "epoch 806 completed\n",
      "loss is 0.5135800921092429\n",
      "epoch 807 completed\n",
      "loss is 0.5136138232584231\n",
      "epoch 808 completed\n",
      "loss is 0.5136485155913189\n",
      "epoch 809 completed\n",
      "loss is 0.5136841614484369\n",
      "epoch 810 completed\n",
      "loss is 0.513720752786061\n",
      "epoch 811 completed\n",
      "loss is 0.5137582811718213\n",
      "epoch 812 completed\n",
      "loss is 0.5137967377805128\n",
      "epoch 813 completed\n",
      "loss is 0.5138361133902416\n",
      "epoch 814 completed\n",
      "loss is 0.5138763983788074\n",
      "epoch 815 completed\n",
      "loss is 0.5139175827204648\n",
      "epoch 816 completed\n",
      "loss is 0.5139596559829891\n",
      "epoch 817 completed\n",
      "loss is 0.5140026073250821\n",
      "epoch 818 completed\n",
      "loss is 0.5140464254941799\n",
      "epoch 819 completed\n",
      "loss is 0.5140910988246318\n",
      "epoch 820 completed\n",
      "loss is 0.5141366152362666\n",
      "epoch 821 completed\n",
      "loss is 0.5141829622334012\n",
      "epoch 822 completed\n",
      "loss is 0.5142301269042817\n",
      "epoch 823 completed\n",
      "loss is 0.5142780959209641\n",
      "epoch 824 completed\n",
      "loss is 0.51432685553967\n",
      "epoch 825 completed\n",
      "loss is 0.5143763916016392\n",
      "epoch 826 completed\n",
      "loss is 0.5144266895344579\n",
      "epoch 827 completed\n",
      "loss is 0.5144777343539017\n",
      "epoch 828 completed\n",
      "loss is 0.514529510666319\n",
      "epoch 829 completed\n",
      "loss is 0.514582002671533\n",
      "epoch 830 completed\n",
      "loss is 0.5146351941663095\n",
      "epoch 831 completed\n",
      "loss is 0.5146890685483503\n",
      "epoch 832 completed\n",
      "loss is 0.5147436088208988\n",
      "epoch 833 completed\n",
      "loss is 0.5147987975978777\n",
      "epoch 834 completed\n",
      "loss is 0.5148546171096495\n",
      "epoch 835 completed\n",
      "loss is 0.5149110492093387\n",
      "epoch 836 completed\n",
      "loss is 0.5149680753797423\n",
      "epoch 837 completed\n",
      "loss is 0.5150256767408605\n",
      "epoch 838 completed\n",
      "loss is 0.515083834058015\n",
      "epoch 839 completed\n",
      "loss is 0.5151425277505324\n",
      "epoch 840 completed\n",
      "loss is 0.5152017379010959\n",
      "epoch 841 completed\n",
      "loss is 0.5152614442656044\n",
      "epoch 842 completed\n",
      "loss is 0.5153216262836808\n",
      "epoch 843 completed\n",
      "loss is 0.5153822630897327\n",
      "epoch 844 completed\n",
      "loss is 0.5154433335246043\n",
      "epoch 845 completed\n",
      "loss is 0.5155048161477543\n",
      "epoch 846 completed\n",
      "loss is 0.5155666892500329\n",
      "epoch 847 completed\n",
      "loss is 0.5156289308669614\n",
      "epoch 848 completed\n",
      "loss is 0.5156915187925446\n",
      "epoch 849 completed\n",
      "loss is 0.5157544305935802\n",
      "epoch 850 completed\n",
      "loss is 0.5158176436244853\n",
      "epoch 851 completed\n",
      "loss is 0.5158811350425296\n",
      "epoch 852 completed\n",
      "loss is 0.5159448818235636\n",
      "epoch 853 completed\n",
      "loss is 0.5160088607781603\n",
      "epoch 854 completed\n",
      "loss is 0.5160730485680997\n",
      "epoch 855 completed\n",
      "loss is 0.5161374217233019\n",
      "epoch 856 completed\n",
      "loss is 0.5162019566590192\n",
      "epoch 857 completed\n",
      "loss is 0.5162666296933969\n",
      "epoch 858 completed\n",
      "loss is 0.5163314170652723\n",
      "epoch 859 completed\n",
      "loss is 0.5163962949522499\n",
      "epoch 860 completed\n",
      "loss is 0.5164612394889592\n",
      "epoch 861 completed\n",
      "loss is 0.5165262267855112\n",
      "epoch 862 completed\n",
      "loss is 0.5165912329460973\n",
      "epoch 863 completed\n",
      "loss is 0.5166562340876922\n",
      "epoch 864 completed\n",
      "loss is 0.5167212063588192\n",
      "epoch 865 completed\n",
      "loss is 0.5167861259583784\n",
      "epoch 866 completed\n",
      "loss is 0.5168509691544325\n",
      "epoch 867 completed\n",
      "loss is 0.5169157123029886\n",
      "epoch 868 completed\n",
      "loss is 0.5169803318666952\n",
      "epoch 869 completed\n",
      "loss is 0.5170448044334031\n",
      "epoch 870 completed\n",
      "loss is 0.5171091067346156\n",
      "epoch 871 completed\n",
      "loss is 0.5171732156637113\n",
      "epoch 872 completed\n",
      "loss is 0.5172371082939721\n",
      "epoch 873 completed\n",
      "loss is 0.5173007618963453\n",
      "epoch 874 completed\n",
      "loss is 0.5173641539569043\n",
      "epoch 875 completed\n",
      "loss is 0.5174272621939886\n",
      "epoch 876 completed\n",
      "loss is 0.5174900645749873\n",
      "epoch 877 completed\n",
      "loss is 0.5175525393327263\n",
      "epoch 878 completed\n",
      "loss is 0.5176146649814397\n",
      "epoch 879 completed\n",
      "loss is 0.5176764203322892\n",
      "epoch 880 completed\n",
      "loss is 0.517737784508388\n",
      "epoch 881 completed\n",
      "loss is 0.5177987369593536\n",
      "epoch 882 completed\n",
      "loss is 0.5178592574753053\n",
      "epoch 883 completed\n",
      "loss is 0.5179193262003166\n",
      "epoch 884 completed\n",
      "loss is 0.5179789236453108\n",
      "epoch 885 completed\n",
      "loss is 0.5180380307003408\n",
      "epoch 886 completed\n",
      "loss is 0.5180966286462747\n",
      "epoch 887 completed\n",
      "loss is 0.5181546991658491\n",
      "epoch 888 completed\n",
      "loss is 0.518212224354106\n",
      "epoch 889 completed\n",
      "loss is 0.5182691867281428\n",
      "epoch 890 completed\n",
      "loss is 0.5183255692362427\n",
      "epoch 891 completed\n",
      "loss is 0.5183813552663108\n",
      "epoch 892 completed\n",
      "loss is 0.5184365286536381\n",
      "epoch 893 completed\n",
      "loss is 0.5184910736880183\n",
      "epoch 894 completed\n",
      "loss is 0.5185449751201363\n",
      "epoch 895 completed\n",
      "loss is 0.518598218167339\n",
      "epoch 896 completed\n",
      "loss is 0.5186507885186621\n",
      "epoch 897 completed\n",
      "loss is 0.5187026723392266\n",
      "epoch 898 completed\n",
      "loss is 0.5187538562739701\n",
      "epoch 899 completed\n",
      "loss is 0.5188043274506788\n",
      "epoch 900 completed\n",
      "loss is 0.5188540734823858\n",
      "epoch 901 completed\n",
      "loss is 0.5189030824691325\n",
      "epoch 902 completed\n",
      "loss is 0.5189513429990811\n",
      "epoch 903 completed\n",
      "loss is 0.5189988441490114\n",
      "epoch 904 completed\n",
      "loss is 0.5190455754842043\n",
      "epoch 905 completed\n",
      "loss is 0.519091527057726\n",
      "epoch 906 completed\n",
      "loss is 0.5191366894091615\n",
      "epoch 907 completed\n",
      "loss is 0.5191810535627512\n",
      "epoch 908 completed\n",
      "loss is 0.5192246110249951\n",
      "epoch 909 completed\n",
      "loss is 0.5192673537817466\n",
      "epoch 910 completed\n",
      "loss is 0.5193092742947714\n",
      "epoch 911 completed\n",
      "loss is 0.5193503654978568\n",
      "epoch 912 completed\n",
      "loss is 0.5193906207924116\n",
      "epoch 913 completed\n",
      "loss is 0.5194300340426417\n",
      "epoch 914 completed\n",
      "loss is 0.5194685995703057\n",
      "epoch 915 completed\n",
      "loss is 0.5195063121490632\n",
      "epoch 916 completed\n",
      "loss is 0.5195431669984168\n",
      "epoch 917 completed\n",
      "loss is 0.5195791597773204\n",
      "epoch 918 completed\n",
      "loss is 0.5196142865774527\n",
      "epoch 919 completed\n",
      "loss is 0.5196485439161361\n",
      "epoch 920 completed\n",
      "loss is 0.5196819287289958\n",
      "epoch 921 completed\n",
      "loss is 0.5197144383623202\n",
      "epoch 922 completed\n",
      "loss is 0.5197460705651824\n",
      "epoch 923 completed\n",
      "loss is 0.5197768234813186\n",
      "epoch 924 completed\n",
      "loss is 0.5198066956407978\n",
      "epoch 925 completed\n",
      "loss is 0.5198356859514928\n",
      "epoch 926 completed\n",
      "loss is 0.519863793690387\n",
      "epoch 927 completed\n",
      "loss is 0.5198910184947294\n",
      "epoch 928 completed\n",
      "loss is 0.5199173603530306\n",
      "epoch 929 completed\n",
      "loss is 0.5199428195959893\n",
      "epoch 930 completed\n",
      "loss is 0.5199673968872524\n",
      "epoch 931 completed\n",
      "loss is 0.5199910932141787\n",
      "epoch 932 completed\n",
      "loss is 0.520013909878457\n",
      "epoch 933 completed\n",
      "loss is 0.5200358484867251\n",
      "epoch 934 completed\n",
      "loss is 0.5200569109411425\n",
      "epoch 935 completed\n",
      "loss is 0.5200770994299384\n",
      "epoch 936 completed\n",
      "loss is 0.5200964164179722\n",
      "epoch 937 completed\n",
      "loss is 0.5201148646372802\n",
      "epoch 938 completed\n",
      "loss is 0.5201324470776671\n",
      "epoch 939 completed\n",
      "loss is 0.5201491669773262\n",
      "epoch 940 completed\n",
      "loss is 0.5201650278135062\n",
      "epoch 941 completed\n",
      "loss is 0.5201800332932118\n",
      "epoch 942 completed\n",
      "loss is 0.520194187344031\n",
      "epoch 943 completed\n",
      "loss is 0.5202074941049756\n",
      "epoch 944 completed\n",
      "loss is 0.5202199579174629\n",
      "epoch 945 completed\n",
      "loss is 0.5202315833163618\n",
      "epoch 946 completed\n",
      "loss is 0.5202423750211377\n",
      "epoch 947 completed\n",
      "loss is 0.5202523379271546\n",
      "epoch 948 completed\n",
      "loss is 0.5202614770970504\n",
      "epoch 949 completed\n",
      "loss is 0.5202697977522536\n",
      "epoch 950 completed\n",
      "loss is 0.520277305264637\n",
      "epoch 951 completed\n",
      "loss is 0.5202840051483111\n",
      "epoch 952 completed\n",
      "loss is 0.52028990305155\n",
      "epoch 953 completed\n",
      "loss is 0.5202950047488689\n",
      "epoch 954 completed\n",
      "loss is 0.5202993161332599\n",
      "epoch 955 completed\n",
      "loss is 0.5203028432085629\n",
      "epoch 956 completed\n",
      "loss is 0.5203055920820142\n",
      "epoch 957 completed\n",
      "loss is 0.5203075689569656\n",
      "epoch 958 completed\n",
      "loss is 0.5203087801257339\n",
      "epoch 959 completed\n",
      "loss is 0.5203092319626325\n",
      "epoch 960 completed\n",
      "loss is 0.5203089309171829\n",
      "epoch 961 completed\n",
      "loss is 0.5203078835074798\n",
      "epoch 962 completed\n",
      "loss is 0.5203060963137293\n",
      "epoch 963 completed\n",
      "loss is 0.5203035759719716\n",
      "epoch 964 completed\n",
      "loss is 0.5203003291679407\n",
      "epoch 965 completed\n",
      "loss is 0.5202963626311435\n",
      "epoch 966 completed\n",
      "loss is 0.5202916831290629\n",
      "epoch 967 completed\n",
      "loss is 0.5202862974615734\n",
      "epoch 968 completed\n",
      "loss is 0.5202802124555025\n",
      "epoch 969 completed\n",
      "loss is 0.5202734349593701\n",
      "epoch 970 completed\n",
      "loss is 0.5202659718383029\n",
      "epoch 971 completed\n",
      "loss is 0.5202578299691061\n",
      "epoch 972 completed\n",
      "loss is 0.5202490162355197\n",
      "epoch 973 completed\n",
      "loss is 0.5202395375236127\n",
      "epoch 974 completed\n",
      "loss is 0.5202294007173759\n",
      "epoch 975 completed\n",
      "loss is 0.5202186126944729\n",
      "epoch 976 completed\n",
      "loss is 0.520207180322097\n",
      "epoch 977 completed\n",
      "loss is 0.5201951104530887\n",
      "epoch 978 completed\n",
      "loss is 0.5201824099221178\n",
      "epoch 979 completed\n",
      "loss is 0.5201690855420742\n",
      "epoch 980 completed\n",
      "loss is 0.52015514410058\n",
      "epoch 981 completed\n",
      "loss is 0.5201405923566881\n",
      "epoch 982 completed\n",
      "loss is 0.5201254370376976\n",
      "epoch 983 completed\n",
      "loss is 0.5201096848361287\n",
      "epoch 984 completed\n",
      "loss is 0.5200933424068468\n",
      "epoch 985 completed\n",
      "loss is 0.5200764163643101\n",
      "epoch 986 completed\n",
      "loss is 0.5200589132799882\n",
      "epoch 987 completed\n",
      "loss is 0.5200408396798715\n",
      "epoch 988 completed\n",
      "loss is 0.5200222020421602\n",
      "epoch 989 completed\n",
      "loss is 0.5200030067950497\n",
      "epoch 990 completed\n",
      "loss is 0.5199832603146599\n",
      "epoch 991 completed\n",
      "loss is 0.519962968923104\n",
      "epoch 992 completed\n",
      "loss is 0.5199421388866333\n",
      "epoch 993 completed\n",
      "loss is 0.5199207764139698\n",
      "epoch 994 completed\n",
      "loss is 0.5198988876546754\n",
      "epoch 995 completed\n",
      "loss is 0.5198764786977222\n",
      "epoch 996 completed\n",
      "loss is 0.5198535555700998\n",
      "epoch 997 completed\n",
      "loss is 0.5198301242355795\n",
      "epoch 998 completed\n",
      "loss is 0.5198061905935606\n",
      "epoch 999 completed\n",
      "loss is 0.519781760478033\n",
      "\n",
      "\n",
      "printing network: [<__main__.InputLayer object at 0x000002A81CA72010>\n",
      " <__main__.Layer object at 0x000002A8101D17D0>\n",
      " <__main__.OutputLayer object at 0x000002A80F98AD10>]\n",
      "epoch 0 completed\n",
      "loss is 1.5384650542577163\n",
      "epoch 1 completed\n",
      "loss is 1.4738719081022549\n",
      "epoch 2 completed\n",
      "loss is 1.4004723423822756\n",
      "epoch 3 completed\n",
      "loss is 1.32264358311009\n",
      "epoch 4 completed\n",
      "loss is 1.2475717397128332\n",
      "epoch 5 completed\n",
      "loss is 1.1824535623987298\n",
      "epoch 6 completed\n",
      "loss is 1.128580197928306\n",
      "epoch 7 completed\n",
      "loss is 1.0833889476844138\n",
      "epoch 8 completed\n",
      "loss is 1.0449333443827304\n",
      "epoch 9 completed\n",
      "loss is 1.01187632535596\n",
      "epoch 10 completed\n",
      "loss is 0.9831983146987847\n",
      "epoch 11 completed\n",
      "loss is 0.9580891801170596\n",
      "epoch 12 completed\n",
      "loss is 0.9359042194439285\n",
      "epoch 13 completed\n",
      "loss is 0.9161341034379143\n",
      "epoch 14 completed\n",
      "loss is 0.8983782741740661\n",
      "epoch 15 completed\n",
      "loss is 0.88232144484382\n",
      "epoch 16 completed\n",
      "loss is 0.8677140380882188\n",
      "epoch 17 completed\n",
      "loss is 0.8543567183722388\n",
      "epoch 18 completed\n",
      "loss is 0.8420885561557072\n",
      "epoch 19 completed\n",
      "loss is 0.8307781205937337\n",
      "epoch 20 completed\n",
      "loss is 0.8203168109931737\n",
      "epoch 21 completed\n",
      "loss is 0.810613854807056\n",
      "epoch 22 completed\n",
      "loss is 0.8015925339378994\n",
      "epoch 23 completed\n",
      "loss is 0.793187316315656\n",
      "epoch 24 completed\n",
      "loss is 0.7853416582631191\n",
      "epoch 25 completed\n",
      "loss is 0.7780063078388156\n",
      "epoch 26 completed\n",
      "loss is 0.771137985573734\n",
      "epoch 27 completed\n",
      "loss is 0.7646983518690322\n",
      "epoch 28 completed\n",
      "loss is 0.7586531937507457\n",
      "epoch 29 completed\n",
      "loss is 0.7529717805363684\n",
      "epoch 30 completed\n",
      "loss is 0.7476263502413283\n",
      "epoch 31 completed\n",
      "loss is 0.7425916976004829\n",
      "epoch 32 completed\n",
      "loss is 0.7378448413302229\n",
      "epoch 33 completed\n",
      "loss is 0.733364753349473\n",
      "epoch 34 completed\n",
      "loss is 0.729132136557065\n",
      "epoch 35 completed\n",
      "loss is 0.7251292407416734\n",
      "epoch 36 completed\n",
      "loss is 0.7213397085028437\n",
      "epoch 37 completed\n",
      "loss is 0.7177484448502731\n",
      "epoch 38 completed\n",
      "loss is 0.7143415055432747\n",
      "epoch 39 completed\n",
      "loss is 0.7111060003228109\n",
      "epoch 40 completed\n",
      "loss is 0.7080300080422298\n",
      "epoch 41 completed\n",
      "loss is 0.705102501371665\n",
      "epoch 42 completed\n",
      "loss is 0.7023132792749655\n",
      "epoch 43 completed\n",
      "loss is 0.6996529058680088\n",
      "epoch 44 completed\n",
      "loss is 0.6971126545877454\n",
      "epoch 45 completed\n",
      "loss is 0.6946844568512647\n",
      "epoch 46 completed\n",
      "loss is 0.6923608545785624\n",
      "epoch 47 completed\n",
      "loss is 0.6901349561031058\n",
      "epoch 48 completed\n",
      "loss is 0.6880003951098266\n",
      "epoch 49 completed\n",
      "loss is 0.6859512923278117\n",
      "epoch 50 completed\n",
      "loss is 0.6839822197701151\n",
      "epoch 51 completed\n",
      "loss is 0.6820881673600253\n",
      "epoch 52 completed\n",
      "loss is 0.6802645118151828\n",
      "epoch 53 completed\n",
      "loss is 0.678506987680885\n",
      "epoch 54 completed\n",
      "loss is 0.6768116604142714\n",
      "epoch 55 completed\n",
      "loss is 0.6751749014240591\n",
      "epoch 56 completed\n",
      "loss is 0.6735933649682102\n",
      "epoch 57 completed\n",
      "loss is 0.6720639668065042\n",
      "epoch 58 completed\n",
      "loss is 0.6705838644982364\n",
      "epoch 59 completed\n",
      "loss is 0.6691504392289561\n",
      "epoch 60 completed\n",
      "loss is 0.6677612790455645\n",
      "epoch 61 completed\n",
      "loss is 0.6664141633771554\n",
      "epoch 62 completed\n",
      "loss is 0.6651070487202078\n",
      "epoch 63 completed\n",
      "loss is 0.6638380553710638\n",
      "epoch 64 completed\n",
      "loss is 0.6626054550957956\n",
      "epoch 65 completed\n",
      "loss is 0.6614076596368712\n",
      "epoch 66 completed\n",
      "loss is 0.6602432099667873\n",
      "epoch 67 completed\n",
      "loss is 0.659110766210222\n",
      "epoch 68 completed\n",
      "loss is 0.6580090981676568\n",
      "epoch 69 completed\n",
      "loss is 0.656937076384251\n",
      "epoch 70 completed\n",
      "loss is 0.6558936637177205\n",
      "epoch 71 completed\n",
      "loss is 0.6548779073678159\n",
      "epoch 72 completed\n",
      "loss is 0.6538889313377588\n",
      "epoch 73 completed\n",
      "loss is 0.6529259293045836\n",
      "epoch 74 completed\n",
      "loss is 0.6519881578809222\n",
      "epoch 75 completed\n",
      "loss is 0.6510749302553812\n",
      "epoch 76 completed\n",
      "loss is 0.6501856102023992\n",
      "epoch 77 completed\n",
      "loss is 0.6493196064554152\n",
      "epoch 78 completed\n",
      "loss is 0.6484763674393493\n",
      "epoch 79 completed\n",
      "loss is 0.6476553763598243\n",
      "epoch 80 completed\n",
      "loss is 0.6468561466473267\n",
      "epoch 81 completed\n",
      "loss is 0.646078217754578\n",
      "epoch 82 completed\n",
      "loss is 0.6453211513049028\n",
      "epoch 83 completed\n",
      "loss is 0.6445845275882951\n",
      "epoch 84 completed\n",
      "loss is 0.6438679424004183\n",
      "epoch 85 completed\n",
      "loss is 0.6431710042179039\n",
      "epoch 86 completed\n",
      "loss is 0.6424933317012177\n",
      "epoch 87 completed\n",
      "loss is 0.6418345515141668\n",
      "epoch 88 completed\n",
      "loss is 0.6411942964468692\n",
      "epoch 89 completed\n",
      "loss is 0.6405722038269405\n",
      "epoch 90 completed\n",
      "loss is 0.6399679142016903\n",
      "epoch 91 completed\n",
      "loss is 0.63938107027256\n",
      "epoch 92 completed\n",
      "loss is 0.6388113160617207\n",
      "epoch 93 completed\n",
      "loss is 0.6382582962899145\n",
      "epoch 94 completed\n",
      "loss is 0.6377216559441562\n",
      "epoch 95 completed\n",
      "loss is 0.6372010400138701\n",
      "epoch 96 completed\n",
      "loss is 0.6366960933744009\n",
      "epoch 97 completed\n",
      "loss is 0.6362064607975769\n",
      "epoch 98 completed\n",
      "loss is 0.6357317870700157\n",
      "epoch 99 completed\n",
      "loss is 0.6352717172011798\n",
      "epoch 100 completed\n",
      "loss is 0.6348258967046834\n",
      "epoch 101 completed\n",
      "loss is 0.6343939719379774\n",
      "epoch 102 completed\n",
      "loss is 0.6339755904872233\n",
      "epoch 103 completed\n",
      "loss is 0.6335704015858896\n",
      "epoch 104 completed\n",
      "loss is 0.6331780565572384\n",
      "epoch 105 completed\n",
      "loss is 0.6327982092724298\n",
      "epoch 106 completed\n",
      "loss is 0.6324305166174081\n",
      "epoch 107 completed\n",
      "loss is 0.6320746389629583\n",
      "epoch 108 completed\n",
      "loss is 0.6317302406334165\n",
      "epoch 109 completed\n",
      "loss is 0.6313969903703888\n",
      "epoch 110 completed\n",
      "loss is 0.6310745617884755\n",
      "epoch 111 completed\n",
      "loss is 0.6307626338205217\n",
      "epoch 112 completed\n",
      "loss is 0.6304608911501819\n",
      "epoch 113 completed\n",
      "loss is 0.6301690246297498\n",
      "epoch 114 completed\n",
      "loss is 0.6298867316811984\n",
      "epoch 115 completed\n",
      "loss is 0.6296137166782767\n",
      "epoch 116 completed\n",
      "loss is 0.6293496913073405\n",
      "epoch 117 completed\n",
      "loss is 0.6290943749043557\n",
      "epoch 118 completed\n",
      "loss is 0.6288474947652681\n",
      "epoch 119 completed\n",
      "loss is 0.6286087864267126\n",
      "epoch 120 completed\n",
      "loss is 0.6283779939138128\n",
      "epoch 121 completed\n",
      "loss is 0.628154869951696\n",
      "epoch 122 completed\n",
      "loss is 0.6279391761372637\n",
      "epoch 123 completed\n",
      "loss is 0.6277306830677876\n",
      "epoch 124 completed\n",
      "loss is 0.6275291704230054\n",
      "epoch 125 completed\n",
      "loss is 0.6273344269975956\n",
      "epoch 126 completed\n",
      "loss is 0.6271462506812403\n",
      "epoch 127 completed\n",
      "loss is 0.6269644483838681\n",
      "epoch 128 completed\n",
      "loss is 0.6267888359041661\n",
      "epoch 129 completed\n",
      "loss is 0.626619237740013\n",
      "epoch 130 completed\n",
      "loss is 0.6264554868401134\n",
      "epoch 131 completed\n",
      "loss is 0.626297424296767\n",
      "epoch 132 completed\n",
      "loss is 0.6261448989804334\n",
      "epoch 133 completed\n",
      "loss is 0.6259977671174279\n",
      "epoch 134 completed\n",
      "loss is 0.6258558918128501\n",
      "epoch 135 completed\n",
      "loss is 0.6257191425214813\n",
      "epoch 136 completed\n",
      "loss is 0.6255873944701167\n",
      "epoch 137 completed\n",
      "loss is 0.6254605280354008\n",
      "epoch 138 completed\n",
      "loss is 0.625338428081801\n",
      "epoch 139 completed\n",
      "loss is 0.6252209832649416\n",
      "epoch 140 completed\n",
      "loss is 0.6251080853059118\n",
      "epoch 141 completed\n",
      "loss is 0.6249996282426328\n",
      "epoch 142 completed\n",
      "loss is 0.6248955076646615\n",
      "epoch 143 completed\n",
      "loss is 0.624795619938096\n",
      "epoch 144 completed\n",
      "loss is 0.6246998614274314\n",
      "epoch 145 completed\n",
      "loss is 0.6246081277213543\n",
      "epoch 146 completed\n",
      "loss is 0.6245203128695052\n",
      "epoch 147 completed\n",
      "loss is 0.624436308637223\n",
      "epoch 148 completed\n",
      "loss is 0.6243560037851875\n",
      "epoch 149 completed\n",
      "loss is 0.6242792833806953\n",
      "epoch 150 completed\n",
      "loss is 0.6242060281470442\n",
      "epoch 151 completed\n",
      "loss is 0.6241361138571571\n",
      "epoch 152 completed\n",
      "loss is 0.6240694107771084\n",
      "epoch 153 completed\n",
      "loss is 0.6240057831646962\n",
      "epoch 154 completed\n",
      "loss is 0.6239450888275072\n",
      "epoch 155 completed\n",
      "loss is 0.6238871787441899\n",
      "epoch 156 completed\n",
      "loss is 0.6238318967517246\n",
      "epoch 157 completed\n",
      "loss is 0.6237790793005124\n",
      "epoch 158 completed\n",
      "loss is 0.6237285552779404\n",
      "epoch 159 completed\n",
      "loss is 0.6236801458998973\n",
      "epoch 160 completed\n",
      "loss is 0.6236336646683678\n",
      "epoch 161 completed\n",
      "loss is 0.6235889173918454\n",
      "epoch 162 completed\n",
      "loss is 0.6235457022638815\n",
      "epoch 163 completed\n",
      "loss is 0.6235038099936177\n",
      "epoch 164 completed\n",
      "loss is 0.6234630239807598\n",
      "epoch 165 completed\n",
      "loss is 0.623423120526107\n",
      "epoch 166 completed\n",
      "loss is 0.623383869067611\n",
      "epoch 167 completed\n",
      "loss is 0.6233450324309736\n",
      "epoch 168 completed\n",
      "loss is 0.6233063670832281\n",
      "epoch 169 completed\n",
      "loss is 0.6232676233775902\n",
      "epoch 170 completed\n",
      "loss is 0.623228545778253\n",
      "epoch 171 completed\n",
      "loss is 0.6231888730550206\n",
      "epoch 172 completed\n",
      "loss is 0.6231483384396417\n",
      "epoch 173 completed\n",
      "loss is 0.6231066697388987\n",
      "epoch 174 completed\n",
      "loss is 0.6230635894037785\n",
      "epoch 175 completed\n",
      "loss is 0.6230188145597878\n",
      "epoch 176 completed\n",
      "loss is 0.6229720570105388\n",
      "epoch 177 completed\n",
      "loss is 0.6229230232350952\n",
      "epoch 178 completed\n",
      "loss is 0.6228714144089189\n",
      "epoch 179 completed\n",
      "loss is 0.6228169264880493\n",
      "epoch 180 completed\n",
      "loss is 0.6227592504053757\n",
      "epoch 181 completed\n",
      "loss is 0.622698072435459\n",
      "epoch 182 completed\n",
      "loss is 0.622633074788591\n",
      "epoch 183 completed\n",
      "loss is 0.6225639364940776\n",
      "epoch 184 completed\n",
      "loss is 0.6224903346253923\n",
      "epoch 185 completed\n",
      "loss is 0.622411945904723\n",
      "epoch 186 completed\n",
      "loss is 0.6223284487010728\n",
      "epoch 187 completed\n",
      "loss is 0.622239525405455\n",
      "epoch 188 completed\n",
      "loss is 0.6221448651310295\n",
      "epoch 189 completed\n",
      "loss is 0.6220441666490846\n",
      "epoch 190 completed\n",
      "loss is 0.6219371414383906\n",
      "epoch 191 completed\n",
      "loss is 0.6218235167007213\n",
      "epoch 192 completed\n",
      "loss is 0.6217030381837407\n",
      "epoch 193 completed\n",
      "loss is 0.6215754726568895\n",
      "epoch 194 completed\n",
      "loss is 0.6214406099065759\n",
      "epoch 195 completed\n",
      "loss is 0.6212982641519701\n",
      "epoch 196 completed\n",
      "loss is 0.621148274827303\n",
      "epoch 197 completed\n",
      "loss is 0.6209905067250979\n",
      "epoch 198 completed\n",
      "loss is 0.620824849540978\n",
      "epoch 199 completed\n",
      "loss is 0.6206512168994286\n",
      "epoch 200 completed\n",
      "loss is 0.6204695449674162\n",
      "epoch 201 completed\n",
      "loss is 0.6202797907775724\n",
      "epoch 202 completed\n",
      "loss is 0.6200819303849874\n",
      "epoch 203 completed\n",
      "loss is 0.619875956973487\n",
      "epoch 204 completed\n",
      "loss is 0.6196618790111809\n",
      "epoch 205 completed\n",
      "loss is 0.6194397185343898\n",
      "epoch 206 completed\n",
      "loss is 0.6192095096164593\n",
      "epoch 207 completed\n",
      "loss is 0.6189712970560124\n",
      "epoch 208 completed\n",
      "loss is 0.6187251352994984\n",
      "epoch 209 completed\n",
      "loss is 0.6184710875965884\n",
      "epoch 210 completed\n",
      "loss is 0.6182092253743611\n",
      "epoch 211 completed\n",
      "loss is 0.6179396278074288\n",
      "epoch 212 completed\n",
      "loss is 0.6176623815556981\n",
      "epoch 213 completed\n",
      "loss is 0.6173775806389391\n",
      "epoch 214 completed\n",
      "loss is 0.617085326416962\n",
      "epoch 215 completed\n",
      "loss is 0.6167857276456341\n",
      "epoch 216 completed\n",
      "loss is 0.6164789005814103\n",
      "epoch 217 completed\n",
      "loss is 0.6161649691102624\n",
      "epoch 218 completed\n",
      "loss is 0.6158440648803694\n",
      "epoch 219 completed\n",
      "loss is 0.6155163274215076\n",
      "epoch 220 completed\n",
      "loss is 0.6151819042374863\n",
      "epoch 221 completed\n",
      "loss is 0.6148409508611538\n",
      "epoch 222 completed\n",
      "loss is 0.6144936308643905\n",
      "epoch 223 completed\n",
      "loss is 0.6141401158179506\n",
      "epoch 224 completed\n",
      "loss is 0.6137805851982392\n",
      "epoch 225 completed\n",
      "loss is 0.6134152262398902\n",
      "epoch 226 completed\n",
      "loss is 0.6130442337345734\n",
      "epoch 227 completed\n",
      "loss is 0.6126678097776592\n",
      "epoch 228 completed\n",
      "loss is 0.6122861634654286\n",
      "epoch 229 completed\n",
      "loss is 0.6118995105462829\n",
      "epoch 230 completed\n",
      "loss is 0.6115080730300341\n",
      "epoch 231 completed\n",
      "loss is 0.6111120787598895\n",
      "epoch 232 completed\n",
      "loss is 0.6107117609520094\n",
      "epoch 233 completed\n",
      "loss is 0.6103073577078635\n",
      "epoch 234 completed\n",
      "loss is 0.6098991115047032\n",
      "epoch 235 completed\n",
      "loss is 0.6094872686696294\n",
      "epoch 236 completed\n",
      "loss is 0.6090720788426985\n",
      "epoch 237 completed\n",
      "loss is 0.60865379443459\n",
      "epoch 238 completed\n",
      "loss is 0.6082326700842069\n",
      "epoch 239 completed\n",
      "loss is 0.6078089621215929\n",
      "epoch 240 completed\n",
      "loss is 0.6073829280413682\n",
      "epoch 241 completed\n",
      "loss is 0.6069548259918156\n",
      "epoch 242 completed\n",
      "loss is 0.6065249142845456\n",
      "epoch 243 completed\n",
      "loss is 0.6060934509295615\n",
      "epoch 244 completed\n",
      "loss is 0.605660693200328\n",
      "epoch 245 completed\n",
      "loss is 0.6052268972333091\n",
      "epoch 246 completed\n",
      "loss is 0.6047923176661674\n",
      "epoch 247 completed\n",
      "loss is 0.6043572073186881\n",
      "epoch 248 completed\n",
      "loss is 0.6039218169201519\n",
      "epoch 249 completed\n",
      "loss is 0.6034863948866714\n",
      "epoch 250 completed\n",
      "loss is 0.6030511871516329\n",
      "epoch 251 completed\n",
      "loss is 0.6026164370520306\n",
      "epoch 252 completed\n",
      "loss is 0.6021823852729877\n",
      "epoch 253 completed\n",
      "loss is 0.6017492698522315\n",
      "epoch 254 completed\n",
      "loss is 0.60131732624559\n",
      "epoch 255 completed\n",
      "loss is 0.6008867874537926\n",
      "epoch 256 completed\n",
      "loss is 0.6004578842098794\n",
      "epoch 257 completed\n",
      "loss is 0.6000308452253784\n",
      "epoch 258 completed\n",
      "loss is 0.5996058974920854\n",
      "epoch 259 completed\n",
      "loss is 0.5991832666347089\n",
      "epoch 260 completed\n",
      "loss is 0.5987631773078869\n",
      "epoch 261 completed\n",
      "loss is 0.5983458536291085\n",
      "epoch 262 completed\n",
      "loss is 0.5979315196368405\n",
      "epoch 263 completed\n",
      "loss is 0.5975203997607724\n",
      "epoch 264 completed\n",
      "loss is 0.5971127192885483\n",
      "epoch 265 completed\n",
      "loss is 0.5967087048106018\n",
      "epoch 266 completed\n",
      "loss is 0.5963085846220347\n",
      "epoch 267 completed\n",
      "loss is 0.5959125890576136\n",
      "epoch 268 completed\n",
      "loss is 0.5955209507334027\n",
      "epoch 269 completed\n",
      "loss is 0.5951339046660518\n",
      "epoch 270 completed\n",
      "loss is 0.5947516882387361\n",
      "epoch 271 completed\n",
      "loss is 0.5943745409813369\n",
      "epoch 272 completed\n",
      "loss is 0.5940027041318101\n",
      "epoch 273 completed\n",
      "loss is 0.5936364199464573\n",
      "epoch 274 completed\n",
      "loss is 0.5932759307289733\n",
      "epoch 275 completed\n",
      "loss is 0.5929214775526795\n",
      "epoch 276 completed\n",
      "loss is 0.5925732986573637\n",
      "epoch 277 completed\n",
      "loss is 0.5922316275124271\n",
      "epoch 278 completed\n",
      "loss is 0.5918966905518224\n",
      "epoch 279 completed\n",
      "loss is 0.5915687046037842\n",
      "epoch 280 completed\n",
      "loss is 0.5912478740592472\n",
      "epoch 281 completed\n",
      "loss is 0.59093438784629\n",
      "epoch 282 completed\n",
      "loss is 0.5906284163023531\n",
      "epoch 283 completed\n",
      "loss is 0.5903301080590451\n",
      "epoch 284 completed\n",
      "loss is 0.5900395870730929\n",
      "epoch 285 completed\n",
      "loss is 0.5897569499482355\n",
      "epoch 286 completed\n",
      "loss is 0.5894822636935796\n",
      "epoch 287 completed\n",
      "loss is 0.5892155640518381\n",
      "epoch 288 completed\n",
      "loss is 0.5889568545054544\n",
      "epoch 289 completed\n",
      "loss is 0.5887061060308917\n",
      "epoch 290 completed\n",
      "loss is 0.5884632576247231\n",
      "epoch 291 completed\n",
      "loss is 0.588228217574553\n",
      "epoch 292 completed\n",
      "loss is 0.5880008653991342\n",
      "epoch 293 completed\n",
      "loss is 0.5877810543412886\n",
      "epoch 294 completed\n",
      "loss is 0.5875686142690538\n",
      "epoch 295 completed\n",
      "loss is 0.5873633548277386\n",
      "epoch 296 completed\n",
      "loss is 0.5871650686886505\n",
      "epoch 297 completed\n",
      "loss is 0.5869735347572369\n",
      "epoch 298 completed\n",
      "loss is 0.586788521230784\n",
      "epoch 299 completed\n",
      "loss is 0.5866097884287534\n",
      "epoch 300 completed\n",
      "loss is 0.5864370913528756\n",
      "epoch 301 completed\n",
      "loss is 0.5862701819648533\n",
      "epoch 302 completed\n",
      "loss is 0.5861088111940013\n",
      "epoch 303 completed\n",
      "loss is 0.5859527307036989\n",
      "epoch 304 completed\n",
      "loss is 0.5858016944536436\n",
      "epoch 305 completed\n",
      "loss is 0.585655460095371\n",
      "epoch 306 completed\n",
      "loss is 0.5855137902328565\n",
      "epoch 307 completed\n",
      "loss is 0.5853764535705054\n",
      "epoch 308 completed\n",
      "loss is 0.5852432259594641\n",
      "epoch 309 completed\n",
      "loss is 0.5851138913422422\n",
      "epoch 310 completed\n",
      "loss is 0.5849882425868257\n",
      "epoch 311 completed\n",
      "loss is 0.584866082195658\n",
      "epoch 312 completed\n",
      "loss is 0.5847472228730113\n",
      "epoch 313 completed\n",
      "loss is 0.5846314879356169\n",
      "epoch 314 completed\n",
      "loss is 0.5845187115560603\n",
      "epoch 315 completed\n",
      "loss is 0.5844087388347164\n",
      "epoch 316 completed\n",
      "loss is 0.5843014257034032\n",
      "epoch 317 completed\n",
      "loss is 0.5841966386711505\n",
      "epoch 318 completed\n",
      "loss is 0.5840942544286618\n",
      "epoch 319 completed\n",
      "loss is 0.5839941593328962\n",
      "epoch 320 completed\n",
      "loss is 0.5838962487960839\n",
      "epoch 321 completed\n",
      "loss is 0.5838004266046712\n",
      "epoch 322 completed\n",
      "loss is 0.5837066041932302\n",
      "epoch 323 completed\n",
      "loss is 0.5836146998965638\n",
      "epoch 324 completed\n",
      "loss is 0.583524638200565\n",
      "epoch 325 completed\n",
      "loss is 0.5834363490090863\n",
      "epoch 326 completed\n",
      "loss is 0.5833497669405611\n",
      "epoch 327 completed\n",
      "loss is 0.5832648306647479\n",
      "epoch 328 completed\n",
      "loss is 0.5831814822866583\n",
      "epoch 329 completed\n",
      "loss is 0.583099666781992\n",
      "epoch 330 completed\n",
      "loss is 0.5830193314859243\n",
      "epoch 331 completed\n",
      "loss is 0.5829404256352788\n",
      "epoch 332 completed\n",
      "loss is 0.582862899962516\n",
      "epoch 333 completed\n",
      "loss is 0.5827867063389551\n",
      "epoch 334 completed\n",
      "loss is 0.5827117974639009\n",
      "epoch 335 completed\n",
      "loss is 0.5826381265958853\n",
      "epoch 336 completed\n",
      "loss is 0.5825656473220298\n",
      "epoch 337 completed\n",
      "loss is 0.5824943133615256\n",
      "epoch 338 completed\n",
      "loss is 0.5824240783992716\n",
      "epoch 339 completed\n",
      "loss is 0.5823548959459885\n",
      "epoch 340 completed\n",
      "loss is 0.5822867192212725\n",
      "epoch 341 completed\n",
      "loss is 0.5822195010564581\n",
      "epoch 342 completed\n",
      "loss is 0.5821531938143565\n",
      "epoch 343 completed\n",
      "loss is 0.5820877493233405\n",
      "epoch 344 completed\n",
      "loss is 0.5820231188234308\n",
      "epoch 345 completed\n",
      "loss is 0.5819592529224473\n",
      "epoch 346 completed\n",
      "loss is 0.5818961015604599\n",
      "epoch 347 completed\n",
      "loss is 0.5818336139810187\n",
      "epoch 348 completed\n",
      "loss is 0.5817717387079038\n",
      "epoch 349 completed\n",
      "loss is 0.581710423526325\n",
      "epoch 350 completed\n",
      "loss is 0.5816496154676154\n",
      "epoch 351 completed\n",
      "loss is 0.58158926079667\n",
      "epoch 352 completed\n",
      "loss is 0.581529305001533\n",
      "epoch 353 completed\n",
      "loss is 0.5814696927845876\n",
      "epoch 354 completed\n",
      "loss is 0.5814103680549347\n",
      "epoch 355 completed\n",
      "loss is 0.5813512739216946\n",
      "epoch 356 completed\n",
      "loss is 0.5812923526879307\n",
      "epoch 357 completed\n",
      "loss is 0.5812335458450779\n",
      "epoch 358 completed\n",
      "loss is 0.5811747940677021\n",
      "epoch 359 completed\n",
      "loss is 0.5811160372086053\n",
      "epoch 360 completed\n",
      "loss is 0.5810572142941776\n",
      "epoch 361 completed\n",
      "loss is 0.5809982635200901\n",
      "epoch 362 completed\n",
      "loss is 0.5809391222473257\n",
      "epoch 363 completed\n",
      "loss is 0.5808797269986777\n",
      "epoch 364 completed\n",
      "loss is 0.5808200134558041\n",
      "epoch 365 completed\n",
      "loss is 0.5807599164569724\n",
      "epoch 366 completed\n",
      "loss is 0.5806993699956617\n",
      "epoch 367 completed\n",
      "loss is 0.5806383072201856\n",
      "epoch 368 completed\n",
      "loss is 0.5805766604345385\n",
      "epoch 369 completed\n",
      "loss is 0.5805143611006285\n",
      "epoch 370 completed\n",
      "loss is 0.580451339842176\n",
      "epoch 371 completed\n",
      "loss is 0.5803875264504444\n",
      "epoch 372 completed\n",
      "loss is 0.5803228498920568\n",
      "epoch 373 completed\n",
      "loss is 0.5802572383191451\n",
      "epoch 374 completed\n",
      "loss is 0.5801906190820657\n",
      "epoch 375 completed\n",
      "loss is 0.5801229187449221\n",
      "epoch 376 completed\n",
      "loss is 0.5800540631041798\n",
      "epoch 377 completed\n",
      "loss is 0.5799839772105456\n",
      "epoch 378 completed\n",
      "loss is 0.5799125853944322\n",
      "epoch 379 completed\n",
      "loss is 0.5798398112952174\n",
      "epoch 380 completed\n",
      "loss is 0.579765577894491\n",
      "epoch 381 completed\n",
      "loss is 0.5796898075535714\n",
      "epoch 382 completed\n",
      "loss is 0.5796124220554447\n",
      "epoch 383 completed\n",
      "loss is 0.5795333426513415\n",
      "epoch 384 completed\n",
      "loss is 0.5794524901121243\n",
      "epoch 385 completed\n",
      "loss is 0.5793697847845968\n",
      "epoch 386 completed\n",
      "loss is 0.5792851466529082\n",
      "epoch 387 completed\n",
      "loss is 0.5791984954051034\n",
      "epoch 388 completed\n",
      "loss is 0.5791097505048987\n",
      "epoch 389 completed\n",
      "loss is 0.5790188312686984\n",
      "epoch 390 completed\n",
      "loss is 0.5789256569478577\n",
      "epoch 391 completed\n",
      "loss is 0.5788301468161269\n",
      "epoch 392 completed\n",
      "loss is 0.5787322202621643\n",
      "epoch 393 completed\n",
      "loss is 0.5786317968870246\n",
      "epoch 394 completed\n",
      "loss is 0.5785287966063791\n",
      "epoch 395 completed\n",
      "loss is 0.5784231397572864\n",
      "epoch 396 completed\n",
      "loss is 0.5783147472091688\n",
      "epoch 397 completed\n",
      "loss is 0.5782035404787181\n",
      "epoch 398 completed\n",
      "loss is 0.5780894418483057\n",
      "epoch 399 completed\n",
      "loss is 0.577972374487494\n",
      "epoch 400 completed\n",
      "loss is 0.5778522625771423\n",
      "epoch 401 completed\n",
      "loss is 0.5777290314356289\n",
      "epoch 402 completed\n",
      "loss is 0.5776026076465797\n",
      "epoch 403 completed\n",
      "loss is 0.5774729191875617\n",
      "epoch 404 completed\n",
      "loss is 0.5773398955590653\n",
      "epoch 405 completed\n",
      "loss is 0.5772034679131794\n",
      "epoch 406 completed\n",
      "loss is 0.5770635691812629\n",
      "epoch 407 completed\n",
      "loss is 0.5769201341999247\n",
      "epoch 408 completed\n",
      "loss is 0.5767730998346906\n",
      "epoch 409 completed\n",
      "loss is 0.5766224051006071\n",
      "epoch 410 completed\n",
      "loss is 0.5764679912792307\n",
      "epoch 411 completed\n",
      "loss is 0.5763098020312549\n",
      "epoch 412 completed\n",
      "loss is 0.5761477835042745\n",
      "epoch 413 completed\n",
      "loss is 0.5759818844350667\n",
      "epoch 414 completed\n",
      "loss is 0.5758120562459147\n",
      "epoch 415 completed\n",
      "loss is 0.5756382531344983\n",
      "epoch 416 completed\n",
      "loss is 0.5754604321569854\n",
      "epoch 417 completed\n",
      "loss is 0.5752785533039875\n",
      "epoch 418 completed\n",
      "loss is 0.5750925795691325\n",
      "epoch 419 completed\n",
      "loss is 0.5749024770101127\n",
      "epoch 420 completed\n",
      "loss is 0.5747082148021068\n",
      "epoch 421 completed\n",
      "loss is 0.5745097652835645\n",
      "epoch 422 completed\n",
      "loss is 0.574307103994486\n",
      "epoch 423 completed\n",
      "loss is 0.5741002097072959\n",
      "epoch 424 completed\n",
      "loss is 0.5738890644506319\n",
      "epoch 425 completed\n",
      "loss is 0.5736736535263331\n",
      "epoch 426 completed\n",
      "loss is 0.5734539655200456\n",
      "epoch 427 completed\n",
      "loss is 0.5732299923059399\n",
      "epoch 428 completed\n",
      "loss is 0.5730017290460689\n",
      "epoch 429 completed\n",
      "loss is 0.5727691741849475\n",
      "epoch 430 completed\n",
      "loss is 0.5725323294400618\n",
      "epoch 431 completed\n",
      "loss is 0.5722911997889697\n",
      "epoch 432 completed\n",
      "loss is 0.5720457934537232\n",
      "epoch 433 completed\n",
      "loss is 0.5717961218834279\n",
      "epoch 434 completed\n",
      "loss is 0.5715421997356767\n",
      "epoch 435 completed\n",
      "loss is 0.5712840448577253\n",
      "epoch 436 completed\n",
      "loss is 0.5710216782681091\n",
      "epoch 437 completed\n",
      "loss is 0.57075512413969\n",
      "epoch 438 completed\n",
      "loss is 0.5704844097847304\n",
      "epoch 439 completed\n",
      "loss is 0.5702095656428968\n",
      "epoch 440 completed\n",
      "loss is 0.5699306252729219\n",
      "epoch 441 completed\n",
      "loss is 0.5696476253485595\n",
      "epoch 442 completed\n",
      "loss is 0.5693606056595825\n",
      "epoch 443 completed\n",
      "loss is 0.5690696091183446\n",
      "epoch 444 completed\n",
      "loss is 0.5687746817724741\n",
      "epoch 445 completed\n",
      "loss is 0.568475872824085\n",
      "epoch 446 completed\n",
      "loss is 0.5681732346559318\n",
      "epoch 447 completed\n",
      "loss is 0.5678668228646514\n",
      "epoch 448 completed\n",
      "loss is 0.5675566963012728\n",
      "epoch 449 completed\n",
      "loss is 0.5672429171189625\n",
      "epoch 450 completed\n",
      "loss is 0.5669255508278168\n",
      "epoch 451 completed\n",
      "loss is 0.5666046663564207\n",
      "epoch 452 completed\n",
      "loss is 0.5662803361196548\n",
      "epoch 453 completed\n",
      "loss is 0.5659526360920952\n",
      "epoch 454 completed\n",
      "loss is 0.5656216458861456\n",
      "epoch 455 completed\n",
      "loss is 0.5652874488338492\n",
      "epoch 456 completed\n",
      "loss is 0.5649501320711506\n",
      "epoch 457 completed\n",
      "loss is 0.5646097866231141\n",
      "epoch 458 completed\n",
      "loss is 0.5642665074885092\n",
      "epoch 459 completed\n",
      "loss is 0.5639203937218794\n",
      "epoch 460 completed\n",
      "loss is 0.5635715485111089\n",
      "epoch 461 completed\n",
      "loss is 0.5632200792482883\n",
      "epoch 462 completed\n",
      "loss is 0.5628660975915852\n",
      "epoch 463 completed\n",
      "loss is 0.562509719515635\n",
      "epoch 464 completed\n",
      "loss is 0.5621510653480006\n",
      "epoch 465 completed\n",
      "loss is 0.5617902597890283\n",
      "epoch 466 completed\n",
      "loss is 0.5614274319126479\n",
      "epoch 467 completed\n",
      "loss is 0.561062715145457\n",
      "epoch 468 completed\n",
      "loss is 0.560696247221706\n",
      "epoch 469 completed\n",
      "loss is 0.5603281701118863\n",
      "epoch 470 completed\n",
      "loss is 0.5599586299227024\n",
      "epoch 471 completed\n",
      "loss is 0.5595877767667143\n",
      "epoch 472 completed\n",
      "loss is 0.5592157645999183\n",
      "epoch 473 completed\n",
      "loss is 0.5588427510261633\n",
      "epoch 474 completed\n",
      "loss is 0.558468897067523\n",
      "epoch 475 completed\n",
      "loss is 0.5580943669002888\n",
      "epoch 476 completed\n",
      "loss is 0.5577193275566107\n",
      "epoch 477 completed\n",
      "loss is 0.5573439485924916\n",
      "epoch 478 completed\n",
      "loss is 0.5569684017231348\n",
      "epoch 479 completed\n",
      "loss is 0.556592860427397\n",
      "epoch 480 completed\n",
      "loss is 0.5562174995234138\n",
      "epoch 481 completed\n",
      "loss is 0.5558424947181548\n",
      "epoch 482 completed\n",
      "loss is 0.5554680221339698\n",
      "epoch 483 completed\n",
      "loss is 0.5550942578157806\n",
      "epoch 484 completed\n",
      "loss is 0.5547213772228142\n",
      "epoch 485 completed\n",
      "loss is 0.5543495547092152\n",
      "epoch 486 completed\n",
      "loss is 0.553978962998039\n",
      "epoch 487 completed\n",
      "loss is 0.5536097726533801\n",
      "epoch 488 completed\n",
      "loss is 0.553242151555439\n",
      "epoch 489 completed\n",
      "loss is 0.5528762643833803\n",
      "epoch 490 completed\n",
      "loss is 0.5525122721106965\n",
      "epoch 491 completed\n",
      "loss is 0.5521503315178052\n",
      "epoch 492 completed\n",
      "loss is 0.5517905947261544\n",
      "epoch 493 completed\n",
      "loss is 0.5514332087580215\n",
      "epoch 494 completed\n",
      "loss is 0.5510783151256926\n",
      "epoch 495 completed\n",
      "loss is 0.5507260494533691\n",
      "epoch 496 completed\n",
      "loss is 0.5503765411346271\n",
      "epoch 497 completed\n",
      "loss is 0.5500299130278127\n",
      "epoch 498 completed\n",
      "loss is 0.5496862811912152\n",
      "epoch 499 completed\n",
      "loss is 0.5493457546592758\n",
      "epoch 500 completed\n",
      "loss is 0.5490084352606952\n",
      "epoch 501 completed\n",
      "loss is 0.5486744174786204\n",
      "epoch 502 completed\n",
      "loss is 0.5483437883527259\n",
      "epoch 503 completed\n",
      "loss is 0.5480166274224261\n",
      "epoch 504 completed\n",
      "loss is 0.5476930067100904\n",
      "epoch 505 completed\n",
      "loss is 0.5473729907427249\n",
      "epoch 506 completed\n",
      "loss is 0.5470566366102083\n",
      "epoch 507 completed\n",
      "loss is 0.5467439940578627\n",
      "epoch 508 completed\n",
      "loss is 0.5464351056109619\n",
      "epoch 509 completed\n",
      "loss is 0.5461300067284757\n",
      "epoch 510 completed\n",
      "loss is 0.5458287259832776\n",
      "epoch 511 completed\n",
      "loss is 0.5455312852659211\n",
      "epoch 512 completed\n",
      "loss is 0.5452377000090274\n",
      "epoch 513 completed\n",
      "loss is 0.5449479794293243\n",
      "epoch 514 completed\n",
      "loss is 0.5446621267843541\n",
      "epoch 515 completed\n",
      "loss is 0.5443801396411045\n",
      "epoch 516 completed\n",
      "loss is 0.5441020101536179\n",
      "epoch 517 completed\n",
      "loss is 0.5438277253470896\n",
      "epoch 518 completed\n",
      "loss is 0.5435572674058073\n",
      "epoch 519 completed\n",
      "loss is 0.5432906139627365\n",
      "epoch 520 completed\n",
      "loss is 0.5430277383884141\n",
      "epoch 521 completed\n",
      "loss is 0.542768610077348\n",
      "epoch 522 completed\n",
      "loss is 0.5425131947300246\n",
      "epoch 523 completed\n",
      "loss is 0.5422614546290014\n",
      "epoch 524 completed\n",
      "loss is 0.5420133489077444\n",
      "epoch 525 completed\n",
      "loss is 0.5417688338109286\n",
      "epoch 526 completed\n",
      "loss is 0.5415278629453864\n",
      "epoch 527 completed\n",
      "loss is 0.541290387520838\n",
      "epoch 528 completed\n",
      "loss is 0.5410563565798119\n",
      "epoch 529 completed\n",
      "loss is 0.5408257172164348\n",
      "epoch 530 completed\n",
      "loss is 0.5405984147837662\n",
      "epoch 531 completed\n",
      "loss is 0.5403743930896819\n",
      "epoch 532 completed\n",
      "loss is 0.5401535945813396\n",
      "epoch 533 completed\n",
      "loss is 0.5399359605184612\n",
      "epoch 534 completed\n",
      "loss is 0.5397214311357509\n",
      "epoch 535 completed\n",
      "loss is 0.5395099457949476\n",
      "epoch 536 completed\n",
      "loss is 0.5393014431269508\n",
      "epoch 537 completed\n",
      "loss is 0.5390958611647498\n",
      "epoch 538 completed\n",
      "loss is 0.5388931374677449\n",
      "epoch 539 completed\n",
      "loss is 0.5386932092382258\n",
      "epoch 540 completed\n",
      "loss is 0.5384960134307408\n",
      "epoch 541 completed\n",
      "loss is 0.5383014868551129\n",
      "epoch 542 completed\n",
      "loss is 0.5381095662738813\n",
      "epoch 543 completed\n",
      "loss is 0.5379201884948727\n",
      "epoch 544 completed\n",
      "loss is 0.537733290459606\n",
      "epoch 545 completed\n",
      "loss is 0.5375488093281928\n",
      "epoch 546 completed\n",
      "loss is 0.5373666825612885\n",
      "epoch 547 completed\n",
      "loss is 0.5371868479996426\n",
      "epoch 548 completed\n",
      "loss is 0.5370092439416263\n",
      "epoch 549 completed\n",
      "loss is 0.5368338092191448\n",
      "epoch 550 completed\n",
      "loss is 0.5366604832721031\n",
      "epoch 551 completed\n",
      "loss is 0.536489206221625\n",
      "epoch 552 completed\n",
      "loss is 0.536319918942094\n",
      "epoch 553 completed\n",
      "loss is 0.5361525631319556\n",
      "epoch 554 completed\n",
      "loss is 0.5359870813831706\n",
      "epoch 555 completed\n",
      "loss is 0.535823417249155\n",
      "epoch 556 completed\n",
      "loss is 0.535661515310911\n",
      "epoch 557 completed\n",
      "loss is 0.5355013212410985\n",
      "epoch 558 completed\n",
      "loss is 0.5353427818656331\n",
      "epoch 559 completed\n",
      "loss is 0.5351858452224643\n",
      "epoch 560 completed\n",
      "loss is 0.535030460617164\n",
      "epoch 561 completed\n",
      "loss is 0.5348765786748523\n",
      "epoch 562 completed\n",
      "loss is 0.5347241513881782\n",
      "epoch 563 completed\n",
      "loss is 0.5345731321609167\n",
      "epoch 564 completed\n",
      "loss is 0.5344234758468775\n",
      "epoch 565 completed\n",
      "loss is 0.5342751387838726\n",
      "epoch 566 completed\n",
      "loss is 0.5341280788224453\n",
      "epoch 567 completed\n",
      "loss is 0.5339822553492449\n",
      "epoch 568 completed\n",
      "loss is 0.5338376293048744\n",
      "epoch 569 completed\n",
      "loss is 0.5336941631961927\n",
      "epoch 570 completed\n",
      "loss is 0.5335518211030283\n",
      "epoch 571 completed\n",
      "loss is 0.5334105686794037\n",
      "epoch 572 completed\n",
      "loss is 0.5332703731493744\n",
      "epoch 573 completed\n",
      "loss is 0.5331312032976175\n",
      "epoch 574 completed\n",
      "loss is 0.5329930294550358\n",
      "epoch 575 completed\n",
      "loss is 0.5328558234796111\n",
      "epoch 576 completed\n",
      "loss is 0.5327195587327958\n",
      "epoch 577 completed\n",
      "loss is 0.5325842100517496\n",
      "epoch 578 completed\n",
      "loss is 0.5324497537178299\n",
      "epoch 579 completed\n",
      "loss is 0.5323161674216197\n",
      "epoch 580 completed\n",
      "loss is 0.5321834302249164\n",
      "epoch 581 completed\n",
      "loss is 0.5320515225200316\n",
      "epoch 582 completed\n",
      "loss is 0.531920425986813\n",
      "epoch 583 completed\n",
      "loss is 0.5317901235477012\n",
      "epoch 584 completed\n",
      "loss is 0.5316605993212514\n",
      "epoch 585 completed\n",
      "loss is 0.531531838574427\n",
      "epoch 586 completed\n",
      "loss is 0.5314038276739925\n",
      "epoch 587 completed\n",
      "loss is 0.5312765540373501\n",
      "epoch 588 completed\n",
      "loss is 0.5311500060830923\n",
      "epoch 589 completed\n",
      "loss is 0.5310241731815462\n",
      "epoch 590 completed\n",
      "loss is 0.5308990456055753\n",
      "epoch 591 completed\n",
      "loss is 0.5307746144818263\n",
      "epoch 592 completed\n",
      "loss is 0.5306508717426841\n",
      "epoch 593 completed\n",
      "loss is 0.5305278100790363\n",
      "epoch 594 completed\n",
      "loss is 0.5304054228941153\n",
      "epoch 595 completed\n",
      "loss is 0.5302837042584251\n",
      "epoch 596 completed\n",
      "loss is 0.5301626488659799\n",
      "epoch 597 completed\n",
      "loss is 0.5300422519919061\n",
      "epoch 598 completed\n",
      "loss is 0.5299225094514709\n",
      "epoch 599 completed\n",
      "loss is 0.5298034175606341\n",
      "epoch 600 completed\n",
      "loss is 0.5296849730981551\n",
      "epoch 601 completed\n",
      "loss is 0.5295671732692956\n",
      "epoch 602 completed\n",
      "loss is 0.5294500156711048\n",
      "epoch 603 completed\n",
      "loss is 0.5293334982593556\n",
      "epoch 604 completed\n",
      "loss is 0.529217619317072\n",
      "epoch 605 completed\n",
      "loss is 0.5291023774246791\n",
      "epoch 606 completed\n",
      "loss is 0.528987771431714\n",
      "epoch 607 completed\n",
      "loss is 0.5288738004301278\n",
      "epoch 608 completed\n",
      "loss is 0.5287604637290764\n",
      "epoch 609 completed\n",
      "loss is 0.5286477608312317\n",
      "epoch 610 completed\n",
      "loss is 0.5285356914105142\n",
      "epoch 611 completed\n",
      "loss is 0.5284242552912465\n",
      "epoch 612 completed\n",
      "loss is 0.5283134524286519\n",
      "epoch 613 completed\n",
      "loss is 0.5282032828906676\n",
      "epoch 614 completed\n",
      "loss is 0.5280937468409738\n",
      "epoch 615 completed\n",
      "loss is 0.5279848445232741\n",
      "epoch 616 completed\n",
      "loss is 0.5278765762466886\n",
      "epoch 617 completed\n",
      "loss is 0.5277689423722407\n",
      "epoch 618 completed\n",
      "loss is 0.5276619433004095\n",
      "epoch 619 completed\n",
      "loss is 0.5275555794596317\n",
      "epoch 620 completed\n",
      "loss is 0.5274498512957683\n",
      "epoch 621 completed\n",
      "loss is 0.5273447592624502\n",
      "epoch 622 completed\n",
      "loss is 0.527240303812243\n",
      "epoch 623 completed\n",
      "loss is 0.5271364853886137\n",
      "epoch 624 completed\n",
      "loss is 0.5270333044186343\n",
      "epoch 625 completed\n",
      "loss is 0.5269307613063704\n",
      "epoch 626 completed\n",
      "loss is 0.5268288564269129\n",
      "epoch 627 completed\n",
      "loss is 0.5267275901210399\n",
      "epoch 628 completed\n",
      "loss is 0.5266269626904212\n",
      "epoch 629 completed\n",
      "loss is 0.526526974393353\n",
      "epoch 630 completed\n",
      "loss is 0.5264276254409923\n",
      "epoch 631 completed\n",
      "loss is 0.5263289159940234\n",
      "epoch 632 completed\n",
      "loss is 0.5262308461597699\n",
      "epoch 633 completed\n",
      "loss is 0.5261334159896514\n",
      "epoch 634 completed\n",
      "loss is 0.5260366254770502\n",
      "epoch 635 completed\n",
      "loss is 0.5259404745554527\n",
      "epoch 636 completed\n",
      "loss is 0.5258449630969053\n",
      "epoch 637 completed\n",
      "loss is 0.5257500909107649\n",
      "epoch 638 completed\n",
      "loss is 0.5256558577426441\n",
      "epoch 639 completed\n",
      "loss is 0.5255622632736362\n",
      "epoch 640 completed\n",
      "loss is 0.5254693071197002\n",
      "epoch 641 completed\n",
      "loss is 0.5253769888312436\n",
      "epoch 642 completed\n",
      "loss is 0.5252853078928572\n",
      "epoch 643 completed\n",
      "loss is 0.5251942637232011\n",
      "epoch 644 completed\n",
      "loss is 0.5251038556749963\n",
      "epoch 645 completed\n",
      "loss is 0.525014083035155\n",
      "epoch 646 completed\n",
      "loss is 0.5249249450249766\n",
      "epoch 647 completed\n",
      "loss is 0.5248364408004476\n",
      "epoch 648 completed\n",
      "loss is 0.5247485694525852\n",
      "epoch 649 completed\n",
      "loss is 0.5246613300078526\n",
      "epoch 650 completed\n",
      "loss is 0.5245747214286087\n",
      "epoch 651 completed\n",
      "loss is 0.5244887426136117\n",
      "epoch 652 completed\n",
      "loss is 0.5244033923985241\n",
      "epoch 653 completed\n",
      "loss is 0.524318669556443\n",
      "epoch 654 completed\n",
      "loss is 0.5242345727984518\n",
      "epoch 655 completed\n",
      "loss is 0.5241511007741625\n",
      "epoch 656 completed\n",
      "loss is 0.5240682520722505\n",
      "epoch 657 completed\n",
      "loss is 0.5239860252209922\n",
      "epoch 658 completed\n",
      "loss is 0.5239044186887848\n",
      "epoch 659 completed\n",
      "loss is 0.52382343088466\n",
      "epoch 660 completed\n",
      "loss is 0.5237430601587394\n",
      "epoch 661 completed\n",
      "loss is 0.523663304802719\n",
      "epoch 662 completed\n",
      "loss is 0.5235841630502865\n",
      "epoch 663 completed\n",
      "loss is 0.5235056330775394\n",
      "epoch 664 completed\n",
      "loss is 0.5234277130033471\n",
      "epoch 665 completed\n",
      "loss is 0.5233504008897105\n",
      "epoch 666 completed\n",
      "loss is 0.5232736947420623\n",
      "epoch 667 completed\n",
      "loss is 0.5231975925095881\n",
      "epoch 668 completed\n",
      "loss is 0.5231220920854505\n",
      "epoch 669 completed\n",
      "loss is 0.5230471913070446\n",
      "epoch 670 completed\n",
      "loss is 0.5229728879561946\n",
      "epoch 671 completed\n",
      "loss is 0.5228991797593501\n",
      "epoch 672 completed\n",
      "loss is 0.5228260643877268\n",
      "epoch 673 completed\n",
      "loss is 0.522753539457469\n",
      "epoch 674 completed\n",
      "loss is 0.5226816025297614\n",
      "epoch 675 completed\n",
      "loss is 0.5226102511109498\n",
      "epoch 676 completed\n",
      "loss is 0.5225394826526494\n",
      "epoch 677 completed\n",
      "loss is 0.5224692945518189\n",
      "epoch 678 completed\n",
      "loss is 0.5223996841508904\n",
      "epoch 679 completed\n",
      "loss is 0.5223306487378313\n",
      "epoch 680 completed\n",
      "loss is 0.5222621855462738\n",
      "epoch 681 completed\n",
      "loss is 0.5221942917556152\n",
      "epoch 682 completed\n",
      "loss is 0.5221269644911583\n",
      "epoch 683 completed\n",
      "loss is 0.522060200824235\n",
      "epoch 684 completed\n",
      "loss is 0.5219939977724132\n",
      "epoch 685 completed\n",
      "loss is 0.5219283522996807\n",
      "epoch 686 completed\n",
      "loss is 0.5218632613166618\n",
      "epoch 687 completed\n",
      "loss is 0.5217987216809319\n",
      "epoch 688 completed\n",
      "loss is 0.5217347301972988\n",
      "epoch 689 completed\n",
      "loss is 0.5216712836181832\n",
      "epoch 690 completed\n",
      "loss is 0.5216083786440292\n",
      "epoch 691 completed\n",
      "loss is 0.5215460119237829\n",
      "epoch 692 completed\n",
      "loss is 0.521484180055412\n",
      "epoch 693 completed\n",
      "loss is 0.5214228795865191\n",
      "epoch 694 completed\n",
      "loss is 0.5213621070149999\n",
      "epoch 695 completed\n",
      "loss is 0.5213018587897771\n",
      "epoch 696 completed\n",
      "loss is 0.5212421313116268\n",
      "epoch 697 completed\n",
      "loss is 0.5211829209340518\n",
      "epoch 698 completed\n",
      "loss is 0.5211242239642825\n",
      "epoch 699 completed\n",
      "loss is 0.5210660366643103\n",
      "epoch 700 completed\n",
      "loss is 0.5210083552520473\n",
      "epoch 701 completed\n",
      "loss is 0.5209511759025502\n",
      "epoch 702 completed\n",
      "loss is 0.5208944947493608\n",
      "epoch 703 completed\n",
      "loss is 0.520838307885894\n",
      "epoch 704 completed\n",
      "loss is 0.5207826113669648\n",
      "epoch 705 completed\n",
      "loss is 0.520727401210395\n",
      "epoch 706 completed\n",
      "loss is 0.5206726733986694\n",
      "epoch 707 completed\n",
      "loss is 0.5206184238807592\n",
      "epoch 708 completed\n",
      "loss is 0.5205646485739723\n",
      "epoch 709 completed\n",
      "loss is 0.5205113433659351\n",
      "epoch 710 completed\n",
      "loss is 0.5204585041166278\n",
      "epoch 711 completed\n",
      "loss is 0.5204061266605526\n",
      "epoch 712 completed\n",
      "loss is 0.5203542068089209\n",
      "epoch 713 completed\n",
      "loss is 0.5203027403519895\n",
      "epoch 714 completed\n",
      "loss is 0.520251723061451\n",
      "epoch 715 completed\n",
      "loss is 0.5202011506928551\n",
      "epoch 716 completed\n",
      "loss is 0.520151018988179\n",
      "epoch 717 completed\n",
      "loss is 0.5201013236783943\n",
      "epoch 718 completed\n",
      "loss is 0.5200520604861475\n",
      "epoch 719 completed\n",
      "loss is 0.520003225128453\n",
      "epoch 720 completed\n",
      "loss is 0.5199548133194789\n",
      "epoch 721 completed\n",
      "loss is 0.5199068207733383\n",
      "epoch 722 completed\n",
      "loss is 0.5198592432069591\n",
      "epoch 723 completed\n",
      "loss is 0.5198120763429535\n",
      "epoch 724 completed\n",
      "loss is 0.5197653159125487\n",
      "epoch 725 completed\n",
      "loss is 0.5197189576584982\n",
      "epoch 726 completed\n",
      "loss is 0.5196729973380438\n",
      "epoch 727 completed\n",
      "loss is 0.5196274307258794\n",
      "epoch 728 completed\n",
      "loss is 0.5195822536170823\n",
      "epoch 729 completed\n",
      "loss is 0.5195374618300879\n",
      "epoch 730 completed\n",
      "loss is 0.5194930512096204\n",
      "epoch 731 completed\n",
      "loss is 0.5194490176296105\n",
      "epoch 732 completed\n",
      "loss is 0.5194053569960727\n",
      "epoch 733 completed\n",
      "loss is 0.519362065249991\n",
      "epoch 734 completed\n",
      "loss is 0.5193191383701147\n",
      "epoch 735 completed\n",
      "loss is 0.5192765723757259\n",
      "epoch 736 completed\n",
      "loss is 0.5192343633293854\n",
      "epoch 737 completed\n",
      "loss is 0.5191925073395453\n",
      "epoch 738 completed\n",
      "loss is 0.5191510005631795\n",
      "epoch 739 completed\n",
      "loss is 0.5191098392082766\n",
      "epoch 740 completed\n",
      "loss is 0.519069019536291\n",
      "epoch 741 completed\n",
      "loss is 0.5190285378645051\n",
      "epoch 742 completed\n",
      "loss is 0.5189883905682924\n",
      "epoch 743 completed\n",
      "loss is 0.5189485740832923\n",
      "epoch 744 completed\n",
      "loss is 0.5189090849074984\n",
      "epoch 745 completed\n",
      "loss is 0.518869919603232\n",
      "epoch 746 completed\n",
      "loss is 0.5188310747990214\n",
      "epoch 747 completed\n",
      "loss is 0.5187925471913564\n",
      "epoch 748 completed\n",
      "loss is 0.5187543335463571\n",
      "epoch 749 completed\n",
      "loss is 0.5187164307012906\n",
      "epoch 750 completed\n",
      "loss is 0.5186788355660094\n",
      "epoch 751 completed\n",
      "loss is 0.5186415451242588\n",
      "epoch 752 completed\n",
      "loss is 0.5186045564348473\n",
      "epoch 753 completed\n",
      "loss is 0.5185678666327311\n",
      "epoch 754 completed\n",
      "loss is 0.5185314729299239\n",
      "epoch 755 completed\n",
      "loss is 0.5184953726163436\n",
      "epoch 756 completed\n",
      "loss is 0.518459563060502\n",
      "epoch 757 completed\n",
      "loss is 0.5184240417100842\n",
      "epoch 758 completed\n",
      "loss is 0.5183888060923858\n",
      "epoch 759 completed\n",
      "loss is 0.5183538538146826\n",
      "epoch 760 completed\n",
      "loss is 0.5183191825644077\n",
      "epoch 761 completed\n",
      "loss is 0.518284790109283\n",
      "epoch 762 completed\n",
      "loss is 0.5182506742972975\n",
      "epoch 763 completed\n",
      "loss is 0.5182168330565682\n",
      "epoch 764 completed\n",
      "loss is 0.5181832643951235\n",
      "epoch 765 completed\n",
      "loss is 0.5181499664005444\n",
      "epoch 766 completed\n",
      "loss is 0.5181169372395211\n",
      "epoch 767 completed\n",
      "loss is 0.5180841751573189\n",
      "epoch 768 completed\n",
      "loss is 0.5180516784771145\n",
      "epoch 769 completed\n",
      "loss is 0.5180194455992668\n",
      "epoch 770 completed\n",
      "loss is 0.5179874750004952\n",
      "epoch 771 completed\n",
      "loss is 0.5179557652329617\n",
      "epoch 772 completed\n",
      "loss is 0.5179243149232569\n",
      "epoch 773 completed\n",
      "loss is 0.5178931227713601\n",
      "epoch 774 completed\n",
      "loss is 0.5178621875494649\n",
      "epoch 775 completed\n",
      "loss is 0.5178315081007846\n",
      "epoch 776 completed\n",
      "loss is 0.5178010833382451\n",
      "epoch 777 completed\n",
      "loss is 0.5177709122431696\n",
      "epoch 778 completed\n",
      "loss is 0.5177409938638666\n",
      "epoch 779 completed\n",
      "loss is 0.5177113273141833\n",
      "epoch 780 completed\n",
      "loss is 0.517681911771992\n",
      "epoch 781 completed\n",
      "loss is 0.517652746477688\n",
      "epoch 782 completed\n",
      "loss is 0.5176238307325626\n",
      "epoch 783 completed\n",
      "loss is 0.5175951638972113\n",
      "epoch 784 completed\n",
      "loss is 0.5175667453899025\n",
      "epoch 785 completed\n",
      "loss is 0.5175385746848646\n",
      "epoch 786 completed\n",
      "loss is 0.517510651310626\n",
      "epoch 787 completed\n",
      "loss is 0.5174829748482741\n",
      "epoch 788 completed\n",
      "loss is 0.5174555449297396\n",
      "epoch 789 completed\n",
      "loss is 0.5174283612360517\n",
      "epoch 790 completed\n",
      "loss is 0.5174014234955804\n",
      "epoch 791 completed\n",
      "loss is 0.5173747314822843\n",
      "epoch 792 completed\n",
      "loss is 0.5173482850139537\n",
      "epoch 793 completed\n",
      "loss is 0.5173220839504575\n",
      "epoch 794 completed\n",
      "loss is 0.5172961281919787\n",
      "epoch 795 completed\n",
      "loss is 0.5172704176772907\n",
      "epoch 796 completed\n",
      "loss is 0.5172449523819995\n",
      "epoch 797 completed\n",
      "loss is 0.5172197323168355\n",
      "epoch 798 completed\n",
      "loss is 0.5171947575259566\n",
      "epoch 799 completed\n",
      "loss is 0.5171700280852316\n",
      "epoch 800 completed\n",
      "loss is 0.5171455441005884\n",
      "epoch 801 completed\n",
      "loss is 0.5171213057063695\n",
      "epoch 802 completed\n",
      "loss is 0.5170973130636832\n",
      "epoch 803 completed\n",
      "loss is 0.5170735663588459\n",
      "epoch 804 completed\n",
      "loss is 0.5170500658017533\n",
      "epoch 805 completed\n",
      "loss is 0.5170268116243926\n",
      "epoch 806 completed\n",
      "loss is 0.5170038040792746\n",
      "epoch 807 completed\n",
      "loss is 0.5169810434379914\n",
      "epoch 808 completed\n",
      "loss is 0.516958529989735\n",
      "epoch 809 completed\n",
      "loss is 0.5169362640398989\n",
      "epoch 810 completed\n",
      "loss is 0.5169142459086713\n",
      "epoch 811 completed\n",
      "loss is 0.5168924759296994\n",
      "epoch 812 completed\n",
      "loss is 0.5168709544487673\n",
      "epoch 813 completed\n",
      "loss is 0.5168496818225191\n",
      "epoch 814 completed\n",
      "loss is 0.5168286584172138\n",
      "epoch 815 completed\n",
      "loss is 0.5168078846075186\n",
      "epoch 816 completed\n",
      "loss is 0.5167873607753325\n",
      "epoch 817 completed\n",
      "loss is 0.5167670873086662\n",
      "epoch 818 completed\n",
      "loss is 0.5167470646005423\n",
      "epoch 819 completed\n",
      "loss is 0.5167272930479264\n",
      "epoch 820 completed\n",
      "loss is 0.5167077730507214\n",
      "epoch 821 completed\n",
      "loss is 0.5166885050107846\n",
      "epoch 822 completed\n",
      "loss is 0.5166694893309701\n",
      "epoch 823 completed\n",
      "loss is 0.5166507264142336\n",
      "epoch 824 completed\n",
      "loss is 0.5166322166627391\n",
      "epoch 825 completed\n",
      "loss is 0.5166139604770466\n",
      "epoch 826 completed\n",
      "loss is 0.5165959582553079\n",
      "epoch 827 completed\n",
      "loss is 0.5165782103924975\n",
      "epoch 828 completed\n",
      "loss is 0.5165607172796736\n",
      "epoch 829 completed\n",
      "loss is 0.5165434793033086\n",
      "epoch 830 completed\n",
      "loss is 0.5165264968446104\n",
      "epoch 831 completed\n",
      "loss is 0.5165097702789063\n",
      "epoch 832 completed\n",
      "loss is 0.5164932999750309\n",
      "epoch 833 completed\n",
      "loss is 0.5164770862947908\n",
      "epoch 834 completed\n",
      "loss is 0.5164611295924213\n",
      "epoch 835 completed\n",
      "loss is 0.5164454302140938\n",
      "epoch 836 completed\n",
      "loss is 0.5164299884974479\n",
      "epoch 837 completed\n",
      "loss is 0.5164148047711276\n",
      "epoch 838 completed\n",
      "loss is 0.5163998793544281\n",
      "epoch 839 completed\n",
      "loss is 0.5163852125568645\n",
      "epoch 840 completed\n",
      "loss is 0.5163708046778299\n",
      "epoch 841 completed\n",
      "loss is 0.5163566560062934\n",
      "epoch 842 completed\n",
      "loss is 0.5163427668204656\n",
      "epoch 843 completed\n",
      "loss is 0.5163291373875418\n",
      "epoch 844 completed\n",
      "loss is 0.5163157679634545\n",
      "epoch 845 completed\n",
      "loss is 0.5163026587926471\n",
      "epoch 846 completed\n",
      "loss is 0.5162898101078356\n",
      "epoch 847 completed\n",
      "loss is 0.5162772221298889\n",
      "epoch 848 completed\n",
      "loss is 0.5162648950676094\n",
      "epoch 849 completed\n",
      "loss is 0.5162528291176371\n",
      "epoch 850 completed\n",
      "loss is 0.5162410244642963\n",
      "epoch 851 completed\n",
      "loss is 0.5162294812795184\n",
      "epoch 852 completed\n",
      "loss is 0.5162181997227574\n",
      "epoch 853 completed\n",
      "loss is 0.5162071799409147\n",
      "epoch 854 completed\n",
      "loss is 0.516196422068305\n",
      "epoch 855 completed\n",
      "loss is 0.5161859262266081\n",
      "epoch 856 completed\n",
      "loss is 0.5161756925248873\n",
      "epoch 857 completed\n",
      "loss is 0.5161657210595545\n",
      "epoch 858 completed\n",
      "loss is 0.5161560119144021\n",
      "epoch 859 completed\n",
      "loss is 0.5161465651606437\n",
      "epoch 860 completed\n",
      "loss is 0.5161373808569223\n",
      "epoch 861 completed\n",
      "loss is 0.5161284590494037\n",
      "epoch 862 completed\n",
      "loss is 0.5161197997718074\n",
      "epoch 863 completed\n",
      "loss is 0.5161114030455157\n",
      "epoch 864 completed\n",
      "loss is 0.5161032688796363\n",
      "epoch 865 completed\n",
      "loss is 0.5160953972711124\n",
      "epoch 866 completed\n",
      "loss is 0.5160877882048317\n",
      "epoch 867 completed\n",
      "loss is 0.5160804416537366\n",
      "epoch 868 completed\n",
      "loss is 0.5160733575789601\n",
      "epoch 869 completed\n",
      "loss is 0.5160665359299464\n",
      "epoch 870 completed\n",
      "loss is 0.516059976644585\n",
      "epoch 871 completed\n",
      "loss is 0.5160536796493984\n",
      "epoch 872 completed\n",
      "loss is 0.5160476448596364\n",
      "epoch 873 completed\n",
      "loss is 0.5160418721794882\n",
      "epoch 874 completed\n",
      "loss is 0.5160363615022181\n",
      "epoch 875 completed\n",
      "loss is 0.5160311127103433\n",
      "epoch 876 completed\n",
      "loss is 0.5160261256758273\n",
      "epoch 877 completed\n",
      "loss is 0.5160214002602017\n",
      "epoch 878 completed\n",
      "loss is 0.5160169363148367\n",
      "epoch 879 completed\n",
      "loss is 0.5160127336810557\n",
      "epoch 880 completed\n",
      "loss is 0.516008792190351\n",
      "epoch 881 completed\n",
      "loss is 0.5160051116645805\n",
      "epoch 882 completed\n",
      "loss is 0.5160016919161753\n",
      "epoch 883 completed\n",
      "loss is 0.5159985327483035\n",
      "epoch 884 completed\n",
      "loss is 0.515995633955078\n",
      "epoch 885 completed\n",
      "loss is 0.5159929953217903\n",
      "epoch 886 completed\n",
      "loss is 0.515990616625063\n",
      "epoch 887 completed\n",
      "loss is 0.5159884976331077\n",
      "epoch 888 completed\n",
      "loss is 0.5159866381058711\n",
      "epoch 889 completed\n",
      "loss is 0.5159850377952829\n",
      "epoch 890 completed\n",
      "loss is 0.5159836964454351\n",
      "epoch 891 completed\n",
      "loss is 0.5159826137927919\n",
      "epoch 892 completed\n",
      "loss is 0.5159817895664081\n",
      "epoch 893 completed\n",
      "loss is 0.5159812234881083\n",
      "epoch 894 completed\n",
      "loss is 0.5159809152727035\n",
      "epoch 895 completed\n",
      "loss is 0.5159808646281885\n",
      "epoch 896 completed\n",
      "loss is 0.515981071255948\n",
      "epoch 897 completed\n",
      "loss is 0.5159815348509332\n",
      "epoch 898 completed\n",
      "loss is 0.5159822551018973\n",
      "epoch 899 completed\n",
      "loss is 0.5159832316915547\n",
      "epoch 900 completed\n",
      "loss is 0.5159844642968129\n",
      "epoch 901 completed\n",
      "loss is 0.5159859525889047\n",
      "epoch 902 completed\n",
      "loss is 0.5159876962336527\n",
      "epoch 903 completed\n",
      "loss is 0.5159896948915921\n",
      "epoch 904 completed\n",
      "loss is 0.515991948218199\n",
      "epoch 905 completed\n",
      "loss is 0.5159944558640609\n",
      "epoch 906 completed\n",
      "loss is 0.515997217475043\n",
      "epoch 907 completed\n",
      "loss is 0.5160002326924851\n",
      "epoch 908 completed\n",
      "loss is 0.5160035011533521\n",
      "epoch 909 completed\n",
      "loss is 0.5160070224904464\n",
      "epoch 910 completed\n",
      "loss is 0.5160107963325442\n",
      "epoch 911 completed\n",
      "loss is 0.5160148223045614\n",
      "epoch 912 completed\n",
      "loss is 0.51601910002774\n",
      "epoch 913 completed\n",
      "loss is 0.5160236291197946\n",
      "epoch 914 completed\n",
      "loss is 0.5160284091950583\n",
      "epoch 915 completed\n",
      "loss is 0.516033439864662\n",
      "epoch 916 completed\n",
      "loss is 0.5160387207366548\n",
      "epoch 917 completed\n",
      "loss is 0.5160442514161867\n",
      "epoch 918 completed\n",
      "loss is 0.5160500315056166\n",
      "epoch 919 completed\n",
      "loss is 0.5160560606046815\n",
      "epoch 920 completed\n",
      "loss is 0.5160623383106162\n",
      "epoch 921 completed\n",
      "loss is 0.5160688642182928\n",
      "epoch 922 completed\n",
      "loss is 0.5160756379203697\n",
      "epoch 923 completed\n",
      "loss is 0.5160826590073722\n",
      "epoch 924 completed\n",
      "loss is 0.5160899270678678\n",
      "epoch 925 completed\n",
      "loss is 0.5160974416885529\n",
      "epoch 926 completed\n",
      "loss is 0.5161052024543978\n",
      "epoch 927 completed\n",
      "loss is 0.5161132089487255\n",
      "epoch 928 completed\n",
      "loss is 0.516121460753349\n",
      "epoch 929 completed\n",
      "loss is 0.5161299574486802\n",
      "epoch 930 completed\n",
      "loss is 0.5161386986138115\n",
      "epoch 931 completed\n",
      "loss is 0.5161476838266339\n",
      "epoch 932 completed\n",
      "loss is 0.5161569126639308\n",
      "epoch 933 completed\n",
      "loss is 0.516166384701479\n",
      "epoch 934 completed\n",
      "loss is 0.5161760995141176\n",
      "epoch 935 completed\n",
      "loss is 0.5161860566758529\n",
      "epoch 936 completed\n",
      "loss is 0.5161962557599694\n",
      "epoch 937 completed\n",
      "loss is 0.5162066963390433\n",
      "epoch 938 completed\n",
      "loss is 0.5162173779850915\n",
      "epoch 939 completed\n",
      "loss is 0.516228300269602\n",
      "epoch 940 completed\n",
      "loss is 0.5162394627636263\n",
      "epoch 941 completed\n",
      "loss is 0.5162508650378521\n",
      "epoch 942 completed\n",
      "loss is 0.5162625066626675\n",
      "epoch 943 completed\n",
      "loss is 0.5162743872081926\n",
      "epoch 944 completed\n",
      "loss is 0.5162865062444293\n",
      "epoch 945 completed\n",
      "loss is 0.5162988633412126\n",
      "epoch 946 completed\n",
      "loss is 0.516311458068335\n",
      "epoch 947 completed\n",
      "loss is 0.5163242899955987\n",
      "epoch 948 completed\n",
      "loss is 0.5163373586928351\n",
      "epoch 949 completed\n",
      "loss is 0.5163506637299983\n",
      "epoch 950 completed\n",
      "loss is 0.5163642046771507\n",
      "epoch 951 completed\n",
      "loss is 0.5163779811045879\n",
      "epoch 952 completed\n",
      "loss is 0.5163919925828149\n",
      "epoch 953 completed\n",
      "loss is 0.5164062386826173\n",
      "epoch 954 completed\n",
      "loss is 0.5164207189751027\n",
      "epoch 955 completed\n",
      "loss is 0.5164354330317147\n",
      "epoch 956 completed\n",
      "loss is 0.5164503804243082\n",
      "epoch 957 completed\n",
      "loss is 0.5164655607251425\n",
      "epoch 958 completed\n",
      "loss is 0.5164809735069382\n",
      "epoch 959 completed\n",
      "loss is 0.5164966183429076\n",
      "epoch 960 completed\n",
      "loss is 0.5165124948067684\n",
      "epoch 961 completed\n",
      "loss is 0.5165286024727759\n",
      "epoch 962 completed\n",
      "loss is 0.5165449409157657\n",
      "epoch 963 completed\n",
      "loss is 0.5165615097111491\n",
      "epoch 964 completed\n",
      "loss is 0.5165783084349684\n",
      "epoch 965 completed\n",
      "loss is 0.5165953366638989\n",
      "epoch 966 completed\n",
      "loss is 0.516612593975268\n",
      "epoch 967 completed\n",
      "loss is 0.5166300799470893\n",
      "epoch 968 completed\n",
      "loss is 0.5166477941580815\n",
      "epoch 969 completed\n",
      "loss is 0.5166657361876532\n",
      "epoch 970 completed\n",
      "loss is 0.5166839056159735\n",
      "epoch 971 completed\n",
      "loss is 0.5167023020239583\n",
      "epoch 972 completed\n",
      "loss is 0.5167209249932623\n",
      "epoch 973 completed\n",
      "loss is 0.5167397741063473\n",
      "epoch 974 completed\n",
      "loss is 0.5167588489464636\n",
      "epoch 975 completed\n",
      "loss is 0.5167781490976613\n",
      "epoch 976 completed\n",
      "loss is 0.5167976741448129\n",
      "epoch 977 completed\n",
      "loss is 0.5168174236736277\n",
      "epoch 978 completed\n",
      "loss is 0.516837397270658\n",
      "epoch 979 completed\n",
      "loss is 0.5168575945233009\n",
      "epoch 980 completed\n",
      "loss is 0.5168780150198443\n",
      "epoch 981 completed\n",
      "loss is 0.5168986583494293\n",
      "epoch 982 completed\n",
      "loss is 0.5169195241021053\n",
      "epoch 983 completed\n",
      "loss is 0.5169406118688058\n",
      "epoch 984 completed\n",
      "loss is 0.5169619212413752\n",
      "epoch 985 completed\n",
      "loss is 0.5169834518125908\n",
      "epoch 986 completed\n",
      "loss is 0.5170052031761425\n",
      "epoch 987 completed\n",
      "loss is 0.5170271749266477\n",
      "epoch 988 completed\n",
      "loss is 0.5170493666596911\n",
      "epoch 989 completed\n",
      "loss is 0.5170717779718147\n",
      "epoch 990 completed\n",
      "loss is 0.51709440846051\n",
      "epoch 991 completed\n",
      "loss is 0.5171172577242472\n",
      "epoch 992 completed\n",
      "loss is 0.517140325362475\n",
      "epoch 993 completed\n",
      "loss is 0.5171636109756401\n",
      "epoch 994 completed\n",
      "loss is 0.5171871141651954\n",
      "epoch 995 completed\n",
      "loss is 0.5172108345335903\n",
      "epoch 996 completed\n",
      "loss is 0.5172347716843013\n",
      "epoch 997 completed\n",
      "loss is 0.5172589252218264\n",
      "epoch 998 completed\n",
      "loss is 0.5172832947517126\n",
      "epoch 999 completed\n",
      "loss is 0.5173078798805416\n",
      "\n",
      "\n",
      "printing network: [<__main__.InputLayer object at 0x000002A8179C5010>\n",
      " <__main__.Layer object at 0x000002A80FEB7610>\n",
      " <__main__.OutputLayer object at 0x000002A80FBEC2D0>]\n",
      "epoch 0 completed\n",
      "loss is 1.5719133610244556\n",
      "epoch 1 completed\n",
      "loss is 1.5325457517953032\n",
      "epoch 2 completed\n",
      "loss is 1.4706460173616123\n",
      "epoch 3 completed\n",
      "loss is 1.3875276733924056\n",
      "epoch 4 completed\n",
      "loss is 1.2978566901511908\n",
      "epoch 5 completed\n",
      "loss is 1.21809840006182\n",
      "epoch 6 completed\n",
      "loss is 1.1536948156487052\n",
      "epoch 7 completed\n",
      "loss is 1.1027200669153778\n",
      "epoch 8 completed\n",
      "loss is 1.061894709525008\n",
      "epoch 9 completed\n",
      "loss is 1.028528747179947\n",
      "epoch 10 completed\n",
      "loss is 1.0006996797358676\n",
      "epoch 11 completed\n",
      "loss is 0.9770667464751578\n",
      "epoch 12 completed\n",
      "loss is 0.9566865881366977\n",
      "epoch 13 completed\n",
      "loss is 0.9388832403719685\n",
      "epoch 14 completed\n",
      "loss is 0.923162790644281\n",
      "epoch 15 completed\n",
      "loss is 0.9091573014461134\n",
      "epoch 16 completed\n",
      "loss is 0.896586919449144\n",
      "epoch 17 completed\n",
      "loss is 0.8852336543304529\n",
      "epoch 18 completed\n",
      "loss is 0.8749232900564063\n",
      "epoch 19 completed\n",
      "loss is 0.8655133960480054\n",
      "epoch 20 completed\n",
      "loss is 0.8568859037442007\n",
      "epoch 21 completed\n",
      "loss is 0.8489426407586212\n",
      "epoch 22 completed\n",
      "loss is 0.8416021712276521\n",
      "epoch 23 completed\n",
      "loss is 0.8347968032033118\n",
      "epoch 24 completed\n",
      "loss is 0.8284695128045393\n",
      "epoch 25 completed\n",
      "loss is 0.8225711294869217\n",
      "epoch 26 completed\n",
      "loss is 0.8170581530006298\n",
      "epoch 27 completed\n",
      "loss is 0.8118913229150028\n",
      "epoch 28 completed\n",
      "loss is 0.8070348590227978\n",
      "epoch 29 completed\n",
      "loss is 0.8024562017905912\n",
      "epoch 30 completed\n",
      "loss is 0.7981260384023153\n",
      "epoch 31 completed\n",
      "loss is 0.7940183608849684\n",
      "epoch 32 completed\n",
      "loss is 0.7901102978586244\n",
      "epoch 33 completed\n",
      "loss is 0.7863815865724362\n",
      "epoch 34 completed\n",
      "loss is 0.7828139420008432\n",
      "epoch 35 completed\n",
      "loss is 0.7793911539014335\n",
      "epoch 36 completed\n",
      "loss is 0.7761006773525112\n",
      "epoch 37 completed\n",
      "loss is 0.7729357831602157\n",
      "epoch 38 completed\n",
      "loss is 0.7698956035360566\n",
      "epoch 39 completed\n",
      "loss is 0.7669820923581064\n",
      "epoch 40 completed\n",
      "loss is 0.7641963194039074\n",
      "epoch 41 completed\n",
      "loss is 0.7615365986409407\n",
      "epoch 42 completed\n",
      "loss is 0.7589985074998586\n",
      "epoch 43 completed\n",
      "loss is 0.7565755958650995\n",
      "epoch 44 completed\n",
      "loss is 0.7542599179875704\n",
      "epoch 45 completed\n",
      "loss is 0.7520421937760361\n",
      "epoch 46 completed\n",
      "loss is 0.7499119760380503\n",
      "epoch 47 completed\n",
      "loss is 0.7478588214529316\n",
      "epoch 48 completed\n",
      "loss is 0.7458754155856422\n",
      "epoch 49 completed\n",
      "loss is 0.7439605392064763\n",
      "epoch 50 completed\n",
      "loss is 0.7421171677136683\n",
      "epoch 51 completed\n",
      "loss is 0.7403471595715168\n",
      "epoch 52 completed\n",
      "loss is 0.7386486618581741\n",
      "epoch 53 completed\n",
      "loss is 0.7370170860002663\n",
      "epoch 54 completed\n",
      "loss is 0.7354468757654444\n",
      "epoch 55 completed\n",
      "loss is 0.7339325937269675\n",
      "epoch 56 completed\n",
      "loss is 0.7324693292779272\n",
      "epoch 57 completed\n",
      "loss is 0.7310527648090966\n",
      "epoch 58 completed\n",
      "loss is 0.7296791269981605\n",
      "epoch 59 completed\n",
      "loss is 0.7283451221049654\n",
      "epoch 60 completed\n",
      "loss is 0.7270478844572602\n",
      "epoch 61 completed\n",
      "loss is 0.7257849387744664\n",
      "epoch 62 completed\n",
      "loss is 0.7245541689612641\n",
      "epoch 63 completed\n",
      "loss is 0.7233537866524865\n",
      "epoch 64 completed\n",
      "loss is 0.7221822961221771\n",
      "epoch 65 completed\n",
      "loss is 0.7210384553069382\n",
      "epoch 66 completed\n",
      "loss is 0.7199212345440448\n",
      "epoch 67 completed\n",
      "loss is 0.7188297751770646\n",
      "epoch 68 completed\n",
      "loss is 0.7177633499424293\n",
      "epoch 69 completed\n",
      "loss is 0.716721326565258\n",
      "epoch 70 completed\n",
      "loss is 0.7157031355691371\n",
      "epoch 71 completed\n",
      "loss is 0.7147082429654773\n",
      "epoch 72 completed\n",
      "loss is 0.7137361280956358\n",
      "epoch 73 completed\n",
      "loss is 0.7127862663188914\n",
      "epoch 74 completed\n",
      "loss is 0.7118581154624108\n",
      "epoch 75 completed\n",
      "loss is 0.710951104116552\n",
      "epoch 76 completed\n",
      "loss is 0.7100646192037573\n",
      "epoch 77 completed\n",
      "loss is 0.7091979900118092\n",
      "epoch 78 completed\n",
      "loss is 0.7083504662762317\n",
      "epoch 79 completed\n",
      "loss is 0.7075211892696359\n",
      "epoch 80 completed\n",
      "loss is 0.7067091583018486\n",
      "epoch 81 completed\n",
      "loss is 0.7059132041860325\n",
      "epoch 82 completed\n",
      "loss is 0.705132007122022\n",
      "epoch 83 completed\n",
      "loss is 0.7043642691581334\n",
      "epoch 84 completed\n",
      "loss is 0.7036093333629456\n",
      "epoch 85 completed\n",
      "loss is 0.702868824634054\n",
      "epoch 86 completed\n",
      "loss is 0.7021492524471791\n",
      "epoch 87 completed\n",
      "loss is 0.7014597386762345\n",
      "epoch 88 completed\n",
      "loss is 0.7007943251042295\n",
      "epoch 89 completed\n",
      "loss is 0.7001289479400299\n",
      "epoch 90 completed\n",
      "loss is 0.699459922672031\n",
      "epoch 91 completed\n",
      "loss is 0.698801334386903\n",
      "epoch 92 completed\n",
      "loss is 0.6981579633115826\n",
      "epoch 93 completed\n",
      "loss is 0.6975268868086162\n",
      "epoch 94 completed\n",
      "loss is 0.6969047429877043\n",
      "epoch 95 completed\n",
      "loss is 0.6962895358463045\n",
      "epoch 96 completed\n",
      "loss is 0.6956803486499624\n",
      "epoch 97 completed\n",
      "loss is 0.6950769509735736\n",
      "epoch 98 completed\n",
      "loss is 0.6944795903091852\n",
      "epoch 99 completed\n",
      "loss is 0.6938888877939693\n",
      "epoch 100 completed\n",
      "loss is 0.6933057591255761\n",
      "epoch 101 completed\n",
      "loss is 0.6927313214377937\n",
      "epoch 102 completed\n",
      "loss is 0.6921667746091473\n",
      "epoch 103 completed\n",
      "loss is 0.6916132634763921\n",
      "epoch 104 completed\n",
      "loss is 0.6910717391371233\n",
      "epoch 105 completed\n",
      "loss is 0.6905428428621984\n",
      "epoch 106 completed\n",
      "loss is 0.690026834135678\n",
      "epoch 107 completed\n",
      "loss is 0.6895235754811794\n",
      "epoch 108 completed\n",
      "loss is 0.689032573757757\n",
      "epoch 109 completed\n",
      "loss is 0.6885530648005698\n",
      "epoch 110 completed\n",
      "loss is 0.6880841198335056\n",
      "epoch 111 completed\n",
      "loss is 0.6876247503950824\n",
      "epoch 112 completed\n",
      "loss is 0.6871739933116336\n",
      "epoch 113 completed\n",
      "loss is 0.6867309659203555\n",
      "epoch 114 completed\n",
      "loss is 0.6862948907764992\n",
      "epoch 115 completed\n",
      "loss is 0.6858650956761881\n",
      "epoch 116 completed\n",
      "loss is 0.6854409978437804\n",
      "epoch 117 completed\n",
      "loss is 0.6850220810060025\n",
      "epoch 118 completed\n",
      "loss is 0.6846078720746258\n",
      "epoch 119 completed\n",
      "loss is 0.684197921587461\n",
      "epoch 120 completed\n",
      "loss is 0.6837917897868659\n",
      "epoch 121 completed\n",
      "loss is 0.683389038612625\n",
      "epoch 122 completed\n",
      "loss is 0.6829892289569226\n",
      "epoch 123 completed\n",
      "loss is 0.682591922112697\n",
      "epoch 124 completed\n",
      "loss is 0.6821966842492572\n",
      "epoch 125 completed\n",
      "loss is 0.6818030928196316\n",
      "epoch 126 completed\n",
      "loss is 0.68141074394651\n",
      "epoch 127 completed\n",
      "loss is 0.6810192599992182\n",
      "epoch 128 completed\n",
      "loss is 0.6806282967440774\n",
      "epoch 129 completed\n",
      "loss is 0.6802375496180537\n",
      "epoch 130 completed\n",
      "loss is 0.6798467588361313\n",
      "epoch 131 completed\n",
      "loss is 0.6794557131887095\n",
      "epoch 132 completed\n",
      "loss is 0.6790642525049394\n",
      "epoch 133 completed\n",
      "loss is 0.6786722688401499\n",
      "epoch 134 completed\n",
      "loss is 0.6782797064842293\n",
      "epoch 135 completed\n",
      "loss is 0.6778865608842826\n",
      "epoch 136 completed\n",
      "loss is 0.6774928765412648\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\surbh\\Desktop\\aaveg\\sem5\\COL774\\COL774_A3\\neural_networks\\nn2.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m nn \u001b[39m=\u001b[39m Neural_Network(\u001b[39m32\u001b[39m, \u001b[39m1024\u001b[39m , hidden_layer, \u001b[39m5\u001b[39m) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m nn\u001b[39m.\u001b[39mmake_network() \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m nn\u001b[39m.\u001b[39mtrain(X_train, Y_train, \u001b[39m0.01\u001b[39m) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_pred \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mcompute_predictions(X_train) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m test_pred \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mcompute_predictions(X_test) \n",
      "\u001b[1;32mc:\\Users\\surbh\\Desktop\\aaveg\\sem5\\COL774\\COL774_A3\\neural_networks\\nn2.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, labels \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batches_list, labels_list):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_prop(batch) \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward_prop(labels) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_update(learning_rate) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m completed\u001b[39m\u001b[39m\"\u001b[39m) \n",
      "\u001b[1;32mc:\\Users\\surbh\\Desktop\\aaveg\\sem5\\COL774\\COL774_A3\\neural_networks\\nn2.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mbackward(y) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork)\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork[i]\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;32mc:\\Users\\surbh\\Desktop\\aaveg\\sem5\\COL774\\COL774_A3\\neural_networks\\nn2.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_grad_a()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_grad_z()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_grad_W()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_grad_b()\n",
      "\u001b[1;32mc:\\Users\\surbh\\Desktop\\aaveg\\sem5\\COL774\\COL774_A3\\neural_networks\\nn2.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         \u001b[39m# self.grad_a_sum = np.sum(self.grad_a, axis=1) \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39melse\u001b[39;00m: \u001b[39m# output layer\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         \u001b[39mpass\u001b[39;00m \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_grad_z\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_derivative(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_a  \u001b[39m# element-wise multiplication \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/surbh/Desktop/aaveg/sem5/COL774/COL774_A3/neural_networks/nn2.ipynb#X11sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39m# self.grad_z_sum = np.sum(self.grad_z, axis=1)  \u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# experimenting for part b \n",
    "hidden_layers = np.array([[1], [5], [10], [50], [100]])\n",
    "nn_list = [] \n",
    "for hidden_layer in hidden_layers:\n",
    "    nn = Neural_Network(32, 1024 , hidden_layer, 5) \n",
    "    nn.make_network() \n",
    "    nn.train(X_train, Y_train, 0.01) \n",
    "    train_pred = nn.compute_predictions(X_train) \n",
    "    test_pred = nn.compute_predictions(X_test) \n",
    "    report_test = classification_report( Y_test, test_pred)  \n",
    "    report_train = classification_report(Y_train, train_pred) \n",
    "    to_print = f\"hidden_layer is {hidden_layer}\\n\"\n",
    "    to_print += \"printing train report\\n\"\n",
    "    to_print += \"\\n\" + report_train + \"\\n\"\n",
    "    to_print += \"printing test report\\n\"\n",
    "    to_print += \"\\n\" + report_test + \"\\n\"\n",
    "    print() \n",
    "    print()  \n",
    "    with open(\"logfile\", \"a\") as file:\n",
    "        file.write(to_print)\n",
    "    nn_list.append(nn) \n",
    "    # print(f\"hidden layer is {hidden_layer}\")\n",
    "    # print(report)\n",
    "    # print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot by reading from logfile\n",
    "def read_logfile():\n",
    "    hidden_layer_sizes = [] \n",
    "    train_recall, test_recall = [[] for i in range(5)], [[] for i in range(5)] \n",
    "    train_precision, test_precision = [[] for i in range(5)], [[] for i in range(5)] \n",
    "    train_f1, test_f1 = [[] for i in range(5)], [[] for i in range(5)] \n",
    "    train_acc, test_acc= [], [] \n",
    "    in_train, in_test = False, False \n",
    "    with open(\"logfile\", \"r\") as file:\n",
    "        report = \"\" \n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line = line.lstrip() \n",
    "            if (line.startswith(\"hidden_layer\")):\n",
    "                size = re.findall(r'\\d+', line) \n",
    "                hidden_layer_sizes.append(int(size[0])) \n",
    "            elif (line.startswith(\"printing train report\")):\n",
    "                in_train = True\n",
    "                in_test = False \n",
    "            elif (line.startswith(\"printing test report\")):\n",
    "                in_test = True \n",
    "                in_train = False\n",
    "\n",
    "            elif (line.startswith(\"accuracy\")):\n",
    "                if (in_train):\n",
    "                    acc = re.findall(r'\\d+\\.\\d+', line)[0]\n",
    "                    train_acc.append(float(acc))  \n",
    "                elif (in_test):\n",
    "                    acc = re.findall(r'\\d+\\.\\d+', line)[0] \n",
    "                    test_acc.append(float(acc)) \n",
    "\n",
    "            elif (line.startswith(\"macro avg\") or line.startswith(\"weighted avg\")):\n",
    "                continue \n",
    "            \n",
    "            else:\n",
    "                vals = re.findall(r'\\d+\\.\\d+', line)\n",
    "                if (len(vals) == 0): continue\n",
    "                label = int(float(vals[0]))\n",
    "                if (in_train):\n",
    "                    train_recall[label].append(float(vals[2])) \n",
    "                    train_precision[label].append(float(vals[1])) \n",
    "                    train_f1[label].append(float(vals[3])) \n",
    "                elif (in_test):\n",
    "                    test_recall[label].append(float(vals[2])) \n",
    "                    test_precision[label].append(float(vals[1])) \n",
    "                    test_f1[label].append(float(vals[3])) \n",
    "    return (hidden_layer_sizes, train_recall, train_precision, train_f1, test_recall, test_precision, test_f1, train_acc, test_acc)\n",
    "    \n",
    "\n",
    "def plot_accuracies(y_lines, x, labels, x_label, y_label, name):\n",
    "\n",
    "    fig,axis = plt.subplots()\n",
    "    for y_line,label  in zip(y_lines, labels):\n",
    "        axis.plot(x, y_line, label = label) \n",
    "    axis.set_xlabel(x_label)\n",
    "    axis.set_ylabel(y_label)\n",
    "    axis.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{name}.pdf\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGwCAYAAABB4NqyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABs2ElEQVR4nO3deVjU5frH8fcw7CAgioCK4r5rLifTNC0VS7PMMrO0LD0ds1KzRc0s85TartXJU6Z5Sk1/ZtmGKWXuZmValvuKC4SgAoLAMPP9/TEyhaACDgwDn9d1zQXznWeeuecBmdtnNRmGYSAiIiJSiXi4OgARERGRsqYESERERCodJUAiIiJS6SgBEhERkUpHCZCIiIhUOkqAREREpNJRAiQiIiKVjqerAyiPbDYbJ06coEqVKphMJleHIyIiIkVgGAbp6enUrFkTD49L9/EoASrEiRMniIqKcnUYIiIiUgJHjx6ldu3alyyjBKgQVapUAewNGBQUVOJ6LBYLq1atIiYmBi8vL2eFJ4VQW5cdtXXZUnuXHbV12Smttk5LSyMqKsrxOX4pSoAKkTfsFRQUdMUJkL+/P0FBQfrHVMrU1mVHbV221N5lR21ddkq7rYsyfUWToEVERKTSUQIkIiIilY4SIBEREal0NAfoClitViwWy0Uft1gseHp6kpWVhdVqLcPIKh93bmsvLy/MZrOrwxARqVSUAJWAYRgkJiZy5syZy5aLiIjg6NGj2k+olLl7W4eEhBAREeGWsYuIuCMlQCWQl/zUqFEDf3//i35o2Ww2zp49S2Bg4GU3ZJIr465tbRgGmZmZJCUlARAZGeniiEREKgclQMVktVodyU+1atUuWdZms5GTk4Ovr69bfSi7I3duaz8/PwCSkpKoUaOGhsNERMqAe31SlAN5c378/f1dHIlUJHm/T5eaUyYiIs6jBKiENFdDnEm/TyIiZUsJkIiIiFQ6Lk+A3nnnHerVq4evry/t27dn/fr1lyy/cOFC2rRpg7+/P5GRkdx///2kpKTkK7Ns2TKaN2+Oj48PzZs357PPPivNtyAiIiJuxqUJ0JIlSxg7diyTJk1i27ZtdO3alZtuuon4+PhCy2/YsIF7772X4cOH88cff7B06VJ++uknRowY4SizefNmBg0axNChQ/n1118ZOnQod955J1u2bCmrt1VpREdHM3PmTFeHISIiUmwuXQX2+uuvM3z4cEcCM3PmTFauXMns2bOZPn16gfI//PAD0dHRjB49GoB69erxr3/9i5dfftlRZubMmfTq1YuJEycCMHHiRNauXcvMmTP5+OOPy+BdlV/du3fnqquuclrS8tNPPxEQEOCUukRKW5bFSvLZbFeHUWy5ubmcyobjZ87h6alJ8qVJbV12cnNzSctxbQwuS4BycnLYunUrEyZMyHc9JiaGTZs2Ffqczp07M2nSJGJjY7nppptISkrik08+oW/fvo4ymzdv5rHHHsv3vN69e1/yQz87O5vs7L/+MKalpQH2FTkXrsqxWCwYhoHNZsNms13yPRqG4fh6ubJl5XKxGIaB1WrF0/Pyvxp52wCUh/fmrLYuzvt3JpvNhmEYWCyWcr8MPu/fhLusWMvOtbHox6P8Z80BUs/lujqcEvLk+V8uPT1AnEVtXVaiA83c7uS/I8X5u+SyBCg5ORmr1Up4eHi+6+Hh4SQmJhb6nM6dO7Nw4UIGDRpEVlYWubm53HLLLbz11luOMomJicWqE2D69Ok8//zzBa6vWrWqwHJ3T09PIiIiOHv2LDk59vTVMAyyLBf/0D2Xcuaij10pXy+PIq0gGjVqFGvXrmXt2rW8+eabAPz666/Ex8fTr18/PvnkE1544QX++OMPli1bRu3atZk0aRI///wzmZmZNG7cmGeffZbu3bs76mzdujUPPfQQDz30EABVq1Zl1qxZrFq1itWrVxMZGcm///1v+vTpc9G4lixZwn//+1/279+Pv78/Xbt2Zfr06YSFhTnK7Nq1i+eee44ffvgBwzBo2bKlY+4YwIIFC/jPf/7DwYMHqVq1Kv369eOVV14hPj6eNm3asG7dOlq1agVAamoq0dHRfPnll3Tp0oUNGzaU+P1nZ2fz4osvsmzZMpKTk6lduzZjx45lyJAhtG/fnvvvv59HH33UUX7nzp106dKFrVu3OmLPk5OTw7lz51i3bh25ue7xIR0XF+fqEC7JMGD7KRNfHvEgJdv+b8RsMlw/8VFEAPD0cP7fkczMzKK/vlNfuQQu/PA2DOOiH+g7d+5k9OjRPPvss/Tu3ZuEhASefPJJRo4cydy5c0tUJ9iHycaNG+e4n5aWRlRUFDExMQQFBeUrm5WVxdGjRwkMDMTX1xeAzJxc2r7kmg+D36f0wt/78j/G//znPxw+fJgWLVo4kr2wsDCSk5MBmDp1Ki+//DL169cnJCSEY8eO0a9fP6ZPn46vry8ffvghgwcPZteuXdSpUwcADw8PfH1987XRK6+8wowZM3j99dd5++23+de//sWhQ4cIDQ0tNC6z2cwLL7xAkyZNSEpK4vHHH2f06NF8/fXXABw/fpybb76Zbt268e233xIUFMTGjRsdrzt79myefPJJpk2bRteuXcnNzWXz5s0EBQURGBgIQEBAgCPGvN4hf39/goKCHAluSd7/XXfdxQ8//MCbb75JmzZtOHToEMnJyQQHBzN8+HAWLVrEpEmTHO916dKldO3alTZt2hRoh6ysLPz8/Ljuuuscv1fllcViIS4ujl69euHl5eXqcAq19chpZqzcy/ajqQCEBXrzWM+GDGhbC7OHe2054A7tXVGorctOabV13ghOUbgsAapevTpms7lAz0xSUlKBHpw806dP59prr+XJJ58E7D0QAQEBdO3alRdeeIHIyEgiIiKKVSeAj48PPj4+Ba57eXkV+MFYrVZMJhMeHh6OHYddufPw3+O4lKpVq+Lt7U1AQAA1a9bM93ywJwC9e/d2XA8LC6Nt27aO+y+++CLLly/nq6++4pFHHnFcz2uLPMOGDeOee+4B7D+vt99+m59//pkbb7yx0Lj+PoG9YcOGvPnmm1x99dVkZmYSGBjI7NmzCQ4OZsmSJY6fRdOmTR3PmTZtGo8//jhjxowhLS2NoKAgrrnmmnzvrbCfVd61kr7/vXv3snTpUuLi4ujZs6cj/jwPPPAAzz33HD///DNXX301FouFhQsX8sorrxT68/LwsPfkFfY7V16Vx1gPJWfw0ordfPOH/W+Av7eZB6+rzz+71ifAx+X/37si5bG9Kyq1ddlxdlsXpy6X/UXw9vamffv2xMXFcdtttzmux8XFceuttxb6nMzMzAJzM/LmS+TNAenUqRNxcXH55gGtWrWKzp07O/stOPh5mdk5tXeB6zabjfS0dKoEVSm1JMnPyznzRTp06JDvfkZGBs8//zxfffUVJ06cIDc3l3Pnzl10hV6e1q1bO74PCAigSpUqjnOuCrNt2zamTJnC9u3bOXXqlKOHJj4+nubNm7N9+3a6du1a6C91UlISJ06coEePHsV5q4Uq7vvfvn07ZrOZbt26FVpfZGQkffv2Zd68eVx99dV89dVXZGVlMXDgwCuOVQo6lZHDm9/tY8EPR8i1GXiYYNA/onisZ2NqBJXvHjURcQ2X/pdo3LhxDB06lA4dOtCpUyfee+894uPjGTlyJGAfmjp+/DgffvghAP369eOf//wns2fPdgyBjR07lquvvtrRqzFmzBiuu+46XnrpJW699VY+//xzvv32WzZs2FBq78NkMhU6DGWz2cj1NuPv7Vnuz6e6cDXXk08+ycqVK3n11Vdp2LAhfn5+3HHHHY55TxdzYaJiMpkuOik5IyODmJgYYmJiWLBgAWFhYcTHx9O7d2/H6+Sdk1WYSz0Gf/X25CXHcPEJcsV9/5d7bbD3bg0dOpQ33niDDz74gEGDBukIFSfLsliZv+kw/1m9n/Rs+9yp65uEMbFPMxqHV3FxdCJSnrk0ARo0aBApKSlMnTqVhIQEWrZsSWxsLHXr1gUgISEhX4/DsGHDSE9P5+233+bxxx8nJCSEG264gZdeeslRpnPnzixevJhnnnmGyZMn06BBA5YsWULHjh3L/P2VN97e3lit1iKVXb9+PcOGDXP0zp09e5bDhw87NZ7du3eTnJzMjBkziIqKAuDnn3/OV6Z169b873//w2KxFEiuqlSpQnR0NN99912hPTF5E6kTEhIcw1nbt28vUmyXe/+tWrXCZrOxdu1axxDYhfr06UNAQACzZ89mxYoVrFu3rkivLZdnsxl88esJXlm5h+NnzgHQPDKISX2bcW3D6i6OTkTcgcsHxUeNGsWoUaMKfWz+/PkFrj366KP5VtYU5o477uCOO+5wRngVSnR0NFu2bOHw4cMEBgZedGIy2OezfPrpp/Tr1w+TycTkyZOdvty9Tp06eHt789ZbbzFy5Eh+//13/v3vf+cr88gjj/DWW29x1113MXHiRIKDg/nhhx+4+uqradKkCVOmTGHkyJGEhYXRpUsXDMNg8+bNPProo/j5+XHNNdcwY8YMoqOjSU5O5plnnilSbJd7/9HR0dx333088MADjknQR44cISkpiTvvvBOwD88OGzaMiRMn0rBhQzp16uS8xqvENh9IYVrsLnYct09wjgz25YmYJtzWthYebjbBWURcp3yPy4hTPfHEE5jNZpo3b+4YbrqYN954g6pVq9K5c2f69etH7969adeunVPjCQsLY/78+SxdupTmzZszY8YMXn311XxlqlWrxurVqzl79izdunWjffv2zJkzx9EbdN999zFz5kxmz55Np06duOWWW9i3b5/j+fPmzcNisdChQwfGjBnDCy+8UKTYivL+Z8+ezR133MGoUaNo2rQp//znP8nIyMhXZvjw4eTk5PDAAw+UpInkb/YnpTPifz8xeM4P7DieSqCPJ0/2bsL3T3Tn9va1lfyISLGYjL9PkBDAvowuODiY1NTUQpfBHzp0yHF+2aXYbDbHyqTyPgfI3ZXXtt64cSPdu3fn2LFjl1yJWJzfK1ezWCzExsbSp0+fMlkpczI9m5nf7mXxT0ex2gzMHibuvroOY3o2onpgwdWbFU1Zt3dlprYuO6XV1pf6/L6Qy4fARCqi7Oxsjh49yuTJk7nzzjsvmfxI4c7lWJm74SCz1xwgI8c+d61X83DG39iUhjUCXRydiLg7JUAipeDjjz9m+PDhXHXVVXz00UeuDsetWG0Gn/5yjNdW7SUxLQuANrWDebpPMzrWr+bi6ESkolACJFIKhg0bxrBhw1wdhttZv+8k02J3syvBvptrrRA/nrqxCf1a19QcHxFxKiVAIuJyuxPTmB67m7V7TwJQxdeTR29oyL2dovF10mafIiJ/pwRIRFzmz7QsXl+1l6Vbj2IzwMtsYug10Tx6Q0OqBni7OjwRqcCUAIlImcvIzuW9dQd5b91BzlnsE5z7tIrgqd5Nia4ecJlni4hcOSVAIlJmcq02lm49xutxezmZng1AuzohTOrbjPZ1L74xp4iIsykBEpFSZxgGa/acZFrsLvYlnQWgbjV/xt/YlJtaRmAyaYKziJQtJUAiUqp+P57K9BW72Lg/BYAQfy9G39CIIdfUxduz/GxaKSKVixKgSqR79+5cddVVzJw502l1Dhs2jDNnzrB8+XKn1SkVw4kz53h11R4+23YcwwBvswf3XxvNqOsbEuynXXZFxLWUAEmlVdgJ83Ll0rMszF5zgLkbDpGdaz9A9pY2NXmydxOiQv1dHJ2IiJ36nyuJYcOGsXbtWmbNmoXJZMJkMnH48GEAdu7cSZ8+fQgMDCQ8PJyhQ4eSnJzseO4nn3xCq1at8PPzo1q1avTs2ZOMjAymTJnC//73Pz7//HNHnWvWrCn09b/55hu6dOlCSEgI1apV4+abb+bAgQP5yhw7doy77rqL0NBQAgIC6NChA1u2bHE8/sUXX9ChQwd8fX2pXr06AwYMcDxmNpv5+uuv89UXEhLC/PnzATh8+DAmk4n/+7//o3v37vj6+rJgwQJSUlIYPHgwtWvXxt/fn1atWvHxxx/nq8dms/HSSy/RsGFDfHx8qFOnDi+++CIAN9xwA4888ki+8ikpKfj4+LB69erL/2AqEIvVxkebD9P9lTW8s+YA2bk2rq4XyucPX8ubg9sq+RGRckU9QM5gGGDJLHjdZrNfzzFDaR3Q6eUPRZhAOmvWLPbu3UvLli2ZOnUqYD+NPSEhgW7duvHPf/6T119/nXPnzjF+/HjuvPNOVq9eTUJCAoMHD+bll1/mtttuIz09nfXr12MYBk888QS7du0iLS2NDz74AIDQ0MJX8mRkZDBu3DhatWpFRkYGzz77LLfddhvbt2/Hw8PDcdp7rVq1+OKLL4iIiOCXX37BZrP3IHz99dcMGDCASZMm8dFHH5GTk1Mg4SmK8ePH89prr/HBBx/g4+NDVlYW7du3Z/z48QQFBfH1118zdOhQ6tevT8eOHQGYOHEic+bM4Y033qBLly4kJCSwe/duAEaMGMEjjzzCa6+9ho+P/WDOhQsXUrNmTa6//vpix+eODMMgbuefzFixm4PJGQDUDwtg4k3N6NmshiY4i0i5pATIGSyZMK1mgcseQEhpv/bTJ8D78vumBAcH4+3tjb+/PxEREY7rs2fPpl27dkybNs1xbd68eURFRbF3717Onj1Lbm4uAwYMoG7dugC0atXKUdbPz4/s7Ox8dRbm9ttvz3d/7ty51KhRg507d9KyZUsWLVrEyZMn+emnnxxJVMOGDR3lX3zxRe666y6ef/55x7U2bdpc9n1faOzYsfl6jgCeeOIJx/ePPvoo33zzDUuXLqVjx46kp6cza9Ys3n77be677z4AGjRoQJcuXRzv69FHH+Xzzz/nzjvvBOCDDz5g2LBhleKDf/vRM0z7ehc/Hj4FQLUAb8b2bMRdV9fBy6wOZhEpv5QAVXJbt27l+++/JzCw4OnaBw4cICYmhh49etCqVSt69+5NTEwMd9xxB1WrVi3W6xw4cIDJkyfzww8/kJyc7OjZiY+Pp2XLlmzfvp22bdtetAdp+/bt/POf/yz+G7xAhw4d8t23Wq3MmDGDJUuWcPz4cbKzs8nOziYgwJ5U7tq1i+zsbHr06FFofT4+PgwZMoR58+Zx5513sn37dn799dcKPyk8JQse+7/f+GpHIgA+nh6M6FqPkd0aUMVX86pEpPxTAuQMXv72npgL2Gw20tLTCapSBY/SHAK7AjabjX79+vHSSy8VeCwyMhKz2UxcXBybNm1i1apVvPXWW0yaNIktW7ZQr169Ir9Ov379iIqKYs6cOdSsWRObzUbLli3JyckB7D1Jl3K5x00mE4Zh5LtmsVgKlMtLbPK89tprvPHGG8ycOZNWrVoREBDA2LFjixwX2IfBrrrqKo4dO8a8efPo0aOHo7esoknNtPDmd3uYv92M1UjEZIIBbWvzeExjaoZcvq1ERMoLJUDOYDIVPgxls4GX1f5YaSVAxeDt7Y3Vas13rV27dixbtozo6Gg8PQv/dTCZTFx77bVce+21PPvss9StW5fPPvuMcePGFVrnhVJSUti1axfvvvsuXbt2BWDDhg35yrRu3Zr333+fU6dOFdoL1Lp1a7777jvuv//+Ql8jLCyMxMREx/19+/aRmVnIvKwLrF+/nltvvZUhQ4YA9oRw3759NGvWDIBGjRrh5+fHd999x4gRIwqto1WrVnTo0IE5c+awaNEi3nrrrcu+rrvJybXx0Q9HeGv1Ps5kWgATneuHMunm5rSoGezq8EREis31n8pSZqKjo9myZQuHDx92DEM9/PDDnDp1isGDB/Pjjz9y8OBBVq1axQMPPIDVamXLli1MmzaNn3/+mfj4eD799FNOnjzpSBCio6P57bff2LNnD8nJyYX2ulStWpVq1arx3nvvsX//flavXs24cePylRk8eDARERH079+fjRs3cvDgQZYtW8bmzZsBeO655/j444957rnn2LVrFzt27ODll192PP/666/n/fff55dffuHnn39m5MiRRVri3rBhQ0cP165du/jXv/6VL5Hy9fVl/PjxPPXUU3z44YccOHCAH374gblz5+arZ8SIEcyYMQOr1cptt91W9B9KOWcYBrE7Euj1xlr+/dVOzmRaaFQjgH81tTJ/WHslPyLitpQAVSJPPPEEZrOZ5s2bExYWRnx8PDVr1mTjxo1YrVZ69+5Ny5YtGTNmDMHBwXh4eBAUFMS6devo06cPjRs35plnnuG1117jpptuAuCf//wnTZo0oUOHDoSFhbFx48YCr+vh4cHixYvZunUrLVu25LHHHuOVV17JV8bb25tVq1ZRo0YN+vTpQ6tWrZgxYwZmsxmwb+K4dOlSvvjiC6666ipuuOGGfEvkX331VWrVqkX37t25++67eeKJJ/D3v/zw4OTJk2nXrh29e/eme/fujiTswjKPP/44zz77LM2aNWPQoEEkJSXlKzN48GA8PT25++678fX1LdLPo7zbeuQUt8/exKiFv3AkJZOwKj7MGNCKL0Z1onlVo1JM8haRistkXDhxQkhLSyM4OJjU1FSCgoLyPZaVlcWhQ4eoV6/eZT/obDYbaWlpBAUFld4cIAFc39ZHjx4lOjqan376iXbt2hX7+cX5vSpth5MzeOmb3az43d4T5udl5sHr6vPgdfUJ8PHEYrEQGxtLnz59tJFkGVB7lx21ddkprba+1Of3hTQHSOQKWCwWEhISmDBhAtdcc02Jkp/y4nRGDm+u3seCH45gsRp4mODODlE81qsx4UEVo1dLRCSPEiCRK7Bx40auv/56GjduzCeffOLqcEoky2Llf5sO8/b3+0nPygWge5MwJt7UjCYRVVwcnYhI6VACJHIFunfvXmD5vbuw2Qy+/O0EL3+zh+NnzgHQLDKIp/s0pWujMBdHJyJSupQAiVRCPxxMYVrsLn47lgpARJAvT/Ruwm1ta2H20ORmEan4lACVkLv+r1/Kp7L6fdqfdJYZK3bz7a4/AQjwNjPq+oY8cG09/LzNZRKDiEh5oASomPJmq2dmZhZpl2CRosjbtLG0Vp4kn81m1rf7WPRjPFabgdnDxOCroxjTozFhVXxK5TVFRMozJUDFZDabCQkJcewD4+/vf9H9UGw2Gzk5OWRlZWkZfClz17Y2DIPMzEySkpIICQlx7HvkLOdyrMzbeIjZaw5wNts+wblns3Am3NSUhjUKnv8mIlJZKAEqgbyTzy/cDO9ChmFw7tw5/Pz8tGlcKXP3tg4JCXH8XjmDzWbw6bbjvLZqDwmpWQC0rh3M032acU39ak57HRERd6UEqARMJhORkZHUqFGj0KMf8lgsFtatW8d1112nTbVKmTu3tZeXl1N7fjbsS2Za7C52JqQBUCvEj6dubEK/1jXx0ARnERFACdAVMZvNl/zgMpvN5Obm4uvr63Yfyu5GbQ17EtOZvmIXa/acBKCKryePXN+Q+zpH4+ulCc4iIn+nBEjEzSWlZfHGt3tZ8tNRbAZ4epgYck1dRvdoRGiAt6vDExEpl5QAibipjOxc5qw/yHvrDpKZYwXgppYRPHVjU+pVD3BxdCIi5ZsSIBE3Y7UZLP35KK/F7eVkejYAbeuE8EzfZrSvG+ri6ERE3IMSIBE3YRgGa/aeZEbsbvb8mQ5AnVB/xt/YlD6tItxy9ZuIiKsoARJxA3+cSGV67G427E8GINjPi9E9GjHkmjr4eGqCs4hIcSkBEinHElLP8erKvXy67RiGAd5mD4ZdG83D3RsS7F85V7uJiDiDEiCRcig9y8K7aw8yZ/1BsnNtANzSpiZP9m5CVKi/i6MTEXF/SoBEyhGL1cbin44yM24vKRk5AFwdHcrTfZtxVVSIa4MTEalAlACJlAOGYfDtriSmr9jFwZMZANSvHsCEm5rSq3m4JjiLiDiZEiARF/vt2Ble/HoXWw6dAiA0wJuxPRsx+Oo6eJnd52BXERF3ogRIxEWOnsrk1VV7+Hz7CQB8PD0Y3qUeI7s3IMhXE5xFREqTEiCRMpZ6zsI73+/ng02Hycm1YTLBbW1r8URME2qG+Lk6PBGRSkEJkEgZycm1sXDLEWZ9t48zmRYAOjeoxtN9mtGyVrCLoxMRqVyUAImUMsMw+Ob3RF76ZjeHUzIBaFQjkKf7NKN7kzBNcBYRcQElQCKlaOuR00yL3cXWI6cBqB7ow+MxjRnYvjaemuAsIuIySoBESsGRlAxe/mYPX+9IAMDPy8w/r6vPv66rT4CP/tmJiLia/hKLONHpjBzeWr2fj344jMVqYDLBne2jGBfTmPAgX1eHJyIi5ykBEnGCLIuVDzcf5u3V+0nLygWgW+MwJvZpStOIIBdHJyIiF1ICJHIFDMPgy98SePmb3Rw7fQ6AphFVmNS3GV0bhbk4OhERuRglQCIltOVgCtNid/HrsVQAwoN8eCKmCQPa1cbsoZVdIiLlmRIgkWI6cPIsL63YzaqdfwIQ4G1mZLcGjOhaHz9vs4ujExGRolACJFJEKWezmfXdPhZuicdqMzB7mLjrH1GM7dmYsCo+rg5PRESKQQmQyGVkWazM3XCI2WsOcDbbPsG5Z7MaTLipKQ1rVHFxdCIiUhJKgEQuwmYz+GzbcV5btYcTqVkAtKoVzNN9mtGpQTUXRyciIldCCZBIITbuT2Za7C7+OJEGQK0QP57s3YRb2tTEQxOcRUTcnsv34n/nnXeoV68evr6+tG/fnvXr11+07LBhwzCZTAVuLVq0yFdu5syZNGnSBD8/P6KionjsscfIysoq7bciFcDeP9O5/4Mfuef9LfxxIo0qPp5MuKkp3z3ejf5tayn5ERGpIFzaA7RkyRLGjh3LO++8w7XXXsu7777LTTfdxM6dO6lTp06B8rNmzWLGjBmO+7m5ubRp04aBAwc6ri1cuJAJEyYwb948OnfuzN69exk2bBgAb7zxRqm/J3FPSelZvBG3jyU/xWMzwNPDxJBr6jK6RyNCA7xdHZ6IiDiZSxOg119/neHDhzNixAjA3nOzcuVKZs+ezfTp0wuUDw4OJjg42HF/+fLlnD59mvvvv99xbfPmzVx77bXcfffdAERHRzN48GB+/PHHi8aRnZ1Ndna2435amn3Yw2KxYLFYSvz+8p57JXVI0ZS0rTNzcpm78QjvbzhMZo4VgJjmNXgyphHR1QJKVGdF55a/15kpmL9+DNPJXRhV62NUawChDTBCG2CE1oegWuBRPrcwcMv2dlNq67JTWm1dnPpMhmEYTn31IsrJycHf35+lS5dy2223Oa6PGTOG7du3s3bt2svW0a9fP7Kzs1m1apXj2uLFixk5ciSrVq3i6quv5uDBg/Tt25f77ruPCRMmFFrPlClTeP755wtcX7RoEf7+/iV4d1Le2QzYkmQi9qgHaRb7sFbdQIP+da3U18kVFYpvTgqd979MleyEi5axmrzI8KnBWZ8IMnwiOOsb4fg+2zMITBr6FHEHmZmZ3H333aSmphIUdOk/5i7rAUpOTsZqtRIeHp7venh4OImJiZd9fkJCAitWrGDRokX5rt91112cPHmSLl26YBgGubm5PPTQQxdNfgAmTpzIuHHjHPfT0tKIiooiJibmsg14KRaLhbi4OHr16oWXl1eJ65HLK05br9+XzEsr97Lnz7MA1K7qx1MxjbixRTgmfdBdllv9Xifvw/PjiZiyEzCq1MTaewZkJmM6dQBTygFMpw7A6cOYbRaCso4TlHW8QBWGTxWMqvWhWl6PUQMIrY8R2hB8Sz9bdqv2dnNq67JTWm2dN4JTFC5fBXbhB45hGEX6EJo/fz4hISH0798/3/U1a9bw4osv8s4779CxY0f279/PmDFjiIyMZPLkyYXW5ePjg49PwY3svLy8nPKDcVY9cnmXauudJ9KYvmIX6/clAxDs58WjNzRkaKe6+HiWz+GP8qzc/14f/wUW3A7nTkG1RpiGfoZnSFTBctZcSD0KKQcgZb/9dur892eOYspOx5T4KyT+WvC5AWFQrSGcH1Kzf98QQuuBl59T3065b+8KRG1ddpzd1sWpy2UJUPXq1TGbzQV6e5KSkgr0Cl3IMAzmzZvH0KFD8fbOP0F18uTJDB061DGvqFWrVmRkZPDggw8yadIkPDxcvvBNylhiahavrdrDJ78cwzDA2+zBfZ3r8sj1jQj21x+5CungWlh8N+SchZpt4Z5PIKB64WXNnvaEJbQeNOqZ/zFLFpw+lD85yvs+IwkyTtpv8ZsvqNQEwbXtiZEjKWpgvx9S1/6aIuJSLvtX6O3tTfv27YmLi8s3ByguLo5bb731ks9du3Yt+/fvZ/jw4QUey8zMLJDkmM1mDMPARdOdxEXOZufy7toDzFl/kCyLDYB+bWryVO8mRIVqbleFtfMLWDYcrDlQ7zq4axH4lHDHbi9fqNHMfrtQVtr5nqID+ROjlP2QnWbvVUo9CgfX5H+ehydUrfdXz9Hfk6QqkZpvJFJGXPrfkHHjxjF06FA6dOhAp06deO+994iPj2fkyJGAfW7O8ePH+fDDD/M9b+7cuXTs2JGWLVsWqLNfv368/vrrtG3b1jEENnnyZG655RbMZg1zVAa5VhuLfzrKzG/3knw2B4B/RFfl6T7NaFunqoujk1L1y4fw5RgwbNCsHwx4357ElAbfIHvvUs22+a8bBmQk5x9Ky0uQTh2E3CxI2We/XcjL/6+eovNJkSm4Ll656aXzHkQqMZcmQIMGDSIlJYWpU6eSkJBAy5YtiY2NpW7duoB9onN8fHy+56SmprJs2TJmzZpVaJ3PPPMMJpOJZ555huPHjxMWFka/fv148cUXS/39iGsZBqzec5KXV+7lwMkMAOpVD2DCTU2Jaa4JzhXehpnw7XP279sOhX6zXLO03WSCwDD7rW6n/I/ZbJB2/G9zjQ7+9f3pI2DJhD932G/neQJ9AGP/M38bSmv4V5IUWh98Asv0LYpUBC4fiB41ahSjRo0q9LH58+cXuBYcHExmZuZF6/P09OS5557jueeec1aI4gZ+P57G2zs92P/DNgBCA7wZ06MRd3esg5dZ874qNMOAuGdh05v2+9eOhZ5TyudQkocHhETZbw2uz/9Ybg6cOXLBfKP9GCkHMKWfwHTuNBz7yX67UJXIC5Ki81+rRoOnNvIUKYzLEyCRK3HsdCavrtzD8u0nAA+8PT0Y3qUeD3VvQJCvJjhXeNZc+GoMbFtgv9/r33DtaNfGVFKe3lC9kf32N7kWCyu//IzeVzfCK/Xw+cTobz1H505BeoL9dviCo4RMHvZJ13+fZ5T3fVBte0ImUkkpARK3lHrOwjtr9vPBxsPk5NonOP+juo1X7+tK3TDtZFgpWLLsk513f2X/oL/lLWg7xNVRlQqr2QfCW0LttgUfzDyVfyjNMSH7AFgy7KvYTh+C/d/mf57Zxz58Vu2CIbVqDe3L+8tjD5qIEykBEreSk2tj0ZYjzPpuH6cz7Vued6pfjfG9G3Fk+wZqhjh37xUpp7LS7MvcD6+3f5DfMQ+a3ezqqFzDP9R+q90h/3XDgPTEC/Y2Oj+8duoQWLPh5C777UI+QQX3NspbseYbXLC8iBtSAiRuwTAMVv6RyIwVuzmcYp8D1rBGIE/3acr1TWqQm5vLke2ujVHKyNmTsPB2SPgVvKvA4EX25e6Sn8kEQZH2W72u+R8rbPPHvCTpzFH7Mv4T2+y3C5Xh5o8ipUkJkJR7v8SfZtrXu/j5yGkAqgf68FivRgzqEIWnJjhXLmfi4aPb7B/W/tVgyLKCy9Dl8oq0+ePf9zYq6uaPUVCt/t96jc6vUtPmj1IO6TdSyq34lExeWrmbr3+zH2Lp6+XBg13r82C3BgT66Fe30knabU9+0k/YP2iHflZgwrA4QbE2f/xbcpSdBqnx9luBzR+97CvStPmjlCP6FJFy50xmDm+t3s+Hmw9jsRqYTDCwfW3G9WpCRHApbWon5duxn2HhHXDuNFRvYk9+gmu5OqrKp4w2f3R87x9aNu9LKiUlQFJuZOda+XDTEd5avY+0rFwArmscxsSbmtIsUiu7Kq0Dq2HxEPuKplrt7ed66YOxfHHy5o8OflULnqWWlyB5B5TNe5MKSwmQuJxhGHz1WwIvr9zN0VPnAGgaUYWn+zTjusZhLo5OXOqP5bBsBNgsUP96GLRAux67mxJs/sipg/akqTibP+YlSdr8UYpICZC41E+HT/Hi17vYfvQMAOFBPjwe04Tb29XG7KF5AZXazx/AV48BBjTvDwPeA08fV0clznSRzR8ByMm4YH+j4m7+2JACexxp80f5GyVA4hIHT57lpW92s/KPPwHw9zbzULcGDO9aD39v/VpWaoYBG16H76ba77e/H/q+5ppzvcR1vAMgopX9dqEib/4Yl/95BTZ//FtyFKDe5spGnzRSplLOZvPmd/tYuCWeXJuBhwnuuroOY3s2okYVTXCu9Gw2WPUM/PAf+/2uj8MNk7VKSPIrpc0fzaH1aZ/li8e6HRDWRJs/VnBKgKRMZFmszNt4iNnfHyA92z7BuUfTGky4qSmNwqu4ODopF6y58MWj8Osi+/2YF6HzI66NSdzLFW7+6JGwndoA63/I/9y/b/7498NmtfmjW1MCJKXKZjNYvv04r67cw4nULABa1gri6T7N6Nyguoujk3LDcg4+eQD2xILJDLf+B64a7OqopCIpwuaPuX/uYc/mWJqFeeKRtxlkSTZ/rNYAguto88dyTj8dKTWbDiQzLXYXvx9PA6BmsC9P3tiEW9vUwkMTnCVPVip8PBiObLTP0Rg4H5r2cXVUUpmc3/zRqNqQ/QdNNO7TBw8vL/tj2vyxwlICJE637890ZqzYzXe7kwCo4uPJqOsbcv+10fh6aSKr/M3ZJFgwABJ32A/gHPwxRHdxdVQifynJ5o8pB+zzjS66+WOAvdco32Gz5xMl7XFVZpQAidMkpWcx89t9LP4xHpsBnh4mhlxTl0dvaEi1QC1flgucPgIf9bev5gkIs5/rFdnG1VGJFE1xNn9MOfBXknT6iH2lWuIO++1C2vyxzCgBkiuWmZPL++sP8d+1B8jMsQLQu0U4429sSv0wbVonhfhzp73nJz3BPlfi3uX2P/AiFUFJNn9MOWA/5+6Smz/WLDicps0fS0wJkJSY1Waw7JdjvLZqD3+mZQPQJiqEZ/o24x/R6saVizj6IywcCFlnIKwZDP0Ugmq6OiqRsnFFmz+esN8uufnjBXOOtPnjRSkBkhJZt/ck02J3sTsxHYCoUD+e6t2Um1tHYtLkPrmY/d/CkqH2859q/wPu/j/NeRDJ44rNHyvx32slQFIsuxLSmBa7i/X7kgEI8vVkdI9GDO1UFx9PTXCWS/h9GXz6L/u5Xg16wKCPNKdBpKhKafPHAnsbVaLNH5UASZEkpmbxetwelm49hmGAl9nEfZ2ieeSGhoT4a+xZLuOn9+HrJwADWgyA297VnAURZ7jCzR85sc1+u9CFmz/mJUkVaPNHJUBySWezc3lv7QHeW3+QLIsNgJtbR/JU76bUqebv4uik3DMMWPcKfP+i/X6H4dDnFZ3rJVIWirD5Y759jfK+FmnzxwYFh9XcbPNH94lUylSu1caSn4/yRtw+ks/aJzh3qFuVp/s2o12dqi6OTtyCzQYrn4Yts+33u42H7hMr9ZwDkXLj/OaP1GhW8LEib/74ff7nFdj88W9fy+Hmj0qApIB1e08y9aud7E86C0B0NX8m3NSU3i0iNMFZisZqgc8fht+W2O/f+BJcM9K1MYlI0ZTB5o+mkHqEZJwsm/dzEUqAJJ+DJ8/ywPyfyLUZVPX3YkyPRtzdsS7enlpGKUWUkwlLh8G+lfZzvfrPhjaDXB2ViFwpJ27+6Alc5RsFuO7AYyVAks/WI6fJtRm0qBnExw9eQ5Cvl6tDEndy7gx8fJd93oCnL9z5ITTu7eqoRKS0FXPzR1vyPk5leOPKmaRKgCSfvX/a9/X5R3Sokh8pnvQ/7bs7//k7+ATD3UsK/i9RRCqfQjZ/tFos/BYbS21XhuXC15ZyaO+f9nk/jcOruDgScSunDtnP9Tp9GAJq2Hd3LmwzNxGRckIJkOSz73wPUONwneElRZT4u73n5+yf9u34711u33lWRKQcUwIkDmlZFk6kZgHQSD1AUhTxP8CiOyErFWq0sPf8VIlwdVQiIpelBEgc9p0f/ooI8iXYT/N/5NJM++Ng2QOQew6iOtrn/PhpjygRcQ9KgMQhb/irkYa/5DJqndqEeen7YMuFhr3sq728tTO4iLgPJUDisOd8AtREw19yCR4/vU+HI/+132k10L7Pj1k9hiLiXpQAicM+rQCTSzEMWDMD89oZAFg7jMDc5xX7/h8iIm5Gf7nEYY+GwORibDZY8RScT352RQzAFjNdyY+IuC31AAkAZzJzOJluP/RUK8Akn9wcWP4Q/P4JYMLaewZ7kyJpqHPhRMSN6b9vAvy1AWKtED8CfZQXy3k5GbB4sD358fCE29/H1mG4q6MSEbli+qQT4K/hL22AKA7nTsOiQXB0C3j6waAF0KgnWCyujkxE5IopARLgbztAR2j4S4C0BPvuzkk7wTcY7l4KdTq6OioREadRAiTAX4egNq6hBKjSSzkAH91mP705MMK+u3N4C1dHJSLiVEqABPhrDlAT9QBVbok74KMBkJEEVevB0M8gtJ6roxIRcTolQELy2WxOZeRgMkGDMM0BqrSObIJFd0F2KoS3giHLoEq4q6MSESkVSoCEvYn24a86of74eZtdHI24xJ5vYOl9kJsFdTrB4MXgF+LqqERESo0SIPlr/o/2/6mcfl0My0eBYYXGN8IdH+hcLxGp8LQPkLA3Ke8IDA1/VTo/zIbP/mVPfloPsi91V/IjIpWAeoDEMQSmHqBKxDDg+xdh3Sv2+x0fgt7TdLSFiFQaSoAqOcMwNARW2disEPsE/DzPfv+GZ6DrE6CjLUSkElECVMn9mZZNWlYuZg8T9cMCXB2OlLbcHPuQ1x+fAibo+xr8Q0dbiEjlowSoksvr/Ymu5o+Pp1aAVWg5GbBkCBxYDR5eMOA9aDnA1VGJiLiEEqBKTsNflUTmKVg4EI7/DF7+9snODXu4OioREZdRAlTJ5SVAjZQAVVxpJ+xHW5zcDb4hcM8nEPUPV0clIuJSSoAqOccRGEqAKqaUA/Bhf0iNhyqR9qMtajRzdVQiIi6nBKgSMwzjr1PgtQdQxZPwq/1cr8xkCG1gT36q1nV1VCIi5YISoErs+JlzZORY8TKbiK6uFWAVyuEN8PFgyE6DiNb2c70Ca7g6KhGRckMJUCW27/zwV/3qgXiZtQFehbE7FpYOA2s21O0CgxeBb7CroxIRKVdc/qn3zjvvUK9ePXx9fWnfvj3r16+/aNlhw4ZhMpkK3Fq0aJGv3JkzZ3j44YeJjIzE19eXZs2aERsbW9pvxe3scUyA1vBXhbF9kX2puzUbmvSx9/wo+RERKcClCdCSJUsYO3YskyZNYtu2bXTt2pWbbrqJ+Pj4QsvPmjWLhIQEx+3o0aOEhoYycOBAR5mcnBx69erF4cOH+eSTT9izZw9z5syhVq1aZfW23EbeCjBNgK4gNr0Nyx+yn+vV5m648yPw8nV1VCIi5ZJLh8Bef/11hg8fzogRIwCYOXMmK1euZPbs2UyfPr1A+eDgYIKD//rf7PLlyzl9+jT333+/49q8efM4deoUmzZtwsvLC4C6dS898TM7O5vs7GzH/bS0NAAsFgsWi6XE7y/vuVdSR2nKOwOsfnW/chtjUZX3ti5VhoHHmhcxb5oJgLXjKGw9poDNAJvz26NSt7ULqL3Ljtq67JRWWxenPpNhGIZTX72IcnJy8Pf3Z+nSpdx2222O62PGjGH79u2sXbv2snX069eP7OxsVq1a5bjWp08fQkND8ff35/PPPycsLIy7776b8ePHYzYXvtPxlClTeP755wtcX7RoEf7+FfNkbJsBT/1oxmIzMemqXGr4uToiKRHDRpuj84lOWQPAzsiB7Au/Wed6iUillJmZyd13301qaipBQUGXLOuyHqDk5GSsVivh4eH5roeHh5OYmHjZ5yckJLBixQoWLVqU7/rBgwdZvXo199xzD7Gxsezbt4+HH36Y3Nxcnn322ULrmjhxIuPGjXPcT0tLIyoqipiYmMs24KVYLBbi4uLo1auXozeqvDhyKhPLDxvw9vRg6G03YfZw7w/M8tzWpSY3G/PnD+GRsgbD5IH1pldp1PZeGpXyy1bKtnYhtXfZUVuXndJq67wRnKJw+Sow0wX/UzUMo8C1wsyfP5+QkBD69++f77rNZqNGjRq89957mM1m2rdvz4kTJ3jllVcumgD5+Pjg4+NT4LqXl5dTfjDOqseZDqVkAdAwLBBfH28XR+M85bGtS0X2WVh6DxxcA2ZvTAPm4Nmif5mGUGnaupxQe5cdtXXZcXZbF6culyVA1atXx2w2F+jtSUpKKtArdCHDMJg3bx5Dhw7F2zv/h3dkZCReXl75hruaNWtGYmIiOTk5BcpXVo4J0BGaAO12MlJg0UA4vhW8AuCuhdDgeldHJSLiVly2Cszb25v27dsTFxeX73pcXBydO3e+5HPXrl3L/v37GT58eIHHrr32Wvbv34/NZnNc27t3L5GRkUp+/mavlsC7p9Rj8MGN9uTHLxTu+1LJj4hICZQoAVqzZo1TXnzcuHG8//77zJs3j127dvHYY48RHx/PyJEjAfvcnHvvvbfA8+bOnUvHjh1p2bJlgcceeughUlJSGDNmDHv37uXrr79m2rRpPPzww06JuaLYc34FWOMa6gFyG8n7YG5vSN4LQbXggW+gdntXRyUi4pZKNAR24403UqtWLe6//37uu+8+oqKiSvTigwYNIiUlhalTp5KQkEDLli2JjY11LFtPSEgosCdQamoqy5YtY9asWYXWGRUVxapVq3jsscdo3bo1tWrVYsyYMYwfP75EMVZEuVYbB09mABoCcxsntsGC2yEzBao1sp/rFVKyf3ciIlLCBOjEiRMsWLCA+fPnM2XKFHr06MHw4cPp379/sYeZRo0axahRowp9bP78+QWuBQcHk5mZeck6O3XqxA8//FCsOCqTI6cyybHa8PMyUytE69/LvUPr7Od65ZyFyKvsuzsHVHd1VCIibq1EQ2ChoaGMHj2aX375hZ9//pkmTZo4jp4YPXo0v/76q7PjFCfK2wCxcXggHm6+/L3C2/Wlvecn5yxEd4VhXyn5ERFxgiueBH3VVVcxYcIEHn74YTIyMpg3bx7t27ena9eu/PHHH86IUZxs7/lDUBvpCIzy7ZeP4P/uBWsONL0Z7vkEfPQzExFxhhInQBaLhU8++YQ+ffpQt25dVq5cydtvv82ff/7JoUOHiIqKyndGl5Qfe5P+6gGScmrjLPjiETBs0HYoDPyfzvUSEXGiEs0BevTRR/n4448BGDJkCC+//HK+FVkBAQHMmDGD6OhopwQpzvXXEJh6E8odw4Bvn7MnQADXjoGez+toCxERJytRArRz507eeustbr/99otOeq5Zsybff//9FQUnzpeTa+NQsn0FmBKgcsaaC1+NhW0f2e/3mmpPgERExOlKlAB99913l6/Y05Nu3bqVpHopRYeSM8i1GVTx8SQyWEMq5YYlC5YNh91fgckD+r0J7Ya6OioRkQqrRHOApk+fzrx58wpcnzdvHi+99NIVByWl5+87QBflzDUpA1lpsPAOe/Jj9oY7P1TyIyJSykqUAL377rs0bdq0wPUWLVrw3//+94qDktKz70/N/ylXMpLhf/3g8HrwDrTv8dOsn6ujEhGp8Eo0BJaYmEhkZGSB62FhYSQkJFxxUFJ69igBKj/OHIWPboOUfeBfzb7MvVY7V0clIlIplKgHKCoqio0bNxa4vnHjRmrWrHnFQUnp2Xd+DyAlQC52cg/M621PfoJqwwMrlfyIiJShEvUAjRgxgrFjx2KxWLjhhhsA+8Top556iscff9ypAYrzZFmsHE7JWwGmPYBc5thW+5yfc6egemP7uV7BtV0dlYhIpVKiBOipp57i1KlTjBo1ipycHAB8fX0ZP348EydOdGqA4jwHTp7FZkCIvxdhVXxcHU7ldOB7WHwPWDKgZjv7sFdANVdHJSJS6ZQoATKZTLz00ktMnjyZXbt24efnR6NGjfDx0YdqeeYY/qpRRSvAXGHn57BshP1oi/rdYdACHW0hIuIiJUqA8gQGBvKPf/zDWbFIKdvztyXwUsa2zoevHrMfbdH8VhgwBzz1HwYREVcpcQL0008/sXTpUuLj4x3DYHk+/fTTKw5MnC9vCXyTCPU6lBnDgA1vwHfP2++3HwZ9XwcPs0vDEhGp7Eq0Cmzx4sVce+217Ny5k88++wyLxcLOnTtZvXo1wcHBzo5RnMRxCnwNJUBlwjBg1TN/JT9dxsHNM5X8iIiUAyVKgKZNm8Ybb7zBV199hbe3N7NmzWLXrl3ceeed1KlTx9kxihNk5uQSfyoT0AqwMmHNhc8fhs1v2+/HvAA9n9OhpiIi5USJEqADBw7Qt29fAHx8fMjIyMBkMvHYY4/x3nvvOTVAcY79Sfben+qB3lQL1NyTUmXJgv+7F7YvBJMZ+s+Gzo+6OioREfmbEiVAoaGhpKfb55PUqlWL33//HYAzZ86QmZnpvOjEafYknp8AreGv0pWVCgtuhz1fg9nHvtLrqrtdHZWIiFygRJOgu3btSlxcHK1ateLOO+9kzJgxrF69mri4OHr06OHsGMUJ9p3vAdIE6FJ09iQsGACJv4F3Fbh7MUR3cXVUIiJSiBIlQG+//TZZWVkATJw4ES8vLzZs2MCAAQOYPHmyUwMU59irJfCl6/QR+7lepw5AQJj9UNPINq6OSkRELqLYCVBubi5ffvklvXv3BsDDw4OnnnqKp556yunBifPsTdQhqKUmaRd8NADST0BwHbh3OVRr4OqoRETkEoo9B8jT05OHHnqI7Ozs0ohHSkF6loUTqfYeu8aaA+RcR3+CeTfak5+wpjB8pZIfERE3UKJJ0B07dmTbtm3OjkVKSd78n/AgH4L9vVwcTQWy/zv48BbIOgO1OsD9KyCopqujEhGRIijRHKBRo0bx+OOPc+zYMdq3b09AQEC+x1u3bu2U4MQ5NPxVCn7/FD59EGwWaHAD3PkR+Gh+lYiIuyhRAjRo0CAARo8e7bhmMpkwDAOTyYTVanVOdOIUeTtAKwFykp/mwtePAwa0GAC3vQue3q6OSkREiqFECdChQ4ecHYeUorwVYNoB+goZBqx/FVa/YL/f4QHo86qOthARcUMlSoDq1q3r7DikFP2VAKkHqMRsNlg1CX54x37/uqfg+qd1tIWIiJsqUQL04YcfXvLxe++9t0TBiPOdycwhKd2+Yq+REqCSsVrg80fgt8X2+zfOgGsecm1MIiJyRUqUAI0ZMybffYvFQmZmJt7e3vj7+ysBKkfy5v/UCvEj0KdEP+7KzXIOlg6Dvd/8da5Xm0GujkpERK5QiT4RT58+XeDavn37eOihh3jyySevOChxHs3/uQLnzsDHgyF+E3j6wsD/QZMbXR2ViIg4QYn2ASpMo0aNmDFjRoHeIXGtfZr/UzLpf8L8m+3Jj08QDP1MyY+ISAXi1DERs9nMiRMnnFmlXKE9SoCK7/Rh+LA/nD4EATXOn+ulva1ERCqSEiVAX3zxRb77hmGQkJDA22+/zbXXXuuUwMQ59mkPoOL58w/7uV5nEyGkrv1cr9D6ro5KREScrEQJUP/+/fPdN5lMhIWFccMNN/Daa685Iy5xguSz2aRk5GAyQcMamgN0WfFbYNFAyEqFGs1hyKcQFOnqqEREpBSUKAGy2WzOjkNKQd4E6Dqh/vh5a7O+S9r3LSwZArnnIKoj3L0E/Kq6OioRESklWhddgeUNfzXSCfCXtuMT+OxfYMuFhj3hzg/BO+DyzxMREbdVolVgd9xxBzNmzChw/ZVXXmHgwIFXHJQ4R94E6CYRGv66qB/nwLIR9uSn5R1w18dKfkREKoESJUBr166lb9++Ba7feOONrFu37oqDEufQEvhLMAxY8xLEPgEYcPWDMGCODjUVEakkSjQEdvbsWby9C35QeHl5kZaWdsVByZUzDIM9ifYESENgF7DZ4JsJ8OO79vvdJ0K38TrXS0SkEilRD1DLli1ZsmRJgeuLFy+mefPmVxyUXLmk9GzSsnIxe5ioH6YhHQerBT578K/k56ZXoPsEJT8iIpVMiXqAJk+ezO23386BAwe44YYbAPjuu+/4+OOPWbp0qVMDlJLJWwFWt5o/vl5aAQZATib8372wPw48PKH/f6G15qyJiFRGJUqAbrnlFpYvX860adP45JNP8PPzo3Xr1nz77bd069bN2TFKCeQNfzXW8JfdudOw6C44+gN4+sGgj6BRL1dHJSIiLlLiZfB9+/YtdCK0lA+OHaAjlACRnmjf3TnpD/ANhrv/D+pc4+qoRETEhUqUAP3000/YbDY6duyY7/qWLVswm8106NDBKcFJye1N0inwAJw6CB/dZj/fKzDcvrtzREtXRyUiIi5WoknQDz/8MEePHi1w/fjx4zz88MNXHJRcGcMwHD1ATSrzEvjEHTC3tz35qRoND6xU8iMiIkAJe4B27txJu3btClxv27YtO3fuvOKg5MqcSM3ibHYuXmYT0dUr6QqwI5th0SDIToXwVvYT3auEuzoqEREpJ0rUA+Tj48Off/5Z4HpCQgKenjpdw9X2np8AXa96AF7mEv2I3dvelfZhr+xUqNMJhn2l5EdERPIp0adjr169mDhxIqmpqY5rZ86c4emnn6ZXL62scbW9lXgHaNPvS+HjwfZDTRv1ts/58QtxdVgiIlLOlKi75rXXXuO6666jbt26tG3bFoDt27cTHh7ORx995NQApfj25q0Aq2QJUP2kVXhuW2C/03oQ3PofMHu5NigRESmXSpQA1apVi99++42FCxfy66+/4ufnx/3338/gwYPx8tIHjqv91QNUSVaAGQYea6fT6vj55KfjQ9B7GnhUwuE/EREpkhJP2AkICKBLly7UqVOHnJwcAFasWAHYN0oU17DZDPYnVaIeIJsNVjyJ+af3AbB2m4i5u871EhGRSytRAnTw4EFuu+02duzYgclkwjAMTH/7wLFarU4LUIrn2OlznLNY8fb0oG61Cr4CLDcHlo+E35dhYOK32vfSvMvjmJX8iIjIZZRojGDMmDHUq1ePP//8E39/f37//XfWrl1Lhw4dWLNmjZNDlOLYc374q2FYIGaPCpwI5GTAx3fB78vAwwvrbe9xOKyHq6MSERE3UaIEaPPmzUydOpWwsDA8PDwwm8106dKF6dOnM3r0aGfHKMVQKeb/ZJ6CD2+FA9+Blz/cvRij+W2ujkpERNxIiRIgq9VKYKD9A7Z69eqcOHECgLp167Jnzx7nRSfFlpcANaqo83/STsAHfeDYT+AbAvd+AQ17ujoqERFxMyVKgFq2bMlvv/0GQMeOHXn55ZfZuHEjU6dOpX79+sWq65133qFevXr4+vrSvn171q9ff9Gyw4YNw2QyFbi1aNGi0PKLFy/GZDLRv3//YsXkzvZW5CMwUg7AvN5wchdUiYQHvoGof7g6KhERcUMlSoCeeeYZbDYbAC+88AJHjhyha9euxMbG8uabbxa5niVLljB27FgmTZrEtm3b6Nq1KzfddBPx8fGFlp81axYJCQmO29GjRwkNDWXgwIEFyh45coQnnniCrl27luQtuqVcq40DJyvoCrCEX+3Jz5l4CK1vP9erRjNXRyUiIm6qRKvAevfu7fi+fv367Ny5k1OnTlG1atV8q8Eu5/XXX2f48OGMGDECgJkzZ7Jy5Upmz57N9OnTC5QPDg4mODjYcX/58uWcPn2a+++/P185q9XKPffcw/PPP8/69es5c+ZMMd+hezpyKpOcXBt+XmZqV/VzdTjOc3ijfcJzdhpEtLLv7hxYw9VRiYiIG3PawV2hoaHFKp+Tk8PWrVuZMGFCvusxMTFs2rSpSHXMnTuXnj17Urdu3XzX8yZoDx8+/JJDanmys7PJzs523E9LSwPAYrFgsViKFEth8p57JXUUx67jZwBoWCMAqzWXirAbgWnvN5g/G4EpNwtbnU5YBy4EnyC4oE3Luq0rM7V12VJ7lx21ddkprbYuTn0uO7k0OTkZq9VKeHj+QyrDw8NJTEy87PMTEhJYsWIFixYtynd948aNzJ07l+3btxc5lunTp/P8888XuL5q1Sr8/f2LXM/FxMXFXXEdRfHNURNgxi/nDLGxsWXymqUpKmUDV8W/jwkbCcFt+bnqcGyrN1zyOWXV1qK2Lmtq77Kjti47zm7rzMzMIpd1+dHtFw6ZXbip4sXMnz+fkJCQfBOc09PTGTJkCHPmzKF69epFjmHixImMGzfOcT8tLY2oqChiYmIICgoqcj0XslgsxMXF0atXrzI5ImTlkl/h2J9c364pfbpEl/rrlSaPLbMxb3sPAFvru6jedyY3elz817Ws27oyU1uXLbV32VFbl53Sauu8EZyicFkCVL16dcxmc4HenqSkpAK9QhcyDIN58+YxdOhQvL29HdcPHDjA4cOH6devn+Na3mRtT09P9uzZQ4MGDQrU5+Pjg4+PT4HrXl5eTvnBOKuey9l/MgOApjWD3fcfr2HA6hdg/av2+50ewaPXv/Eo4rleZdXWorYua2rvsqO2LjvObuvi1OWyBMjb25v27dsTFxfHbbf9tYldXFwct9566yWfu3btWvbv38/w4cPzXW/atCk7duzId+2ZZ54hPT2dWbNmERUV5bw3UM7k5No4eD4BctsVYDYrfP04bP3Afr/Hs9BlnM71EhERp3PpENi4ceMYOnQoHTp0oFOnTrz33nvEx8czcuRIwD40dfz4cT788MN8z5s7dy4dO3akZcuW+a77+voWuBYSEgJQ4HpFczglg1ybQaCPJzWDfV0dTvHlZsOnD8LO5YAJbn4DOtx/uWeJiIiUiEsToEGDBpGSksLUqVNJSEigZcuWxMbGOlZ1JSQkFNgTKDU1lWXLljFr1ixXhFxu/bUDdGCxtiIoF7LPwpIhcPB78PCC29+HFv1dHZWIiFRgLp8EPWrUKEaNGlXoY/Pnzy9wLTg4uFizvAuroyLam2hPgNxuB+jMU7DwDji+FbwC4K4F0OAGV0clIiIVnMsTIHGOvCMw3OoMsNTj8NFtkLwH/KrCPcugdntXRyUiIpWAEqAKwu1OgU/eDx/1h9SjUKUm3Lscwpq4OioREakklABVAFkWK4dT7CvA3GII7MR2WHA7ZCZDtYYw9DMIqePqqEREpBJRAlQBHDyZgc2AYD8vwqoU3M+oXDm0Hj4eDDnpEHkVDFkGAUXftFJERMQZlABVAH8f/irXK8B2fQWfPADWbIjuCnctAt+S77QtIiJSUkqAKoC/EqByPPy1bQF88SgYNmh6M9w+F7zccL8iERGpEIp2voCUa3krwMptArTxTfj8YXvy03YIDPyfkh8REXEp9QBVAOW2B8gw4NspsHGm/X7n0dBrqo62EBERl1MC5OYyc3I5etq+MWS5WgJvs8JXY+GX88eY9Hweuox1ZUQiIiIOSoDc3P6ksxgGVAvwplpgOVkBZsmCT0fAri/B5AH9ZkG7e10dlYiIiIMSIDdX7ub/ZKfD4rvh0Dowe9snOze/xdVRiYiI5KMEyM3tK087QGekwMLb4cQ28A60L3Ov383VUYmIiBSgBMjN7clLgCJc3AOUegw+7A8p+8C/GtzzCdRq59qYRERELkIJkJvbVx6GwE7utR9qmnYMgmrbj7YIa+y6eERERC5DCZAbS8+ycPzMOQAa13BRAnR8Kyy4A86dguqN7clPcG3XxCIiIlJESoDc2L4ke+9PeJAPwf5eZR/AwTWw+B7IOQs129mHvQKqlX0cIiIixaQEyI3tc+UGiDu/gGXDwZoD9brBXQvBp5ysRBMREbkMHYXhxvYk2nuAGpX18NfW/8HS++zJT7Nb4J6lSn5ERMStKAFyY/uS7D1ATSLKcAn8hjfgy9H2c73a3QcD54NnOdmAUUREpIg0BObG8s4Aa1QWQ2CGAXGTYdNb9vtdHoMez+lcLxERcUtKgNxUaqaFP9OyAWhUo5R7gKy58OUY2L7Afj/mBej8aOm+poiISClSAuSm9p4f/qoV4kcV31JcAWbJsk923v0VmMxwy1vQ9p7Sez0REZEyoATITe1JzBv+KsXen6w0+7leh9eD2QcGfgBN+5be64mIiJQRJUBuKm8JfJPSmv9z9qT9XK+EX8G7Cgz+GOp1LZ3XEhERKWNKgNxU3inwpTIB+ky8/WiLlP3gXx2GLIOaVzn/dURERFxECZCb2ltap8BnnoJ5N0LacQiOgqHLoXpD576GiIiIiykBckMn07NJycjBZIKGzl4B9vsye/ITUhfuXwHBtZxbv4iISDmgjRDd0O7ENADqVQvA39vJOewfn9m/Xv2gkh8REamwlAC5od0J9uGvppFOnv+TlgBHNtm/b9HfuXWLiIiUI0qA3NCu8z1ATSOCnFvxzuWAAVEdIbi2c+sWEREpR5QAuaFd53uAmkU6OQH6/VP71xYDnFuviIhIOaMEyM1YrDb2n98FummEE4fAzhyFYz8CJmh+q/PqFRERKYeUALmZgyczsFgNAn08qV3Vz3kV71xu/1r3WgiKdF69IiIi5ZASIDez2zH/pwomZ57E7hj+6u+8OkVERMopJUBuZmeCPQFy6vyfU4fgxC9g8tDwl4iIVApKgNxMqSyBzxv+iu4KgTWcV6+IiEg5pQTIzewujSXwecNfLbX6S0REKgclQG7kVEYOf6ZlA9DEWSvAUg5A4m9gMkPTfs6pU0REpJxTAuRGdp+f/1O3mj+BPk46AiOv96d+dwio5pw6RUREyjklQG5kV2Ip7P+Td/aXhr9ERKQSUQLkRvJ6gJw2/+fkHkj6Azy8oGlf59QpIiLiBpQAuZHdiU4+AiNv+KvBDeBX1Tl1ioiIuAElQG4i12pjz595CZAThsAMQ8NfIiJSaSkBchOHUzLIybXh720mqqr/lVeYtBOS94DZB5r0ufL6RERE3IgSIDeRdwJ8k4gqeHg44QiMvOGvRr3A18mnyouIiJRzSoDcxC5nHoFhGPBH3tlft115fSIiIm5GCZCbcEyAdsYS+MTf4NRB8PSDxjdeeX0iIiJuRgmQm3AsgXdGD1De8FfjGPAJvPL6RERE3IwSIDeQmmnhRGoW4IQjMPINf2n1l4iIVE5KgNzArvMHoNau6keQr9eVVXb8FzgTD14B0CjGCdGJiIi4HyVAbsCpO0Dn9f40uRG8nbCcXkRExA0pAXIDf+0AfYXDXzYb/LHc/r2Gv0REpBJTAuQG/joE9Qp7gI79BGnHwLsKNOzphMhERETckxKgcs5qM9iTmLcH0BX2AOUNfzXtA16+VxiZiIiI+1ICVM4dSckgy2LD18uDutUCSl6Rhr9EREQclACVc3nzf5qEV8F8JUdgxG+Gs4ngG2w//V1ERKQSc3kC9M4771CvXj18fX1p374969evv2jZYcOGYTKZCtxatGjhKDNnzhy6du1K1apVqVq1Kj179uTHH38si7dSKnY5awWYY/irH3h6X2FUIiIi7s2lCdCSJUsYO3YskyZNYtu2bXTt2pWbbrqJ+Pj4QsvPmjWLhIQEx+3o0aOEhoYycOBAR5k1a9YwePBgvv/+ezZv3kydOnWIiYnh+PHjZfW2nCrvENQrmv9jzYWdn9u/19lfIiIirk2AXn/9dYYPH86IESNo1qwZM2fOJCoqitmzZxdaPjg4mIiICMft559/5vTp09x///2OMgsXLmTUqFFcddVVNG3alDlz5mCz2fjuu+/K6m051e5EJxyBcWQjZJwEv1Co381JkYmIiLgvT1e9cE5ODlu3bmXChAn5rsfExLBp06Yi1TF37lx69uxJ3bp1L1omMzMTi8VCaGjoRctkZ2eTnZ3tuJ+WZk86LBYLFoulSLEUJu+5Ja0jPcvCsdPnAGhY3a/E9Xjs+AQzYGvSF6sNsJX8PZVXV9rWUnRq67Kl9i47auuyU1ptXZz6XJYAJScnY7VaCQ8Pz3c9PDycxMTEyz4/ISGBFStWsGjRokuWmzBhArVq1aJnz4vvezN9+nSef/75AtdXrVqFv/+V75YcFxdXoucdSAPwJMTbYOP3JavDZOTSe8enmIHNZ2uSHBtbonrcRUnbWopPbV221N5lR21ddpzd1pmZmUUu67IEKI/JlH9lk2EYBa4VZv78+YSEhNC/f/+Llnn55Zf5+OOPWbNmDb6+F9/3ZuLEiYwbN85xPy0tjaioKGJiYggKKvnQk8ViIS4ujl69euHlVfwzvBZuiYc/dnNVdBh9+rQrUQymA6vx3H4Ww786Vw98DDxc/iMvFVfa1lJ0auuypfYuO2rrslNabZ03glMULvs0rF69OmazuUBvT1JSUoFeoQsZhsG8efMYOnQo3t6Fr2h69dVXmTZtGt9++y2tW7e+ZH0+Pj74+PgUuO7l5eWUH0xJ69l70p7JNq8ZXPI49nwBgKn5rXj5+JWsDjfirJ+ZXJ7aumypvcuO2rrsOLuti1OXyyZBe3t70759+wLdX3FxcXTu3PmSz127di379+9n+PDhhT7+yiuv8O9//5tvvvmGDh06OC3msuY4BLWkE6Bzc2DXl/bvW2rzQxERkTwuHQ8ZN24cQ4cOpUOHDnTq1In33nuP+Ph4Ro4cCdiHpo4fP86HH36Y73lz586lY8eOtGzZskCdL7/8MpMnT2bRokVER0c7epgCAwMJDAws/TflJDab4dgEsXlJl8Af/B6yUiEwAup0cmJ0IiIi7s2lCdCgQYNISUlh6tSpJCQk0LJlS2JjYx2ruhISEgrsCZSamsqyZcuYNWtWoXW+88475OTkcMcdd+S7/txzzzFlypRSeR+l4ejpTDJzrHh7ehBd0iMwfj+/+WGL/uBhdlpsIiIi7s7lM2JHjRrFqFGjCn1s/vz5Ba4FBwdfcpb34cOHnRSZa+VtgNg4PBBPcwlGKi1ZsOf8ii9tfigiIpKPy4/CkMI5NkAs6REYB76D7DQIqgW1r3ZiZCIiIu5PCVA5lXcGWLOSToB2DH/dBh76MYuIiPydPhnLqbwJ0M0iSjABOicT9qywf99Cq79EREQupASoHMrIzuVIin2eU5OSJED748CSASF1oFbJNlAUERGpyJQAlUN5vT81qvhQLbDgBo2X9ffhryLsqi0iIlLZKAEqh/ImQJdo/k/2Wdi70v69hr9EREQKpQSoHNp9fgl805JsgLj3G8g9B6H1IbKNkyMTERGpGJQAlUOOHqCSLIH/4zP7Vw1/iYiIXJQSoHLGMIyS9wBlpcG+82erafhLRETkopQAlTPHTp8jPTsXL7OJBmHFPLtszwqwZkP1xhDeonQCFBERqQCUAJUzeSvAGtaogldxj8D4I2/11wANf4mIiFyCEqByZnfeDtDF3f/n3BnY/539e539JSIicklKgMqZXXlngBV3/s/ur8FmgRrNoUbTUohMRESk4lACVM7kTYAu9h5Afx/+EhERkUtSAlSOnMuxciglAyjmKfCZp+DgGvv3Gv4SERG5LCVA5cjeP9MxDKge6E1YlWIcgbHrS7DlQkQrqN6w9AIUERGpIJQAlSO7Ekp4BIaGv0RERIpFCVA5krcEvmlxVoCdPQmH1tm/1/CXiIhIkSgBKkfyeoCKNf9n1+dg2KBmOwitV0qRiYiIVCxKgMoJwzD+SoCKswT+j+X2r+r9ERERKTIlQOVEQmoWaVm5eHqYaFijiEdgpCfC4Q3275UAiYiIFJkSoHIi7wT4BmGB+Hiai/aknZ8DBtS+GkKiSi84ERGRCkYJUDmxqyQnwP9+fvVXS63+EhERKQ4lQOVEsSdApx6Hoz8AJmh+a+kFJiIiUgEpASon8pbANytqD9DO5favdTpBUM3SCUpERKSC8nR1AJXKyT3wzcQCl62GweQzyeAF12yqBj8WIS9N3GH/quEvERGRYlMCVJay0uDAdwUum4FueTnPkWLU5+kLzW5xRmQiIiKVihKgshRaD257t8DlLYdSWPzjURrWCOTh64txlleN5lAl3IkBioiIVA5KgMpSQHVoc1eByyvjd/KZ7RAPNKgHbZq7IDAREZHKRZOgy4G8PYCKtQReRERESkwJkIv9/QiMZsU5A0xERERKTAmQiyWlZ3M604KHCRqFF/EIDBEREbkiSoBcLK/3p35YIL5eRTwCQ0RERK6IEiAXy9sAsWmE5v+IiIiUFSVALuaY/xOp+T8iIiJlRQmQi+1OKOYRGCIiInLFlAC5UHaulQMnzwLFOARVRERErpgSIBc6kJRBrs0gyNeTyGBfV4cjIiJSaSgBcqG8+T9NI4MwmUwujkZERKTyUALkQnk7QDfXBGgREZEypQTIhbQEXkRExDWUALnQrvMrwJqqB0hERKRMKQFykZPp2SSfzcZkgsY6AkNERKRMKQFykbz5P/WqBeDv7eniaERERCoXJUAustsx/KX5PyIiImVNCZCLOJbAawNEERGRMqcEyEV2aQWYiIiIyygBcgGL1cb+pLwzwNQDJCIiUtaUALnAwZMZWKwGgT6e1K7q5+pwREREKh0lQC7w1/yfKjoCQ0RExAWUALnArsS8M8A0/0dERMQVlAC5QN4SeM3/ERERcQ0lQC6QtwmilsCLiIi4hhKgMnYqI4c/07IBaKIl8CIiIi6hBKiM7T4/AbpuNX8CfXQEhoiIiCsoASpj2gBRRETE9VyeAL3zzjvUq1cPX19f2rdvz/r16y9adtiwYZhMpgK3Fi1a5Cu3bNkymjdvjo+PD82bN+ezzz4r7bdRZLt1BIaIiIjLuTQBWrJkCWPHjmXSpEls27aNrl27ctNNNxEfH19o+VmzZpGQkOC4HT16lNDQUAYOHOgos3nzZgYNGsTQoUP59ddfGTp0KHfeeSdbtmwpq7d1SXlL4JtpCbyIiIjLuDQBev311xk+fDgjRoygWbNmzJw5k6ioKGbPnl1o+eDgYCIiIhy3n3/+mdOnT3P//fc7ysycOZNevXoxceJEmjZtysSJE+nRowczZ84so3d1cblWG3v/PAtoCbyIiIgruWwWbk5ODlu3bmXChAn5rsfExLBp06Yi1TF37lx69uxJ3bp1Hdc2b97MY489lq9c7969L5kAZWdnk52d7biflmbvpbFYLFgsliLFUpi85+Z93Z90lpxcG/7eZiICva6obsnvwraW0qO2Lltq77Kjti47pdXWxanPZQlQcnIyVquV8PDwfNfDw8NJTEy87PMTEhJYsWIFixYtync9MTGx2HVOnz6d559/vsD1VatW4e/vf9lYLicuLg6AvakmAjw9CPPO5ZtvVlxxvVJQXltL6VNbly21d9lRW5cdZ7d1ZmZmkcu6fB32hWdhGYZRpPOx5s+fT0hICP3797/iOidOnMi4ceMc99PS0oiKiiImJoagoJIPVVksFuLi4ujVqxdeXl70AcYYBhk5Vi2Bd7IL21pKj9q6bKm9y47auuyUVlvnjeAUhcs+hatXr47ZbC7QM5OUlFSgB+dChmEwb948hg4dire3d77HIiIiil2nj48PPj4+Ba57eXk55QdzYT0XhCxO5KyfmVye2rpsqb3Ljtq67Di7rYtTl8smQXt7e9O+ffsC3V9xcXF07tz5ks9du3Yt+/fvZ/jw4QUe69SpU4E6V61addk6RUREpPJw6TjMuHHjGDp0KB06dKBTp0689957xMfHM3LkSMA+NHX8+HE+/PDDfM+bO3cuHTt2pGXLlgXqHDNmDNdddx0vvfQSt956K59//jnffvstGzZsKJP3JCIiIuWfSxOgQYMGkZKSwtSpU0lISKBly5bExsY6VnUlJCQU2BMoNTWVZcuWMWvWrELr7Ny5M4sXL+aZZ55h8uTJNGjQgCVLltCxY8dSfz8iIiLiHlw+E3fUqFGMGjWq0Mfmz59f4FpwcPBlZ3nfcccd3HHHHc4IT0RERCoglx+FISIiIlLWlACJiIhIpaMESERERCodJUAiIiJS6SgBEhERkUpHCZCIiIhUOkqAREREpNJRAiQiIiKVjhIgERERqXRcvhN0eWQYBgBpaWlXVI/FYiEzM5O0tDSdLFzK1NZlR21dttTeZUdtXXZKq63zPrfzPscvRQlQIdLT0wGIiopycSQiIiJSXOnp6QQHB1+yjMkoSppUydhsNk6cOEGVKlUwmUwlrictLY2oqCiOHj1KUFCQEyOUC6mty47aumypvcuO2rrslFZbG4ZBeno6NWvWxMPj0rN81ANUCA8PD2rXru20+oKCgvSPqYyorcuO2rpsqb3Ljtq67JRGW1+u5yePJkGLiIhIpaMESERERCodJUClyMfHh+eeew4fHx9Xh1Lhqa3Ljtq6bKm9y47auuyUh7bWJGgRERGpdNQDJCIiIpWOEiARERGpdJQAiYiISKWjBEhEREQqHSVApeidd96hXr16+Pr60r59e9avX+/qkNza9OnT+cc//kGVKlWoUaMG/fv3Z8+ePfnKGIbBlClTqFmzJn5+fnTv3p0//vjDRRFXHNOnT8dkMjF27FjHNbW1cx0/fpwhQ4ZQrVo1/P39ueqqq9i6davjcbW3c+Tm5vLMM89Qr149/Pz8qF+/PlOnTsVmsznKqK1LZt26dfTr14+aNWtiMplYvnx5vseL0q7Z2dk8+uijVK9enYCAAG655RaOHTtWOgEbUioWL15seHl5GXPmzDF27txpjBkzxggICDCOHDni6tDcVu/evY0PPvjA+P33343t27cbffv2NerUqWOcPXvWUWbGjBlGlSpVjGXLlhk7duwwBg0aZERGRhppaWkujNy9/fjjj0Z0dLTRunVrY8yYMY7ramvnOXXqlFG3bl1j2LBhxpYtW4xDhw4Z3377rbF//35HGbW3c7zwwgtGtWrVjK+++so4dOiQsXTpUiMwMNCYOXOmo4zaumRiY2ONSZMmGcuWLTMA47PPPsv3eFHadeTIkUatWrWMuLg445dffjGuv/56o02bNkZubq7T41UCVEquvvpqY+TIkfmuNW3a1JgwYYKLIqp4kpKSDMBYu3atYRiGYbPZjIiICGPGjBmOMllZWUZwcLDx3//+11VhurX09HSjUaNGRlxcnNGtWzdHAqS2dq7x48cbXbp0uejjam/n6du3r/HAAw/kuzZgwABjyJAhhmGorZ3lwgSoKO165swZw8vLy1i8eLGjzPHjxw0PDw/jm2++cXqMGgIrBTk5OWzdupWYmJh812NiYti0aZOLoqp4UlNTAQgNDQXg0KFDJCYm5mt3Hx8funXrpnYvoYcffpi+ffvSs2fPfNfV1s71xRdf0KFDBwYOHEiNGjVo27Ytc+bMcTyu9naeLl268N1337F3714Afv31VzZs2ECfPn0AtXVpKUq7bt26FYvFkq9MzZo1admyZam0vQ5DLQXJyclYrVbCw8PzXQ8PDycxMdFFUVUshmEwbtw4unTpQsuWLQEcbVtYux85cqTMY3R3ixcv5pdffuGnn34q8Jja2rkOHjzI7NmzGTduHE8//TQ//vgjo0ePxsfHh3vvvVft7UTjx48nNTWVpk2bYjabsVqtvPjiiwwePBjQ73ZpKUq7JiYm4u3tTdWqVQuUKY3PTiVApchkMuW7bxhGgWtSMo888gi//fYbGzZsKPCY2v3KHT16lDFjxrBq1Sp8fX0vWk5t7Rw2m40OHTowbdo0ANq2bcsff/zB7Nmzuffeex3l1N5XbsmSJSxYsIBFixbRokULtm/fztixY6lZsyb33Xefo5zaunSUpF1Lq+01BFYKqlevjtlsLpCxJiUlFch+pfgeffRRvvjiC77//ntq167tuB4REQGgdneCrVu3kpSURPv27fH09MTT05O1a9fy5ptv4unp6WhPtbVzREZG0rx583zXmjVrRnx8PKDfbWd68sknmTBhAnfddRetWrVi6NChPPbYY0yfPh1QW5eWorRrREQEOTk5nD59+qJlnEkJUCnw9vamffv2xMXF5bseFxdH586dXRSV+zMMg0ceeYRPP/2U1atXU69evXyP16tXj4iIiHztnpOTw9q1a9XuxdSjRw927NjB9u3bHbcOHTpwzz33sH37durXr6+2dqJrr722wJYOe/fupW7duoB+t50pMzMTD4/8H31ms9mxDF5tXTqK0q7t27fHy8srX5mEhAR+//330ml7p0+rFsMw/loGP3fuXGPnzp3G2LFjjYCAAOPw4cOuDs1tPfTQQ0ZwcLCxZs0aIyEhwXHLzMx0lJkxY4YRHBxsfPrpp8aOHTuMwYMHa/mqk/x9FZhhqK2d6ccffzQ8PT2NF1980di3b5+xcOFCw9/f31iwYIGjjNrbOe677z6jVq1ajmXwn376qVG9enXjqaeecpRRW5dMenq6sW3bNmPbtm0GYLz++uvGtm3bHNu/FKVdR44cadSuXdv49ttvjV9++cW44YYbtAzeHf3nP/8x6tata3h7exvt2rVzLNeWkgEKvX3wwQeOMjabzXjuueeMiIgIw8fHx7juuuuMHTt2uC7oCuTCBEht7Vxffvml0bJlS8PHx8do2rSp8d577+V7XO3tHGlpacaYMWOMOnXqGL6+vkb9+vWNSZMmGdnZ2Y4yauuS+f777wv9G33fffcZhlG0dj137pzxyCOPGKGhoYafn59x8803G/Hx8aUSr8kwDMP5/UoiIiIi5ZfmAImIiEilowRIREREKh0lQCIiIlLpKAESERGRSkcJkIiIiFQ6SoBERESk0lECJCIiIpWOEiARERGpdJQAiYhD9+7dGTt27CXLmEwmli9fftHHDx8+jMlkYvv27Rcts2bNGkwmE2fOnClRnEVVlPdTHpVV+4hUZp6uDkBE3EtCQgJVq1Z1dRgVWufOnUlISCA4ONjVoYhUWEqARKRYIiIiXB2CW7FYLHh5eRXrOd7e3mpnkVKmITARycdms/HUU08RGhpKREQEU6ZMyff4hUNgP/74I23btsXX15cOHTqwbdu2AnXGxsbSuHFj/Pz8uP766zl8+HCBMps2beK6667Dz8+PqKgoRo8eTUZGhuPx6Ohopk2bxgMPPECVKlWoU6cO7733XrHe24IFC+jQoQNVqlQhIiKCu+++m6SkJAAMw6Bhw4a8+uqr+Z7z+++/4+HhwYEDBwBITU3lwQcfpEaNGgQFBXHDDTfw66+/OspPmTKFq666innz5lG/fn18fHwo7MjFI0eO0K9fP6pWrUpAQAAtWrQgNjYWKDgE1r17d0wmU4FbXjteLiYRKUgJkIjk87///Y+AgAC2bNnCyy+/zNSpU4mLiyu0bEZGBjfffDNNmjRh69atTJkyhSeeeCJfmaNHjzJgwAD69OnD9u3bGTFiBBMmTMhXZseOHfTu3ZsBAwbw22+/sWTJEjZs2MAjjzySr9xrr73mSLJGjRrFQw89xO7du4v83nJycvj3v//Nr7/+yvLlyzl06BDDhg0D7IndAw88wAcffJDvOfPmzaNr1640aNAAwzDo27cviYmJxMbGsnXrVtq1a0ePHj04deqU4zn79+/n//7v/1i2bNlF50I9/PDDZGdns27dOnbs2MFLL71EYGBgoWU//fRTEhISHLcBAwbQpEkTwsPDixyTiFygVM6YFxG31K1bN6NLly75rv3jH/8wxo8f77gPGJ999plhGIbx7rvvGqGhoUZGRobj8dmzZxuAsW3bNsMwDGPixIlGs2bNDJvN5igzfvx4AzBOnz5tGIZhDB061HjwwQfzve769esNDw8P49y5c4ZhGEbdunWNIUOGOB632WxGjRo1jNmzZ1/y/YwZM+aij//4448GYKSnpxuGYRgnTpwwzGazsWXLFsMwDCMnJ8cICwsz5s+fbxiGYXz33XdGUFCQkZWVla+eBg0aGO+++65hGIbx3HPPGV5eXkZSUtJFX9cwDKNVq1bGlClTCn3s+++/z9c+f/f6668bISEhxp49e4ock4gUpDlAIpJP69at892PjIx0DBNdaNeuXbRp0wZ/f3/HtU6dOhUoc80112AymS5aZuvWrezfv5+FCxc6rhmGgc1m49ChQzRr1qxAbCaTiYiIiIvGVpht27YxZcoUtm/fzqlTp7DZbADEx8fTvHlzIiMj6du3L/PmzePqq6/mq6++Iisri4EDBzriPHv2LNWqVctX77lz5xxDZAB169YlLCzskrGMHj2ahx56iFWrVtGzZ09uv/32Am1/oRUrVjBhwgS+/PJLGjduXKyYRCQ/JUAiks+FE3ZNJpMjUbiQUcjclpKUsdls/Otf/2L06NEFHqtTp06JYrtQRkYGMTExxMTEsGDBAsLCwoiPj6d3797k5OQ4yo0YMYKhQ4fyxhtv8MEHHzBo0CBHgmez2YiMjGTNmjUF6g8JCXF8HxAQcNl4RowYQe/evfn6669ZtWoV06dP57XXXuPRRx8ttPzOnTu56667mDFjBjExMY7rRY1JRPJTAiQiJda8eXM++ugjzp07h5+fHwA//PBDgTIX7ht0YZl27drxxx9/0LBhw1KLdffu3SQnJzNjxgyioqIA+PnnnwuU69OnDwEBAcyePZsVK1awbt26fHEmJibi6elJdHT0FccUFRXFyJEjGTlyJBMnTmTOnDmFJkApKSn069ePAQMG8Nhjj+V7zNkxiVQWmgQtIiV299134+HhwfDhw9m5cyexsbEFVlGNHDmSAwcOMG7cOPbs2cOiRYuYP39+vjLjx49n8+bNPPzww2zfvp19+/bxxRdfXLQ3pCTq1KmDt7c3b731FgcPHuSLL77g3//+d4FyZrOZYcOGMXHiRBo2bJhvuK5nz5506tSJ/v37s3LlSg4fPsymTZt45plnCk2mLmXs2LGsXLmSQ4cO8csvv7B69WrHUN+FBgwYgJ+fH1OmTCExMdFxs1qtTo1JpDJRAiQiJRYYGMiXX37Jzp07adu2LZMmTeKll17KV6ZOnTosW7aML7/8kjZt2vDf//6XadOm5SvTunVr1q5dy759++jatStt27Zl8uTJREZGOi3WsLAw5s+fz9KlS2nevDkzZswokKzlGT58ODk5OTzwwAP5rptMJmJjY7nuuut44IEHaNy4MXfddReHDx8mPDy8WPFYrVYefvhhmjVrxo033kiTJk145513Ci27bt06/vjjD6Kjo4mMjHTcjh496tSYRCoTk1GUAXoRkUpk48aNdO/enWPHjimJEKmglACJiJyXnZ3N0aNHefDBB4mMjMy3Kk1EKhYNgYmInPfxxx/TpEkTUlNTefnll10djoiUIvUAiYiISKWjHiARERGpdJQAiYiISKWjBEhEREQqHSVAIiIiUukoARIREZFKRwmQiIiIVDpKgERERKTSUQIkIiIilc7/Az5nUVoH3uKJAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.688 0.726 0.766 0.8   0.796]\n",
      "[[0.89 0.92 0.95 0.96 0.96]\n",
      " [0.72 0.77 0.82 0.85 0.84]\n",
      " [0.62 0.63 0.67 0.73 0.73]\n",
      " [0.46 0.53 0.62 0.66 0.64]\n",
      " [0.75 0.78 0.77 0.8  0.81]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGwCAYAAABB4NqyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABmtElEQVR4nO3deVxU5eLH8c8w7KsiCJi4525pevOqaZtiapZZXaubZWlltmheK72230rbTO+ipVezbvXLe7Nss5IWtzRN0zI1DTdMQcQFEASGmfP748jICCjLMAPM9/16nVeeM2cOzzyQfH1Wi2EYBiIiIiI+xM/bBRARERHxNAUgERER8TkKQCIiIuJzFIBERETE5ygAiYiIiM9RABIRERGfowAkIiIiPsff2wWojRwOBwcPHiQiIgKLxeLt4oiIiEgFGIZBTk4OTZo0wc/v7G08CkBlOHjwIImJid4uhoiIiFTB/v37adq06VnvUQAqQ0REBGBWYGRkZJWfY7PZWLZsGUlJSQQEBLireFIG1bXnqK49S/XtOaprz6mpus7OziYxMdH5e/xsFIDKUNztFRkZWe0AFBoaSmRkpP5nqmGqa89RXXuW6ttzVNeeU9N1XZHhKxoELSIiIj5HAUhERER8jgKQiIiI+ByNAaoGu92OzWYr93WbzYa/vz/5+fnY7XYPlsx3BAQEYLVavV0MERGpYxSAqsAwDNLT0zl+/Pg574uPj2f//v1aT6gGNWjQgEaNGnm7GCIiUocoAFVBcfhp3LgxoaGh5YYbh8PBiRMnCA8PP+eCTFJ5hmGQl5dHRkaGWthERKRSFIAqyW63O8PPuVodHA4HhYWFBAcHKwDVkJCQEAAOHTqkVjYREakw/VaupOIxP6GhoV4uiRQr/l5oLJCIiFSUAlAVqbWh9tD3QkREKksBSERERHyO1wPQ7NmzadmyJcHBwXTv3p1Vq1ad9f533nmHCy+8kNDQUBISErjjjjs4cuSIyz2LFy+mY8eOBAUF0bFjRz788MOa/AgiIiJSx3g1AC1atIgJEyYwdepUNm3aRN++fRk0aBCpqall3r969Wpuu+02Ro8ezdatW/nf//7HDz/8wJgxY5z3rF27lhEjRjBy5Eh++uknRo4cyZ/+9CfWrVvnqY/lM1q0aMHMmTOr9Yy8vDyuv/56IiMjsVgs51xaQERExB28GoBmzJjB6NGjGTNmDB06dGDmzJkkJiYyZ86cMu///vvvadGiBQ8++CAtW7bkkksu4Z577mHDhg3Oe2bOnMmAAQOYMmUK7du3Z8qUKVx55ZXV/kVdH1x22WVMmDDBbc/74YcfuPvuu6v1jDfffJNVq1axZs0a0tLSiIqK4oMPPmDgwIHExMRgsVjYvHmzewosPq2gyE5Gdj55hUUYhuHt4oiIl3ltGnxhYSEbN25k8uTJLteTkpJYs2ZNme/p3bs3U6dOZenSpQwaNIiMjAzef/99hgwZ4rxn7dq1PPTQQy7vGzhw4FkDUEFBAQUFBc7z7OxswJzxdeZKzzabDcMwcDgcOByOs37G4r9ki++vDc5VFsMwsNvt+Puf+0ejeBmA6ny2lJQUOnToQMeOHZ1fPycnh969e3P99ddzzz33nLOuHQ6Hs67PtjK3uEdxHdeVui4scvDO+v38a/kusk4WAeBngbAgf8ICrYQH+RMW5H/qvyXPrYQF+hMe7E94ufdZCQmw1uhA/LpW33WZ6tpzaqquK/M8rwWgzMxM7HY7cXFxLtfj4uJIT08v8z29e/fmnXfeYcSIEeTn51NUVMQ111zDP/7xD+c96enplXomwLRp03j66adLXV+2bFmp6e7+/v7Ex8dz4sQJCgsLAfOXdr6t/F/QJ48cL/e16goO8KvQX77jxo1jxYoVrFixgr///e8A/PTTT6SmpjJ06FDef/99nn32WbZu3crixYtp2rQpU6dOZcOGDeTl5dG2bVueeOIJLrvsMuczL7jgAu69917uvfdeABo2bMisWbNYtmwZ33zzDQkJCfztb39j8ODBZZbp6quv5rvvvgPMKex9+vTh008/5dprrwVwdoXm5uY6Q2lZCgsLyc/PByA5OfmcdSHuUdvr2jDg56MWPk71IzPf9f8RhwE5+UXk5BcBBWU/oIIsGARbIcgKwaeOIKvhcl587cz7gkvcF2SFQD8o73/n2l7f9Ynq2nPcXdd5eXkVvtfrCyGe+cvbMIxyf6Fv27aNBx98kCeeeIKBAweSlpbGww8/zNixY5k/f36VngkwZcoUJk6c6DzPzs4mMTGRpKQkIiMjXe7Nz89n//79hIeHExwcDEBeYRHdXvDO/zC/PDWA0MBzfxv/9a9/sXfvXjp16uQMe7GxsWRmZgLwzDPP8OKLL9KqVSsaNGjA77//ztChQ5k2bRrBwcG89dZb3HzzzWzfvp1mzZoB4OfnR3BwsEsdvfTSS0yfPp0ZM2bwz3/+k3vuuYc9e/YQHR1dqkxLlixhypQpbN26lffff5/AwECXZ4WHhwMQFhZW6vtQUn5+vvN7MWDAAAICAs5ZH1J1NpuN5OTkWl3XWw5k8fznO9iw7zgAMeGBTLiyDdd3a0Kh3UFugZ0TBUUl/lvEiYIiThTaOZFfRG5hEScK7M7rZ96XW2i+5jDAwMJJO5x0WYy8ai1CxS1T4cWtU8H+hAb4kXv8CC0TmxAZEnhGK9Wp1qlg/1OtVadarYL8K/yPIzmtLvxs1xc1Vddn+8fymbwWgGJiYrBaraVaZjIyMkq14BSbNm0affr04eGHHwbMFoiwsDD69u3Ls88+S0JCAvHx8ZV6JkBQUBBBQUGlrgcEBJT6xtjtdiwWC35+fs7Vnb25ynPJcpxNw4YNCQwMJCwsjCZNmri8H8wANHDgQOf12NhYunXr5jx/7rnnWLJkCZ9++in333+/83pxXRQbNWoUf/7znwHz+/XPf/6TDRs2cNVVV5UqU0xMDGFhYQQGBrqU6cyynesz+vmd/ou+rO+Z1IzaWNcHj5/kpS938OGmA4DZQnpX31bcc2lrwoPMv+5CgKiw6n8twzA4aTOD0Yl8MyTlFNhOhSUbJwpOhanicFV8X6HZ8uRyvaAIwzizZaokP346Wn4rdln8LJgBKcjsxnMGJmc3nj8R5Vw//R4rEUEBPhemauPPdn3l7rquzLO8FoACAwPp3r07ycnJXHfddc7rycnJzi6QM+Xl5ZUam1K8+m/xGJBevXqRnJzsMg5o2bJl9O7d290fwSkkwMq2ZwaWuu5wOMjJziEiMqLGQlJIgHtWP+7Ro4fLeW5uLk8//TSffvopBw8epKioiJMnT5Y7Q6/YBRdc4PxzWFgYERERZGRkuKWMIuU5UVDEa8t3MW/VbgqKzO7o4d3OY9LAdjRpEFIjX9NisRAa6E9ooD+NI6r3LGeYyncNRSfyi8jOK+D7H3+iRZv2nCwyW6/KClC5p+4/UXg6TGXnF5GdXwRZ1Suf1c/iHC9V2TB15nVfC1NSe3m1C2zixImMHDmSHj160KtXL+bOnUtqaipjx44FzK6pAwcO8NZbbwEwdOhQ7rrrLubMmePsApswYQIXX3yxswVh/Pjx9OvXjxdeeIFrr72Wjz76iK+++orVq1fX2Oco/ovwTA6Hg6JAK6GB/rV+L7CwMNd/Ej/88MN8+eWXvPzyy7Rp04aQkBBuuOEG57in8pyZvi0WS60ZAC71T5HdwX83/M6M5J1knjDH8vRsGc1jQzrSpWmUl0tXcS5h6ozXbDYbAQc3M7hfywr969YwDPJOddHllAxGJYKS63X7OcOU3WG4PUxFBAe4dOdFOLvxyghQ5VwP8leYkqrzagAaMWIER44c4ZlnniEtLY3OnTuzdOlSmjdvDkBaWppLi8OoUaPIycnhn//8J3/5y19o0KABV1xxBS+88ILznt69e/Pee+/x2GOP8fjjj9O6dWsWLVpEz549Pf75apvAwMAK75q+atUqRo0a5WydO3HiBHv37q3B0olUzoqdh3nus23sPHQCgJYxYUwe1J6kjnE+/UvRYrGYM9yCSoepynI4zJapyoSpE6e6AUtezy0oJ0xVk9XPckaLk7VaYUp8i9cHQY8bN45x48aV+drChQtLXXvggQd44IEHzvrMG264gRtuuMEdxatXWrRowbp169i7dy/h4eFlDkwu1qZNGz744AOGDh2KxWLh8ccf91hLztGjR0lNTeXgwYMA7NixA4D4+Hji4+M9UgapvXak5/Dc0u2s3HkYgAahATx4xfnc+sfmBOqXmFv5+bk/TLmMhyoZoM563c6JfNvpweglwlTWSRtZJ6s/ldoMU1b87Fb+tWuNGZKCAwgv2UpVPEYquIwxUyWuq2WqbvB6ABLPmTRpErfffjsdO3bk5MmT7Nmzp9x7X331Ve6880569+5NTEwMjz76aKVG11fHxx9/zB133OE8v+mmmwB48skneeqppzxSBql9DucUMCN5J4t+SMVhQIDVwu29WvDAFecTFaoBq7VdyTBV/pSUinE4DPJsJbru3BamigALxzJOVKt8/qc+65mD0COca0idClZnjI9SmPIsBSAf0rZtW9auXetyrUWLFmWuituiRQu++eYbl2v33Xefy/mZXWJlPedcW1uUtUDlqFGjGDVq1FnfJ74j32bn36t2M2f5LnILzS7cQZ3jmTyoPc0buWE6l9Q5fiW6vtwZpo6dyGfZtyu4sHtP8u2UP2uveKkEl+5AM1ABFLmxZaqqYaqsQegKU64UgESkVnI4DD766QAvfrGDtCxzocsLm0bx2NUd+UOL8rtvRSqjZJiKDrHSPBx6t25UpanZxWGq1Dip8mbteSFMFa8ZVe4SCMH+p7r9zEHq5Y2bqg9hSgFIRGqddbuP8NzS7fz8uznl6LwGITxyVTuGXtAEP7+6/Zeu1F8lw1R1VTpMnWVwenHLaZHD4HiejeN51Q9TAdZTXZpVDFNBVsj18o4jCkAiUmvsycxl+ufb+XLrIcBcyG/c5a25s09Lgt205pVIXeDuMJVbWNyydHqRzlKtURUYT1Ucpmz26oepxDArN5a97J9HKACJiNcdzyvk71+n8J/v92KzG/hZ4OaLm/HQgLbEhJdepV1EKs7Pz0JEcAARwQFAcLWeVRymTgenqoepYKt3m4AUgETEawqLHLy1di//+CbFOcbhsnax/HVwB9rGVXN5ZRFxO9cwVXU2m43PPlvqplJVjQKQiHicYRh8uTWd6Z//yt4j5u7N7eMjmDqkA33Pj/Vy6UTEE7w9hloBSEQ86qf9x3nus+2s33sUgNiIIP4yoC039kjEqgHOIuIhCkAi4hEHjp/kpS9+Zclmc4Xv4AA/7j61U3uYGwZ6iohUhv7WEY8wDIN77rmH999/n2PHjrFp0ya6du3q7WKJB+Tk25izfBfzV+85vVP7Refx8MB2JETVzE7tIiLnoo1zfMhll13GhAkT3PrMUaNGMWzYsHPe98UXX7Bw4UI+/fRT58a3K1euZOjQoTRp0gSLxcKSJUvcWjbxriK7g3fW7ePyl5cze/kuCooc9GwZzSf3X8KMP3VV+BERr1ILkHjErl27SEhIoHfv3s5rubm5XHjhhdxxxx1cf/31XiyduNvyHRk8v3S7y07tUwa1Z4CP79QuIrWHWoB8xKhRo1ixYgWzZs3CYrFgsVice3lt27aNwYMHEx4eTlxcHCNHjiQzM9P53vfff58uXboQEhJCo0aN6N+/P7m5uTz11FO8+eabfPTRR85nLl++vMyv/cADD5CamorFYqFFixYADBo0iGeffZbhw4d7oAbEE35Nz2bk/HWMeuMHdh46QYPQAJ4c2pEvJ/QjqVO8wo+I1BpqAXIHwwBbXunrDod5vdAKfjWUNQNCKzSXcNasWezcuZPOnTvzzDPPABAbG0taWhqXXnopd911FzNmzODkyZM8+uij/OlPf+Kbb74hLS2Nm2++mRdffJHrrruOnJwcVq1ahWEYTJo0ie3bt5Odnc0bb7wBQHR06T2aZs2aRevWrZk7dy4//PADVqtW9K1vMnLyeTV5J4t+2O/cqX1U7xbcf7l2aheR2kkByB1sefB8k1KX/YAGNf21/3oQAs+9I3ZUVBSBgYGEhoYSHx/vvD5nzhwuuuginn/+eee1BQsWkJiYyM6dOzlx4gRFRUUMHz6c5s2bA9ClSxfnvSEhIRQUFLg8s6yvHRERgdVqPet9UvecLLQzf7XrTu2Du8Tz6FXaqV1EajcFIB+3ceNGvv32W8LDw0u9tmvXLpKSkrjyyivp0qULAwcOJCkpiRtuuIGGDRt6obRSWzgMWLL5IDO+Sjm9U3tiAx4f0oEe2qldROoABSB3CAg1W2LO4HA4yM7JITIiAr+a7AKrBofDwdChQ3nhhRdKvZaQkIDVaiU5OZk1a9awbNky/vGPfzB16lTWrVtHy5Ytq/W1pW5at+coM7ZY2f/9L4B2aheRukkByB0slrK7oRwOCLCbr9VUAKqEwMBA7Ha7y7WLLrqIxYsX06JFC/z9y/5xsFgs9OnThz59+vDEE0/QvHlzPvzwQyZOnFjmM6V+2pOZy7Sl21m27RBgISzIyn2Xt9FO7SJSJykA+ZAWLVqwbt069u7dS3h4ONHR0dx3333MmzePm2++mYcffpiYmBhSUlJ47733mDdvHhs2bODrr78mKSmJxo0bs27dOg4fPkyHDh2cz/zyyy/ZsWMHjRo1IioqioCAig16PXHiBCkpKc7zPXv2sHnzZqKjo2nWrFmN1IFU3vG8QmZ9/Rv/WbuPIoeB1c/CH2PtvDLqUuIblu46FRGpC7zfLCEeM2nSJKxWKx07diQ2NpbU1FSaNGnCd999h91uZ+DAgXTu3Jnx48cTFRWFn58fkZGRrFy5ksGDB9O2bVsee+wxXnnlFQYNGgTAXXfdRbt27ejRowexsbF89913FS7Phg0b6NatG926dQNg4sSJdOvWjSeeeKJGPr9UTkGRnX+v2k2/F7/lje/2UuQwuLxdLJ/c14s/tXLQKDzI20UUEakytQD5kLZt27J27dpS188//3w++OCDMt/ToUMHvvjii3KfGRsby7Jly875tSdMmFBqFerLLrsMwzDO+V7xLMMw+OKXdKZ9/iupR0vv1G6z2fjNy2UUEakuBSARcdq8/zjPfbaNH/YeA8yd2iclteWG7tqpXUTqFwUgEeHA8ZO8+MWvfKSd2kXER+hvNhEflpNvY/apndoLixxYLDC8W1MmDWyrzUpFpF5TABLxQUV2B+/9sJ9Xk3dyJLcQgD+2iuaxIR3pfF6Ul0snIlLzFICqSIN3aw99LyrOMAyW7zzM859t57cMc6f2VjFhTBncgf4dGmuzUhHxGQpAlVS8xk1eXh4hIeoiqA3y8syZSlqQ8ex+Tc/muc+2s+q3TAAahgYw/srz+fMfmxNg1YoYIuJbFIAqyWq10qBBAzIyMgAIDQ0t91/NDoeDwsJC8vPza24rDB9mGAZ5eXlkZGQQGRmplqByZOTkM2PZTv67wdypPdDqx6g+Lbjv8jZEhWindhHxTQpAVVC8o3lxCCqPYRicPHmSkJAQdS3UoAYNGtCoUSNvF6PWOVloZ96q3by2Yhd5p3ZqH9IlgUevak+zRtXbQ05EpK5TAKoCi8VCQkICjRs3xmazlXufzWZj5cqV9OvXr8LbQ0jlBAQEYLVaz/p98DUOh8GHmw7w0pc7SM82d2rvmtiAx7RTu4iIkwJQNVitVqzW8jeBtFqtFBUVERwcrAAkHvH97iM8+9k2fjmQDZg7tT86qD1DL0hQK6SISAkKQCL1wO7DJ5j2+a8kbzsEQESQP+Mub8MdfVpop3YRkTIoAInUYcdyzZ3a3/7+9E7tt1zcjAn9z9dmpSIiZ6EAJFIHFRTZeWvNPv7xzW9k5xcBcEX7xvx1cHvaNI7wculERGo/BSCROsQwDD7/JZ3pZ+zU/tiQjlxyfoyXSyciUncoAInUEZtSj/HcZ9vZsM/cqb1xRBCTktpxffem2qldRKSSFIBEarnfj+Xx4hc7+PinEju192vNPf1aaad2EZEq0t+eIrVUWTu1X39RUyYltSM+KtjbxRMRqdMUgERqmbJ2au/VqhFTh3TQTu0iIm6iACRSSxiGwfIdh3l+qetO7X8d3IErtVO7iIhbKQCJ1ALb08yd2lennN6pfUL/ttzSs5l2ahcRqQEKQCJelJGdzyvLdvLfjfsxtFO7iIjHKACJeIF2ahcR8S4FIBEPcjgMPth0gJdL7NTerZm5U3v35tqpXUTEUxSARDxkza5MnvtsO1sPnt6pffKg9lytndpFRDxOAUikhu06fIJpS3/lq+2nd2q/74o2jOqtndpFRLxFAUikhmindhGR2ksBSMTNCorsvLlmL//4JoWcUzu1X9m+MVO0U7uISK3h9QVGZs+eTcuWLQkODqZ79+6sWrWq3HtHjRqFxWIpdXTq1MnlvpkzZ9KuXTtCQkJITEzkoYceIj8/v6Y/ivg4wzD47Oc0+s9YwfNLfyUnv4gOCZG8M6Yn80f9QeFHRKQW8WoL0KJFi5gwYQKzZ8+mT58+vP766wwaNIht27bRrFmzUvfPmjWL6dOnO8+Lioq48MILufHGG53X3nnnHSZPnsyCBQvo3bs3O3fuZNSoUQC8+uqrNf6ZxDdtSj3Gs59tZ2PJndoHtuP6i7RTu4hIbeTVADRjxgxGjx7NmDFjALPl5ssvv2TOnDlMmzat1P1RUVFERZ3eC2nJkiUcO3aMO+64w3lt7dq19OnTh1tuuQWAFi1acPPNN7N+/foa/jTii/YfzePFL3fwyamd2kMCrNzdrxV3a6d2EZFazWt/QxcWFrJx40YmT57scj0pKYk1a9ZU6Bnz58+nf//+NG/e3Hntkksu4e2332b9+vVcfPHF7N69m6VLl3L77beX+5yCggIKCgqc59nZ5jRlm82GzWarzMdyUfze6jxDKsbTdZ2Tb+O1lXtYuDbVuVP7dV2b8FD/NsRHBgNGvf2+6+fas1TfnqO69pyaquvKPM9rASgzMxO73U5cXJzL9bi4ONLT08/5/rS0ND7//HPeffddl+s33XQThw8f5pJLLsEwDIqKirj33ntLBa2Spk2bxtNPP13q+rJlywgNrf6qvMnJydV+hlRMTde13YA1hyx8vt+P3CKza+v8SAfDWjhoGpzKj6tTa/Tr1yb6ufYs1bfnqK49x911nZeXV+F7vd5Gf+YCcIZhVGhRuIULF9KgQQOGDRvmcn358uU899xzzJ49m549e5KSksL48eNJSEjg8ccfL/NZU6ZMYeLEic7z7OxsEhMTSUpKIjIysvIf6hSbzUZycjIDBgwgIED7OtWkmq5rwzBYvjOTF77cya7DuQC0ignlkYFtuaJdrE8tZKifa89SfXuO6tpzaqqui3twKsJrASgmJgar1VqqtScjI6NUq9CZDMNgwYIFjBw5ksDAQJfXHn/8cUaOHOkcV9SlSxdyc3O5++67mTp1Kn5+pSe+BQUFERRUel2WgIAAt3xj3PUcObeaqOttB7N5fqnrTu0PDWjLzRf79k7t+rn2LNW356iuPcfddV2ZZ3ktAAUGBtK9e3eSk5O57rrrnNeTk5O59tprz/reFStWkJKSwujRo0u9lpeXVyrkWK1WDMPAMAz3FF58wqHsfF5ZtoP/bfzduVP7HX1aME47tYuI1Hle7QKbOHEiI0eOpEePHvTq1Yu5c+eSmprK2LFjAbNr6sCBA7z11lsu75s/fz49e/akc+fOpZ45dOhQZsyYQbdu3ZxdYI8//jjXXHMNVqu2HZBzyyssYt7KPby2YhcnbeZO7VdfYO7UnhitndpFROoDrwagESNGcOTIEZ555hnS0tLo3LkzS5cudc7qSktLIzXVdVBpVlYWixcvZtasWWU+87HHHsNisfDYY49x4MABYmNjGTp0KM8991yNfx6p24p3an/py185lG3OCjR3au9I9+YNvVw6ERFxJ68Pgh43bhzjxo0r87WFCxeWuhYVFXXWUd7+/v48+eSTPPnkk+4qoviAM3dqb9owhEev0k7tIiL1ldcDkIg3mTu1b+er7RmAuVP7/Ve04Xbt1C4iUq8pAIlPOppbyKyvdvLOulTnTu1/7tmM8Vdqp3YREV+gACQ+payd2vt3aMzkQR1o0zjcy6UTERFPUQASn2AYBku3pDP9i+3sP3oSgI4JkTw2pAO928R4uXQiIuJpCkBS7/2YeozntFO7iIiUoAAk9db+o3m88MWvfPpzGmDu1H7PpeZO7aGB+tEXEfFl+i0g9U52vo1/fZvCG9/tde7UfsNFTZk0sB1xkcHeLp6IiNQCCkBSb9gd8Pa6VP7x7W6O5hYC0Lt1I6YO6UCnJlFeLp2IiNQmCkBS5xmGwTc7DvPCz1YOnfwVgNaxYfx1cAeuaN9YCxmKiEgpCkBSp207mM1zS7fxXcoRwELD0AAmDmjLTT6+U7uIiJydApDUSYey83n5yx28/6O5U3uA1UK/ODsv3XEJ0RHasFRERM5OAUjqlLzCIuau3M3rK3a77NQ+sX9rtqxdTkRwgJdLKCIidYECkNQJdofB4h9/55VlO5w7tV/UrAGPXd2Ri5o1xGazscXLZRQRkbpDAUhqvTUpmTz72Xa2pZk7tSdGmzu1D+mindpFRKRqFICk1krJOMH0z0vs1B7szwOndmoP8tdO7SIiUnUKQFLrHDlRwKyvf+OddanYT+3UfmvPZozv35bosEBvF0/qKocdbHkQFOHtkohILaAAJLVGvs3cqf2f36SQU6Cd2sWNfkuGj+6DE4cgPB5i20JMW4hpBzHnQ2w7iEgAdamK+AwFIPE6wzD49Oc0XvjiV34/VmKn9qs70Lu1dmqXaigqgK+egu9nn752It089qx0vTcwwgxDMW1dA1J0S7BqdqFIfaMAJF61cd8xnvtsGz+mHgcgLjKISUntGK6d2qW6Mn+D9++A9FPzAy++B/o9DMf3QeZOOLzDvCdzBxzdA4U5cPBH8yjJzx+iW50KROefajU69efgSM9/LhFxCwUg8Yr9R/OY/sWvfFZip/axl7bmrn4ttVO7VI9hwKa34fNHzDE/IdEwbDa0G2S+Hh4LTXu4vqeoEI7uNoNR5qlgVByQbLmnru8s/bUiEk6FobZmN1pxQIqIV3eaSC2n3zTiUVknbcwu3qndbu7UfmP3pvwlSTu1ixucPA6fToCtH5rnLS+F616HyISzv88/EBq3N4+SDAOyD5wKQMWh6FQYOnEIctLMY88K1/cFRZ7uTivZaqTuNJFaQwFIPMJmd/B/61N5NXknx/JsAPRp04ipgzvSsYm6EcQNUtfB4jGQlWp2W13xGPQeD37V2BPOYoGopubR+grX104eP9WFdkar0bE9UJANBzaaR0ku3WklW43aanaaiIcpAEmNMgyDb37N4Lml29l9OBcwd2qfOqQDl7fTTu3iBg47rJoBy6eBYYcGzeGGBaW7udwtpAEk/sE8SioqON2ddnina0Cy5Z2lO61JicHXJY7gRjX7OUR8lAKQ1JitB7N47rPtrNl1BIDosEAe6n++dmoX98k6AB/cDftWm+dd/gRDXvHu4GT/IGjcwTxKcjgg56Dr4OviVqPcDPO1nIOwe/kZj4ugnzUWq32p2WIUe6o7rWFLsOqvcJGq0v894nbpWfm8vGwHi0/t1B7o78edfVoy7vLWRGqzUnGX7Z/Cx/fDyWMQGG4Gnwtv8napyufnd7o7rc2Vrq+dPHa6O61kQDq2F0tBDg3JgZ93n/G8ALM77cw1jWLaQpDWzRI5FwUgcZu8wiJeX7GbuStP79Q+9MImPDKwHYnRoV4undQbtpPw5VTYMN88T+hqdnk1au3VYlVLSENIvNg8SioqwHZoB5uSF9G9eQTWoymnB2Tb8k61Iu0o/bzI80oMvj7/dKtReJxmp4mcogAk1Va8U/vLX+4gI6f0Tu0ibnNoG7x/Jxzebp73fhCueNycxVUfnepOS2t4MY5LBmMNONWC6nCcmp12xpT9zB2Qe9h8LftAqe40gqJKBKISs9MatlB3mvgc/cRLtXx3aqf27SV2ap98VQcGd4nXAGdxH8OAH/4Nyx6DonwIawzXvVa6K8lX+PlBg0TzaNPf9bW8oyVmp5U4ju2Fgiw4sME8XJ4XYLaglRx8HdsWGp2v7jSptxSApEpSMk4wbel2vv5VO7VLDcs7Ch/dDzs+M8/bDIBhc8wFDaW00Gho1tM8SrLln5qdVrLV6FR3WtFJOPyreZwpsmnZrUbhjdWdJnWaApBUypETBcz86jfeXW/u1O7vZ+HWPzbnwSvP107t4n57VpmzvHIOgjUQ+j8NPcdWb20fXxUQDHEdzaMkhwOyfy89ZT9z56nutN/NY/e3ru8Ljio9+Dq2nbkMgbrTpA7QT6lUSL7NzsI1e/mXy07tcUwZ3J7WsWoiFzez22D5dFj1CmCYXTE3zIeEC71dsvrHzw8aNDOP88vrTtvhuq7R8X2QnwW//2AeJVkDIbq16+Dr4hWxA8M897lEzkEBSM7KMAw++TmNF0vs1N6pSSRTh2indqkhx/aZKzr/vt487zYSBr2gX57ecNbutF1nrGm0EzJTTnWnbTeP7Wc8Lyqx7NlpYbHqThOPUwCScm3cd4xnP9vGplM7tcdHBjNpYDuGdzsPP+3ULjVhy/vw6UPmVhJBUTB0JnQe7u1SyZkCgiGuk3mU5HBA1v6yW43yMs3XsvbDrm9c3xfcwHXwdfGf1Z0mNUg/WVLKmTu1hwae2qm9bytCAjXAWWpAwQn4/FHY/LZ53vRiuP7f0LC5d8sllePnZ37PGjYvpzttp+uGspk7zRa//ONmi19xq18xayA0auM6+Dq2rXlNLYJSTQpA4iIjO58hf19Fdn4RFgv8qXsif0lqS2Pt1C415eBmWDwajqQAFuj3MFz6qP7lX9+ERkOzP5pHSbaTcGRX6TWNjvxmLnmQsc08zhTVrMTg6xKrYYfFqDtNKkR/w4iLNbuOkJ1fRLPoUF67tbt2apea43DA97Phq6fAYTNXLx4+F1pc4u2SiScFhEB8Z/MoyeGArNQzpuwXd6cdMV/LSoVdX7u+L7hB6Sn7sae60/zUgi2nKQCJi60HswC4vF2swo/UnBMZsOReSPnKPG9/NVzzD7OVQAROdae1MI/zB7i+lnvEdcp+cUA6nmp2p+1fZx4lWYNKdKe1PR2SGp0PgdqqxxcpAImLrQfNFZ07NYnyckmk3kr5Gj4ca+6A7h8MA5+HHneq20IqLqwRhPWC5r1crxfmmbPTSg6+Ll7s0V4AGVvN40xRzSC2LX7RrWmeWYgltQHEd4LQRvq5rMcUgMTJMAy2ndrSQq0/4nZFhfD107D2n+Z5445w/fzSC/OJVFVgKMR3MY+SHHazdajU7LQdcPKYszvNyld0BfjPG+b7QhqWnrIfc7660+oJBSBxOpiVz/E8G/5+Fs6P0+KG4kZHdpmbmKZtNs//cBck/c0c/yFS0/ysEN3SPNomub6Wm+lsKbIf2k7mr2tobDmOJWu/GY72f28eJRV3p5Wcsl8cjvQzXWcoAInTtlPdX+fHRWg/L3EPw4Cf/g8+mwS2XPNf1Nf+C9oP8XbJRExhMebRvDcOm43vbUsZPHgwAYbNnJlYcvD14Z3mtXK70yzmBrUlB187Z6c18srHk/IpAIlT8QDojgnq/hI3yM+CTyfCL++b5y36wnWvQ9R53i2XSEUEhkLCBeZRksNubgVy5oayxd1px1PNIyXZ9X0h0aVDUcz55hYk6k7zCgUgcdrmHACtACTVtP8Hc22f4/vAYoXL/wqXPKS/6KXu87NCdCvzaDvw9HXDMKfnnzll//BOc4zRyaNld6f5B5+anXbGStiN2qg7rYYpAIlT8QwwDYCWKnPY4buZ8M1zYNjNf91ePx8SL/Z2yURqlsVyujutRR/X1wrzzIUdz1zT6EiKudjjoV/Mw/WB5v8/JafsF3etqTvNLRSABIDjeYUcOG5udqoAJFWSfRA+uBv2rjLPO18PV78KwVpSQXxcYCgkXGgeJRV3pzmn7JdY1yj/uPna8X2lu9NCG7kOvi4OSFHNzPWTpEIUgATAOf09MTqEyOAAL5dG6pwdn8OScWYzf0AYDH4Jut6iNVREzqZkd1q7q05fN4xTs9N2lF7TKGu/2dWWutY8SvIPNhd2jC0xKy2mHTRqre60MigACVBi/E+C/rUulWDLh+THYf1c8zz+ArjhDYhp491yidRlFguEx5rHmVvDFOaeGnT9m2urkbM7bYt5uD7Q3KC2VKtRW59efV0BSAANgJYqOLwDltx9eipwr/vhyifAP8i75RKpzwLDoElX8yjJXnRqdtrOM1qNdpgzMo/tNY/flrm+LzSm7NlpUYn1vjtNAUgADYCWSjAMmmd+g/+Cu81/cYbFwrDX4Pz+3i6ZiO+y+ptdXY1aQ7tBp68bBuQeLj1lP/O3U91pmZCaCalrXJ/nH2K25JYMRbHtILo1BAR79rPVEK8HoNmzZ/PSSy+RlpZGp06dmDlzJn379i3z3lGjRvHmm2+Wut6xY0e2bj29INXx48eZOnUqH3zwAceOHaNly5a88sorDB48uMY+R12Wb7OTcvgEoD3A5BzyjmL96AG67v/UPG99JVz3GoQ39m65RKRsFov5/2d4Y2h5xu/WghOuiz0eLtmddhLSt5iHy/P8zK1Aymo1qmPdaV4NQIsWLWLChAnMnj2bPn368PrrrzNo0CC2bdtGs2bNSt0/a9Yspk+f7jwvKiriwgsv5MYbb3ReKywsZMCAATRu3Jj333+fpk2bsn//fiIiIjzymeqinYdysDsMosMCiYtU94WUY+938MFd+GUfwGGxYlzxBNY+D9b7ZnKReiso/OzdaWWtaVSQBcf2mMdvX7q+LyzWdfB1cUiKbFor/57wagCaMWMGo0ePZsyYMQDMnDmTL7/8kjlz5jBt2rRS90dFRREVdbqFYsmSJRw7dow77rjDeW3BggUcPXqUNWvWEBBgzmZq3rx5DX+Sum1rifE/Fs3akTPZi2Dli7DyJTAcGNGtWBl7O33+eB/WWviXmohUU8nuNEr0nBgGnMgoPWU/8zfI/t3sass9DPu+c31eQOjpxR6Lp+w3aI2fw+bRj3UmrwWgwsJCNm7cyOTJk12uJyUlsWbNmnLe5Wr+/Pn079/fJeB8/PHH9OrVi/vuu4+PPvqI2NhYbrnlFh599FGs1rJXoS0oKKCgoMB5np1tBgKbzYbNVvVvUPF7q/MMT/jl9+MAtI8Lr/VlLU9dqes6J2s/1iVj8ft9HQCOC24m/4q/kbVijeraQ/Sz7Tmq6woIjoamfzSPkgrN7jRL5k4szv/+Bkd3Y7HlQfrP5nFKAHB5UDy2ge4dmlKZ753XAlBmZiZ2u524uDiX63FxcaSnp5/z/WlpaXz++ee8++67Ltd3797NN998w5///GeWLl3Kb7/9xn333UdRURFPPPFEmc+aNm0aTz/9dKnry5YtIzQ0tBKfqmzJycnnvsmLvttmBSwUHNrF0qUp3i5OtdT2uq5LmhxbT9f9C/Cz52HzC+anxFEcsPaGFeY/UFTXnqX69hzVdXWEA10htCuEgqVpEaEFh4koSCM8/yAR+cX/PUhOUALr3VzXeXl5Fb7X64Ogz+xyMQyjQt0wCxcupEGDBgwbNszlusPhoHHjxsydOxer1Ur37t05ePAgL730UrkBaMqUKUycONF5np2dTWJiIklJSURGVn1WlM1mIzk5mQEDBji742obu8NgysZvADu3DOpH69gwbxepSupCXdcZhblYk6fit/dtABxNusOw17mwYQsuRHXtaapvz1Fde46tsJCNX37m9rou7sGpCK8FoJiYGKxWa6nWnoyMjFKtQmcyDIMFCxYwcuRIAgMDXV5LSEggICDApburQ4cOpKenU1hYWOp+gKCgIIKCSg/+DQgIcMs3xl3PqQmph0+QV2gnJMDK+fFRWP3q9hig2lzXdUL6Fnj/TrOPHwv0nYjfZVPws5auU9W1Z6m+PUd17Rl2a5Db67oyz/LaCMbAwEC6d+9eqqkxOTmZ3r17n/W9K1asICUlhdGjR5d6rU+fPqSkpOBwOJzXdu7cSUJCQpnhx9cVD4BunxBR58OPVINhwPdzYN4VZviJSIDbPjIXNiwj/IiI1HVencIxceJE/v3vf7NgwQK2b9/OQw89RGpqKmPHjgXMrqnbbrut1Pvmz59Pz5496dy5c6nX7r33Xo4cOcL48ePZuXMnn332Gc8//zz33XdfjX+euqh4BeiOCVoA0WflZsK7I+CLyWAvhLaDYOx30OpSb5dMRKTGeHUM0IgRIzhy5AjPPPMMaWlpdO7cmaVLlzpndaWlpZGamurynqysLBYvXsysWbPKfGZiYiLLli3joYce4oILLuC8885j/PjxPProozX+eeqirQezAC2A6LN2fQsf3gMnDoE1CAY+B38Yo01MRaTe8/og6HHjxjFu3LgyX1u4cGGpa1FRUecc5d2rVy++//57dxSvXjMMQ3uA+aqiQvj2Wfju74ABse3h+vkQX7pVVUSkPvJ6ABLvycgp4EhuIVY/C+3itVK2zzi6G94fDQd/NM+73wEDn4fA6i/5ICJSVygA+bDi7q/WsWEEB5S9SKTUMz8tgs8mmouWBTeAa/4BHa/xdqlERDxOAciHaQC0D8nPhqWT4OdF5nmz3nD9PIhq6t1yiYh4iQKQDzu9B5gGQNdrBzaaXV7H9pg7OV86GfpNAj+1+omI71IA8mHb0jQAul5zOGDN3+Gbv4GjCKIS4fp/Q7M/nvu9IiL1nAKQj8rOt7HviDmbrqMCUP2Tk25Ob9+93DzvOAyGzoSQhl4slIhI7aEA5KO2n+r+Oq9BCA1CtUJ2vbLzS1hyL+QdgYBQuGo6XHSb1vYRESlBAchHFXd/ddAA6PqjqACSn4R1c8zzuC5wwwKIbevdcomI1EIKQD5qqxZArF8O7zAHOh/aYp73vBf6PwUBwV4tlohIbaUA5KOcU+AVgOo2w4Af34LPH4WikxDaCIbNgbYDvV0yEZFaTQHIBxUWOfgtIwdQC1CddvI4fDIeti0xz1tdBte9DhHxXiyUiEjdoADkg37LyMFmN4gKCeC8BiHeLo5URer3sHgMZO0HP3+44nHo/SD4+Xm7ZCIidYICkA/aWmIFaItmBtUtDjusfBlWTAfDAQ1bmpuYNu3u7ZKJiNQpCkA+SDvA11FZv8MHd8O+78zzC26CIS9DkDayFRGpLAUgH6QB0HXQto/h4wcg/zgEhsOQGXDhCG+XSkSkzlIA8jEOh1FiCwztAVbrFebBl3+FjW+Y500uMrezaNTau+USEanjqjRictWqVdx666306tWLAwcOAPCf//yH1atXu7Vw4n77j+VxoqCIQH8/WseGebs4cjbpv8C8y0+Hnz4T4M4vFX5ERNyg0gFo8eLFDBw4kJCQEDZt2kRBQQEAOTk5PP/8824voLhX8QDo9vER+Fs1Y6hWMgxYNxfmXQGHf4XwOBi5BAY8Df7atkRExB0q/Rvw2Wef5bXXXmPevHkEBAQ4r/fu3Zsff/zRrYUT99t6MAvQAOhaK/cI/N/N8PnDYC+A8wfCvWug9eXeLpmISL1S6TFAO3bsoF+/fqWuR0ZGcvz4cXeUSWrQthJT4KWW2b3C3ME9Jw2sgTDgb9DzHm1iKiJSAyodgBISEkhJSaFFixYu11evXk2rVq3cVS6pIc41gDQAuvaw2+Db52H1q4ABMW3NtX0SLvB2yURE6q1KB6B77rmH8ePHs2DBAiwWCwcPHmTt2rVMmjSJJ554oibKKG5yOKeAjJwCLBZzDJDUAkf3mCs6H9hgnl90G1w1HQI1QF1EpCZVOgA98sgjZGVlcfnll5Ofn0+/fv0ICgpi0qRJ3H///TVRRnGT4unvLWPCCAvSCghet+V9+GQCFOZAcBQM/Tt0GubtUomI+IRK/Ra02+2sXr2av/zlL0ydOpVt27bhcDjo2LEj4eHhNVVGcZPTK0Cr+8urCnJg6SPw07vmeeIf4fp50KCZd8slIuJDKhWArFYrAwcOZPv27URHR9OjR4+aKpfUgOIZYBoA7UUHN8H7d8LR3WDxg34PQ79HwKoWORERT6r037pdunRh9+7dtGzZsibKIzVIe4B5kcMB3/8LvnoaHDaIbGq2+jTv7e2SiYj4pEqvA/Tcc88xadIkPv30U9LS0sjOznY5pHbKLShiz5FcQHuAeVzOIXjnelj2mBl+OgyFsasUfkREvKjSLUBXXXUVANdccw2WEuuTGIaBxWLBbre7r3TiNr+mZ2MYEBcZREx4kLeL4zt+S4Yl90LuYfAPgaueh+53aG0fEREvq3QA+vbbb2uiHFLDNADaw4oK4OtnYO0/zfPGneCGBdC4vXfLJSIiQBUC0KWXXloT5ZAatlUrQHtO5m/mQOf0n83zi+82V3UOCPZuuURExKlKU0+OHz/O/Pnz2b59OxaLhY4dO3LnnXcSFaXWhdpqqwZA1zzDgM3vwNKHwZYHIdFw7b+g/WBvl0xERM5Q6UHQGzZsoHXr1rz66qscPXqUzMxMZsyYQevWrbUZai1lszvYcSgH0ADoGpOfBYtHw0f3meGnRV+49zuFHxGRWqrSLUAPPfQQ11xzDfPmzcPf33x7UVERY8aMYcKECaxcudLthZTq2XX4BIVFDiKC/ElsGOrt4tQ/+9eb4ed4KliscMVU6DMB/KzeLpmIiJSj0gFow4YNLuEHwN/fn0ceeUQLI9ZSxQOgOyRE4uen2Udu47DD6hnw7TQw7NCgubmJaeIfvF0yERE5h0oHoMjISFJTU2nf3nU2y/79+4mI0AabtdHpHeDV/eU2WQfgw3tg7yrzvPMNcPUMc08vERGp9SodgEaMGMHo0aN5+eWX6d27NxaLhdWrV/Pwww9z880310QZpZq0ArSb/fqZOdbn5DEICIMhL8OFN2ttHxGROqTSAejll1/GYrFw2223UVRUBEBAQAD33nsv06dPd3sBpXoMwzi9B5gCUPXYTpqrOf/wb/M8oavZ5RXTxqvFEhGRyqt0AAoMDGTWrFlMmzaNXbt2YRgGbdq0ITRUg2tro9+PnSQ7v4gAq4XzG6uLssoObTMHOmdsM897PwBXPAH+gd4tl4iIVEmlA1BWVhZ2u53o6Gi6dOnivH706FH8/f2JjFQrQ22yLc3s/jq/cQSB/pVe9UAMAzbMhy+nQlE+hDWG6+ZAm/7eLpmIiFRDpX8j3nTTTbz33nulrv/3v//lpptuckuhxH20AGI15B2FRbfCZ38xw0+b/ubaPgo/IiJ1XqUD0Lp167j88stLXb/ssstYt26dWwol7qMB0FW0dzXM6QO/fgp+ATDwebjlfxDe2NslExERN6h0F1hBQYFz8HNJNpuNkydPuqVQ4j7bnAOgNT27QuxFsGI6rHwZMKBRG3Ogc5Ou3i6ZiIi4UaVbgP7whz8wd+7cUtdfe+01unfv7pZCiXscyy3kYFY+AB0SNAD6nI7tgzcGwcqXAAO63gp3r1D4ERGphyrdAvTcc8/Rv39/fvrpJ6688koAvv76a3744QeWLVvm9gJK1RUPgG7eKJSI4AAvl6aW+2UxfPIQFGRBUCQMnQmdr/d2qUREpIZUugWoT58+rF27lsTERP773//yySef0KZNG37++Wf69u1bE2WUKipe/0fjf86iMNdc1PD9O83w0/QPMHaVwo+ISD1X6RYggK5du/LOO++4uyziZsUDoDsmKACVKe0nM/gcSQEs0PcvcNlksKq1TESkvqt0C9CPP/7Ili1bnOcfffQRw4YN469//SuFhYVuLZxUz+kp8BoA7cIwYO1s+Hd/M/xENIHbP4ErH1f4ERHxEZUOQPfccw87d+4EYPfu3YwYMYLQ0FD+97//8cgjj7i9gFI1Jwvt7Dp8AlAXmIsTh+GdG+HLKWAvhHZDzLV9Wqr7VkTEl1Q6AO3cuZOuXbsC8L///Y9LL72Ud999l4ULF7J48WJ3l0+qaMehHBwGxIQHEhsR5O3i1A4pX8Oc3pCSDP7BMPhluOkdCI32dslERMTDKh2ADMPA4XAA8NVXXzF48GAAEhMTyczMrHQBZs+eTcuWLQkODqZ79+6sWrWq3HtHjRqFxWIpdXTq1KnM+9977z0sFgvDhg2rdLnquq0l1v+x+Pou5UWFsOxxeHs45GZAbAe461u4+C7t4C4i4qMqHYB69OjBs88+y3/+8x9WrFjBkCFDANizZw9xcXGVetaiRYuYMGECU6dOZdOmTfTt25dBgwaRmppa5v2zZs0iLS3Neezfv5/o6GhuvPHGUvfu27ePSZMm+ezMNA2APuXILliQBGv+bp73GA13fwtxHb1bLhER8apKB6CZM2fy448/cv/99zN16lTatGkDwPvvv0/v3r0r9awZM2YwevRoxowZQ4cOHZg5cyaJiYnMmTOnzPujoqKIj493Hhs2bODYsWPccccdLvfZ7Xb+/Oc/8/TTT9OqVavKfsR6wef3ADMM2Px/8FpfOLgJQhrCiHfg6hkQEOLt0omIiJdVehr8BRdc4DILrNhLL72E1Wqt8HMKCwvZuHEjkydPdrmelJTEmjVrKvSM+fPn079/f5o3b+5y/ZlnniE2NpbRo0eftUutWEFBAQUFBc7z7GwzPNhsNmw2W4XKUpbi91bnGVVhdxj8mm5+hnaNQz3+9b3Bpa4LcrB+Pgm/reaYNEez3tivfQ0im4AP1EVN89bPta9SfXuO6tpzaqquK/O8Kq0DVJbg4OBK3Z+ZmYndbi/VbRYXF0d6evo535+Wlsbnn3/Ou+++63L9u+++Y/78+WzevLnCZZk2bRpPP/10qevLli0jNDS0ws8pT3JycrWfURnpeZBv8yfQz2DruhVs96FhLhuWvEb3vXMIK8zAgR87Eq5jZ/RQWL0Z2Ozl0tUvnv659nWqb89RXXuOu+s6Ly+vwve6LQBV1ZkDdA3DqNCg3YULF9KgQQOXAc45OTnceuutzJs3j5iYmAqXYcqUKUycONF5np2dTWJiIklJSURGVr0LyWazkZyczIABAwgI8Nz6Mh//lAY/baFz04ZcPeRij31db7IVFrD37Ql0OPQhFkcRRlQijmGv06bpxbTxduHqGW/9XPsq1bfnqK49p6bqurgHpyK8FoBiYmKwWq2lWnsyMjLOOZjaMAwWLFjAyJEjCQwMdF7ftWsXe/fuZejQoc5rxTPW/P392bFjB61bty71vKCgIIKCSk8VDwgIcMs3xl3PqaidGbmAuQCiT/xPnJ2G9YO76Jh2qruz03AsV7+Kf0gDrxarvvP0z7WvU317jurac9xd15V5ltcCUGBgIN27dyc5OZnrrrvOeT05OZlrr732rO9dsWIFKSkpjB492uV6+/btS41Peuyxx8jJyWHWrFkkJia67wPUYj41AHrH57BkHH4nj1LkFwiDXsK/x+2a3i4iImfl1S6wiRMnMnLkSHr06EGvXr2YO3cuqampjB07FjC7pg4cOMBbb73l8r758+fTs2dPOnfu7HI9ODi41LUGDRoAlLpeXxmG4dwFvmN9DkC2fEh+Ata/DoAR14UV0bfSr+ufFX5EROScKj0Nvjz79+/nzjvvrNR7RowYwcyZM3nmmWfo2rUrK1euZOnSpc5ZXWlpaaXWBMrKymLx4sWlWn/ElJ6dz9HcQqx+FtrGRXi7ODUj41f495XO8MMf76No1BecCE7wbrlERKTOcFsL0NGjR3nzzTdZsGBBpd43btw4xo0bV+ZrCxcuLHUtKiqqUqO8y3pGfbb1gNn6c37jcIIDKr4sQZ1gGLBxIXwxBYpOQmgMXPcanD9A09tFRKRSKhyAPv7447O+vnv37moXRqrP2f1V31aAzjsKnzwI2z8xz1tdDte9DhGVW31cREQEKhGAhg0bhsViwTCMcu/x+T2naoHTe4DVowC0bw0svguyfwe/ALjyCeh1P/i5rQdXRER8TIV/gyQkJLB48WIcDkeZx48//liT5ZQKqlcDoO1F8O00WDjEDD/RrWD0MujzoMKPiIhUS4V/i3Tv3v2sIedcrUNS87JO2th/9CQAnRKivFyaajqeagafFdPBcMCFt8A9K+G8i7xdMhERqQcq3AX28MMPk5ubW+7rbdq04dtvv3VLoaRqtp9q/WnaMISo0Dq8iNfWJeZ4n/wsCIyAq1+FC270dqlERKQeqVAA+vnnn+nTpw9+Z+l2CAsL49JLL3VbwaTyihdArLMDoAtzzRleP75pnp/XHa6fD9EtvVsuERGpdyrUBdatWzcyMzMBaNWqFUeOHKnRQknVFA+A7tSkDnZ/pW+BuZedCj8WuGQi3Pmlwo+IiNSICrUANWjQgD179tC4cWP27t3r3F9LapdtB+vgAGjDgPVzYdljYC+E8HgYPhdaqTVRRERqToUC0PXXX8+ll15KQkICFouFHj16YLWWvcie1gPyjoIiOykZJ4A6tAdYbiZ8dB/s/MI8b3sVXDsbwhp5t1wiIlLvVSgAzZ07l+HDh5OSksKDDz7IXXfdRUREPd1moY767dAJihwGDUIDSIgK9nZxzm33cvjgHjiRDtYgSHoWLr5L+3iJiIhHVHgW2FVXXQXAxo0bGT9+vAJQLXN6/E9k7V6Q0m6Db56F72YBBsS0gxsWQLxvbFYrIiK1Q6X3AnvjjTdqohxSTcUzwGr1AOiju2HxGDiw0TzvPgoGToPAUK8WS0REfI/bNkMV79pW26fA//xf+HQiFOZAcBRc8w/oeK23SyUiIj5KAagecDgM5yKItW4AdEEOfDYJfn7PPG/W25zl1SDRu+USERGfpgBUD+w7mkduoZ0gfz9axoR5uzinHdhodnkd3Q0WP7j0Ueg7Caz6sRMREe/Sb6J6oHgAdPuESPyttWCTUIcD1v4Dvn4GHEUQlQjD50HzXt4umYiICKAAVC/UqvE/Oenw4VjYfWpfuI7XwtBZENLQu+USEREpQQGoHjg9A8zLAWjnMlhyL+Rlgn8IDHoBLrpNa/uIiEitowBUD3g9ABUVQPKTsG6OeR7XBW6YD7HtvFMeERGRc1AAquMycvLJPFGAnwXax3shAB3eCYvvNDczBeg5Fvo/DQF1YDVqERHxWQpAdVxx60+r2HBCAsven61GGAZs+g98/ijY8iC0kbmPV7urPFcGERGRKlIAquO8MgD65HH4ZDxsW2Ket7wUrnsdIhM8VwYREZFqUACq47Z5evxP6jpzbZ+sVPDzhyseg97jwa8WTL8XERGpIAWgOu70Jqg1vAeYww6rXoHl08GwQ8MWcP0CaNq9Zr+uiIhIDVAAqsNOFBSx90geAB1rsgUo63f44G7Y9515fsEIGPwyBNeCdYdERESqQAGoDive/yshKpjosMAa+iKfwEf3Q/5xCAyHIa/AhTfVzNcSERHxEAWgOqxGB0AX5sGyqbBhgXnepBtcPx8atXb/1xIREfEwBaA67PT4HzcHoENb4f074fCv5nmf8XD5Y+BfQ61MIiIiHqYAVIcVrwHk1vE/Wb/DvweALRfC4+C616D1Fe57voiISC2gAFRHFRY5+O3QCcDNM8B+/cwMP407wW0fQXis+54tIiJSS2jxljoqJeMEhXYHEcH+NG0Y4r4H7zq1i/sFNyr8iIhIvaUAVEdtSzs9ANrirt3W7TbYu8r8c6vL3fNMERGRWkgBqI6qkQUQf/8BCk+Y+3rFX+C+54qIiNQyCkB11LaaGAC96xvzv60u09YWIiJSr+m3XB1kGIazC8ytU+CLx/9o1peIiNRzCkB1UHp2Pjn5RVj9LLSODXfPQ08eg4M/mn/W+B8REannFIDqoJQMc/p780ahBPq76Vu4ZyUYDohpC1HnueeZIiIitZQCUB1UHIDauKv1B9T9JSIiPkUBqA5yBqDG7gxAxQOg1f0lIiL1nwJQHeT2AHR0NxzfB34B0OIS9zxTRESkFlMAqoN2HXZzACru/kq8GILc2KokIiJSSykA1THH8wrJPFEI4L4ZYOr+EhERH6MAVMcUt/40iQomLMgNe9nai2DPqe0vNABaRER8hAJQHVM8/qe1u7q/Dv4IBVkQ3ACadHXPM0VERGo5BaA6xu0DoIvH/7S6FPys7nmmiIhILacAVMc4W4DcNf5nd3EA0vgfERHxHQpAdUyKO2eA5WfD/vXmn1srAImIiO9QAKpD8m12fj92EnBTANq7Ggw7RLeChi2q/zwREZE6QgGoDtl1+ASGAQ1CA2gUFuiGB2r6u4iI+CavB6DZs2fTsmVLgoOD6d69O6tWrSr33lGjRmGxWEodnTp1ct4zb948+vbtS8OGDWnYsCH9+/dn/fr1nvgoNa7kHmAWi6X6D9yt/b9ERMQ3eTUALVq0iAkTJjB16lQ2bdpE3759GTRoEKmpqWXeP2vWLNLS0pzH/v37iY6O5sYbb3Tes3z5cm6++Wa+/fZb1q5dS7NmzUhKSuLAgQOe+lg1Zpc7Z4AdT4UjKWCxQsu+1X+eiIhIHeLVADRjxgxGjx7NmDFj6NChAzNnziQxMZE5c+aUeX9UVBTx8fHOY8OGDRw7dow77rjDec8777zDuHHj6Nq1K+3bt2fevHk4HA6+/vprT32sGuPWAdDF09/P6w7BUdV/noiISB3ihqWEq6awsJCNGzcyefJkl+tJSUmsWbOmQs+YP38+/fv3p3nz5uXek5eXh81mIzo6utx7CgoKKCgocJ5nZ2cDYLPZsNlsFSpLWYrfW51nlPTboRwAWkQHV/uZ1pSv8QPsLS/F4abyeZO761rKp7r2LNW356iuPaem6royz/NaAMrMzMRutxMXF+dyPS4ujvT09HO+Py0tjc8//5x33333rPdNnjyZ8847j/79+5d7z7Rp03j66adLXV+2bBmhoaHnLMu5JCcnV/sZdgN2H7YCFlJ/+YGlKdV4mOFg0M6vCATWpAdxdOnSapevtnBHXUvFqK49S/XtOaprz3F3Xefl5VX4Xq8FoGJnDuY1DKNCA3wXLlxIgwYNGDZsWLn3vPjii/zf//0fy5cvJzg4uNz7pkyZwsSJE53n2dnZJCYmkpSURGRk5Lk/RDlsNhvJyckMGDCAgICAKj8HYO+RXOzff0dIgB9/HjYIP7+qD4K2HNyE/+ZcjKAI/nj9fWCtXtlqA3fWtZyd6tqzVN+eo7r2nJqq6+IenIrwWgCKiYnBarWWau3JyMgo1Sp0JsMwWLBgASNHjiQwsOzp4C+//DLPP/88X331FRdccMFZnxcUFERQUFCp6wEBAW75xrjjOXuPml10rWLDCQqq5hT4VHOmnaVFPwKCq9/CVZu463sm56a69izVt+eorj3H3XVdmWd5bRB0YGAg3bt3L9X8lZycTO/evc/63hUrVpCSksLo0aPLfP2ll17ib3/7G1988QU9evRwW5m9ya1bYBQPgNbqzyIi4qO82gU2ceJERo4cSY8ePejVqxdz584lNTWVsWPHAmbX1IEDB3jrrbdc3jd//nx69uxJ586dSz3zxRdf5PHHH+fdd9+lRYsWzham8PBwwsPdtH+WF7htE9TCXEj93vyz1v8REREf5dUANGLECI4cOcIzzzxDWloanTt3ZunSpc5ZXWlpaaXWBMrKymLx4sXMmjWrzGfOnj2bwsJCbrjhBpfrTz75JE899VSNfA5PcNsU+L3fgcMGUc3MLTBERER8kNcHQY8bN45x48aV+drChQtLXYuKijrrKO+9e/e6qWS1h2EY7lsEcXeJ7i93rCYtIiJSB3l9Kww5t0PZBZwoKMLqZ6FFo7DqPUzjf0RERBSA6oLi8T/No0MJ9K/Gtyz7IBzeDlig5aXuKZyIiEgdpABUB6RkmCtAt65299dy879NukFo+Stji4iI1HcKQHWA2wZA7/rG/K+6v0RExMcpANUBzinw1VkDyOE43QKk6e8iIuLjFIDqgF2Hc4FqtgBlbIXcwxAQBk0vdlPJRERE6iYFoFou66SNwznmNhjVGgNU3P3V4hLwr+ZWGiIiInWcAlAtV9z9FR8ZTHhQNZZt0vR3ERERJwWgWs4tCyDaTsK+NeafWykAiYiIKADVcm6ZAZa6FuwFENEEYtu5qWQiIiJ1lwJQLefcBb5a43+0/YWIiEhJCkC1nFumwDsDkKa/i4iIgAJQrZZvs7P/mLnxa5W7wE5kwKEt5p+1/YWIiAigAFSr7T6ci2FAVEgAMeFVnLpevPhhfBcIj3Vb2UREROoyBaBarOQAaEtVx+6o+0tERKQUBaBarNrjfwwDdp8KQJr+LiIi4qQAVIvtqu4U+MO/Qk4a+AdDs15uLJmIiEjdpgBUi1V7EcTi7q/mvSEg2E2lEhERqfsUgGopu8Ngd6a5CWrrqnaBFe//pe4vERERFwpAtdT+o3kUFjkI8vfjvIYhlX9AUQHs+878swZAi4iIuFAAqqWKB0C3ig3H6leFGWD714MtD8IaQ1wnN5dORESkblMAqqWqvQdYcfeXtr8QEREpRQGolqr2FHhNfxcRESmXAlAtlVKdGWB5R+HgZvPPrS5zW5lERETqCwWgWsgwjOpNgd+9HDCgcUeITHBr2UREROoDBaBaKCOngJyCIvws0CImtPIPUPeXiIjIWSkA1ULF3V/NG4UR5G+t3JsNQ/t/iYiInIMCUC1UHICqtADikV2QtR+sgeYK0CIiIlKKAlAtVK09wIqnvyf2hMAqdJ+JiIj4AAWgWuh0C1BY5d+8W91fIiIi56IAVAtVeQq83QZ7Vpl/bq0B0CIiIuVRAKplsvNtZOQUANC6sgHo9w1QmAMh0RB/YQ2UTkREpH5QAKplilt/4iKDiAwOqNybndPfLwM/fWtFRETKo9+StUy1VoAuuf+XiIiIlEsBqJbZVdU9wE4ehwMbzT9rAUQREZGzUgCqZarcArR3FRgOaHQ+NEisgZKJiIjUHwpAtUzKqTWAKj0A2tn9penvIiIi56IAVIvk2+zsP5oHVKEFyLn9hbq/REREzkUBqBbZk5mLw4DIYH9iw4Mq/saje+DYHvDzhxaX1FwBRURE6gkFoFqk5Pgfi8VS8TcWT39vejEERdRAyUREROoXBaBapHgPsEpvgqruLxERkUpRAKpFqjQDzGGHPSvMP2sAtIiISIUoANUiVQpABzdBfhYER0GTbjVUMhERkfpFAaiWsDsMdmfmApUMQMXT31v2Az9rDZRMRESk/lEAqiV+P5ZHYZGDQH8/mjYMrfgbneN/1P0lIiJSUQpAtURx91ermDCsfhWcAVaQA7+vN/+s7S9EREQqTAGolqjS+J+9q8FRBA1bQnTLGiqZiIhI/aMAVEtUKQBp+ruIiEiVKADVEsV7gFVpALS6v0RERCrF6wFo9uzZtGzZkuDgYLp3786qVavKvXfUqFFYLJZSR6dOnVzuW7x4MR07diQoKIiOHTvy4Ycf1vTHqBbDMCrfApT1Oxz5DSx+5gwwERERqTCvBqBFixYxYcIEpk6dyqZNm+jbty+DBg0iNTW1zPtnzZpFWlqa89i/fz/R0dHceOONznvWrl3LiBEjGDlyJD/99BMjR47kT3/6E+vWrfPUx6q0wzkF5OQX4WeBFo3CKvam4u6v87pDSIMaK5uIiEh95NUANGPGDEaPHs2YMWPo0KEDM2fOJDExkTlz5pR5f1RUFPHx8c5jw4YNHDt2jDvuuMN5z8yZMxkwYABTpkyhffv2TJkyhSuvvJKZM2d66FNVXnHrT2J0KMEBFVzLp7j7S9PfRUREKs3fW1+4sLCQjRs3MnnyZJfrSUlJrFmzpkLPmD9/Pv3796d58+bOa2vXruWhhx5yuW/gwIFnDUAFBQUUFBQ4z7OzswGw2WzYbLYKlaUsxe891zN2pmcB0ComtGJf79ge/Hd9jQUoat4XoxplrC8qWtdSfaprz1J9e47q2nNqqq4r8zyvBaDMzEzsdjtxcXEu1+Pi4khPTz/n+9PS0vj888959913Xa6np6dX+pnTpk3j6aefLnV92bJlhIZWYlHCciQnJ5/19a/3+AF+WHIyWLp06VnvDSs4RJ/fphFgyyI7uCnLf87A2HL29/iSc9W1uI/q2rNU356juvYcd9d1Xl5ehe/1WgAqZrG4LvpnGEapa2VZuHAhDRo0YNiwYdV+5pQpU5g4caLzPDs7m8TERJKSkoiMjDxnWcpjs9lITk5mwIABBAQElHvfojc2AEcZ0LMLgy86r/wHHknB/+1HsNiOYsS0I+TPHzAoPK78+31IRetaqk917Vmqb89RXXtOTdV1cQ9ORXgtAMXExGC1Wku1zGRkZJRqwTmTYRgsWLCAkSNHEhgY6PJafHx8pZ8ZFBREUFBQqesBAQFu+cac6zm7Tu0B1i4hqvz7Du+Et6+FE4cgtgOW2z8hIDy22mWrb9z1PZNzU117lurbc1TXnuPuuq7Ms7w2CDowMJDu3buXav5KTk6md+/eZ33vihUrSElJYfTo0aVe69WrV6lnLlu27JzP9JbsfBuHss3xR+VOgc/4FRYOMcNP404w6lNQ+BEREakyr3aBTZw4kZEjR9KjRw969erF3LlzSU1NZezYsYDZNXXgwAHeeustl/fNnz+fnj170rlz51LPHD9+PP369eOFF17g2muv5aOPPuKrr75i9erVHvlMlbXr1AywxhFBRAaXkVwPbYO3roHcwxDXBW77CMIaebiUIiIi9YtXA9CIESM4cuQIzzzzDGlpaXTu3JmlS5c6Z3WlpaWVWhMoKyuLxYsXM2vWrDKf2bt3b9577z0ee+wxHn/8cVq3bs2iRYvo2bNnjX+eqjjrAoiHtsKbQyHvCMRfYIaf0GgPl1BERKT+8fog6HHjxjFu3LgyX1u4cGGpa1FRUecc5X3DDTdwww03uKN4Na7cLTDSt8Cb18DJo5DQFUZ+qPAjIiLiJl4PQL5uV1ktQGk/wVvXwslj0OQiGPkBhDT0UglFRETqHwUgL3N2gcWeCkAHN8FbwyD/OJzXwww/wVFeK5+IiEh95PXNUH1Zvs1O6lGzO69143A4sNFs+ck/Dk0vNru9FH5ERETcTi1AXrTvSB4OAyKC/GmctQXevh4KsiDxj3Dr+xAU4e0iioiI1EtqAfKi4u6vwQ33Y3l7uBl+mvVW+BEREalhagHyopSME1xk2cnT2S+CIw+aXwK3LIKgchZEFBEREbdQAPIix741vBU4nWBHPrTsBze/B4Fh3i6WiIhIvacuMG/Zu5qx+x8h3JLPkbjecPMihR8REREPUQDyhj0rMd65kRDyWWnvwonr/gOBod4ulYiIiM9QAPK03cvhnT9hseXxrf1CxhkP07Sx9vYSERHxJAUgT9q9HN4dAUUnyWxyGffYJtI0piFWP4u3SyYiIuJTFIA8KTzenN7edhBL2k6nkABzAUQRERHxKM0C86TG7WH0Mohsys4l24ESW2CIiIiIxygAeVp0K+D0IohqARIREfE8dYF5gWEYpTdBFREREY9RAPKCzBOFZOcXYbFAq1it/SMiIuJpCkBeUNz6k9gwlOAAq5dLIyIi4nsUgLwg5fCp7i+N/xEREfEKBSAv2JWhACQiIuJNCkBeoAHQIiIi3qUA5AWaAi8iIuJdCkAelpNvIz07H1AXmIiIiLcoAHnYrsO5AMRGBBEVEuDl0oiIiPgmBSAP0/gfERER71MA8rDT43+0AKKIiIi3KAB5mFqAREREvE8ByMN2ORdBjPBySURERHyXApAHFRTZST2aB2gGmIiIiDcpAHnQviN52B0G4UH+xEUGebs4IiIiPsvf2wXwJZknCmgQGkDzRmFYLBZvF0dERMRnKQB5UO/WMWx+Iom8wiJvF0VERMSnqQvMC0IDlTtFRES8SQFIREREfI4CkIiIiPgcBSARERHxOQpAIiIi4nMUgERERMTnKACJiIiIz1EAEhEREZ+jACQiIiI+RwFIREREfI4CkIiIiPgcBSARERHxOQpAIiIi4nMUgERERMTnaFvyMhiGAUB2dna1nmOz2cjLyyM7O5uAgAB3FE3Kobr2HNW1Z6m+PUd17Tk1VdfFv7eLf4+fjQJQGXJycgBITEz0cklERESksnJycoiKijrrPRajIjHJxzgcDg4ePEhERAQWi6XKz8nOziYxMZH9+/cTGRnpxhLKmVTXnqO69izVt+eorj2npuraMAxycnJo0qQJfn5nH+WjFqAy+Pn50bRpU7c9LzIyUv8zeYjq2nNU156l+vYc1bXn1ERdn6vlp5gGQYuIiIjPUQASERERn6MAVIOCgoJ48sknCQoK8nZR6j3Vteeorj1L9e05qmvPqQ11rUHQIiIi4nPUAiQiIiI+RwFIREREfI4CkIiIiPgcBSARERHxOQpANWj27Nm0bNmS4OBgunfvzqpVq7xdpDpt2rRp/OEPfyAiIoLGjRszbNgwduzY4XKPYRg89dRTNGnShJCQEC677DK2bt3qpRLXH9OmTcNisTBhwgTnNdW1ex04cIBbb72VRo0aERoaSteuXdm4caPzddW3exQVFfHYY4/RsmVLQkJCaNWqFc888wwOh8N5j+q6alauXMnQoUNp0qQJFouFJUuWuLxekXotKCjggQceICYmhrCwMK655hp+//33mimwITXivffeMwICAox58+YZ27ZtM8aPH2+EhYUZ+/bt83bR6qyBAwcab7zxhvHLL78YmzdvNoYMGWI0a9bMOHHihPOe6dOnGxEREcbixYuNLVu2GCNGjDASEhKM7OxsL5a8blu/fr3RokUL44ILLjDGjx/vvK66dp+jR48azZs3N0aNGmWsW7fO2LNnj/HVV18ZKSkpzntU3+7x7LPPGo0aNTI+/fRTY8+ePcb//vc/Izw83Jg5c6bzHtV11SxdutSYOnWqsXjxYgMwPvzwQ5fXK1KvY8eONc477zwjOTnZ+PHHH43LL7/cuPDCC42ioiK3l1cBqIZcfPHFxtixY12utW/f3pg8ebKXSlT/ZGRkGICxYsUKwzAMw+FwGPHx8cb06dOd9+Tn5xtRUVHGa6+95q1i1mk5OTnG+eefbyQnJxuXXnqpMwCprt3r0UcfNS655JJyX1d9u8+QIUOMO++80+Xa8OHDjVtvvdUwDNW1u5wZgCpSr8ePHzcCAgKM9957z3nPgQMHDD8/P+OLL75wexnVBVYDCgsL2bhxI0lJSS7Xk5KSWLNmjZdKVf9kZWUBEB0dDcCePXtIT093qfegoCAuvfRS1XsV3XfffQwZMoT+/fu7XFddu9fHH39Mjx49uPHGG2ncuDHdunVj3rx5ztdV3+5zySWX8PXXX7Nz504AfvrpJ1avXs3gwYMB1XVNqUi9bty4EZvN5nJPkyZN6Ny5c43UvTZDrQGZmZnY7Xbi4uJcrsfFxZGenu6lUtUvhmEwceJELrnkEjp37gzgrNuy6n3fvn0eL2Nd99577/Hjjz/yww8/lHpNde1eu3fvZs6cOUycOJG//vWvrF+/ngcffJCgoCBuu+021bcbPfroo2RlZdG+fXusVit2u53nnnuOm2++GdDPdk2pSL2mp6cTGBhIw4YNS91TE787FYBqkMVicTk3DKPUNama+++/n59//pnVq1eXek31Xn379+9n/PjxLFu2jODg4HLvU127h8PhoEePHjz//PMAdOvWja1btzJnzhxuu+02532q7+pbtGgRb7/9Nu+++y6dOnVi8+bNTJgwgSZNmnD77bc771Nd14yq1GtN1b26wGpATEwMVqu1VGLNyMgolX6l8h544AE+/vhjvv32W5o2beq8Hh8fD6B6d4ONGzeSkZFB9+7d8ff3x9/fnxUrVvD3v/8df39/Z32qrt0jISGBjh07ulzr0KEDqampgH623enhhx9m8uTJ3HTTTXTp0oWRI0fy0EMPMW3aNEB1XVMqUq/x8fEUFhZy7Nixcu9xJwWgGhAYGEj37t1JTk52uZ6cnEzv3r29VKq6zzAM7r//fj744AO++eYbWrZs6fJ6y5YtiY+Pd6n3wsJCVqxYoXqvpCuvvJItW7awefNm59GjRw/+/Oc/s3nzZlq1aqW6dqM+ffqUWtJh586dNG/eHNDPtjvl5eXh5+f6q89qtTqnwauua0ZF6rV79+4EBAS43JOWlsYvv/xSM3Xv9mHVYhjG6Wnw8+fPN7Zt22ZMmDDBCAsLM/bu3evtotVZ9957rxEVFWUsX77cSEtLcx55eXnOe6ZPn25ERUUZH3zwgbFlyxbj5ptv1vRVNyk5C8wwVNfutH79esPf39947rnnjN9++8145513jNDQUOPtt9923qP6do/bb7/dOO+885zT4D/44AMjJibGeOSRR5z3qK6rJicnx9i0aZOxadMmAzBmzJhhbNq0ybn8S0XqdezYsUbTpk2Nr776yvjxxx+NK664QtPg66J//etfRvPmzY3AwEDjoosuck7XlqoByjzeeOMN5z0Oh8N48sknjfj4eCMoKMjo16+fsWXLFu8Vuh45MwCprt3rk08+MTp37mwEBQUZ7du3N+bOnevyuurbPbKzs43x48cbzZo1M4KDg41WrVoZU6dONQoKCpz3qK6r5ttvvy3z7+jbb7/dMIyK1evJkyeN+++/34iOjjZCQkKMq6++2khNTa2R8loMwzDc364kIiIiUntpDJCIiIj4HAUgERER8TkKQCIiIuJzFIBERETE5ygAiYiIiM9RABIRERGfowAkIiIiPkcBSERERHyOApCIOF122WVMmDDhrPdYLBaWLFlS7ut79+7FYrGwefPmcu9Zvnw5FouF48ePV6mcFVWRz1Mbeap+RHyZv7cLICJ1S1paGg0bNvR2Meq13r17k5aWRlRUlLeLIlJvKQCJSKXEx8d7uwh1is1mIyAgoFLvCQwMVD2L1DB1gYmIC4fDwSOPPEJ0dDTx8fE89dRTLq+f2QW2fv16unXrRnBwMD169GDTpk2lnrl06VLatm1LSEgIl19+OXv37i11z5o1a+jXrx8hISEkJiby4IMPkpub63y9RYsWPP/889x5551ERETQrFkz5s6dW6nP9vbbb9OjRw8iIiKIj4/nlltuISMjAwDDMGjTpg0vv/yyy3t++eUX/Pz82LVrFwBZWVncfffdNG7cmMjISK644gp++ukn5/1PPfUUXbt2ZcGCBbRq1YqgoCDK2nJx3759DB06lIYNGxIWFkanTp1YunQpULoL7LLLLsNisZQ6iuvxXGUSkdIUgETExZtvvklYWBjr1q3jxRdf5JlnniE5ObnMe3Nzc7n66qtp164dGzdu5KmnnmLSpEku9+zfv5/hw4czePBgNm/ezJgxY5g8ebLLPVu2bGHgwIEMHz6cn3/+mUWLFrF69Wruv/9+l/teeeUVZ8gaN24c9957L7/++muFP1thYSF/+9vf+Omnn1iyZAl79uxh1KhRgBns7rzzTt544w2X9yxYsIC+ffvSunVrDMNgyJAhpKens3TpUjZu3MhFF13ElVdeydGjR53vSUlJ4b///S+LFy8udyzUfffdR0FBAStXrmTLli288MILhIeHl3nvBx98QFpamvMYPnw47dq1Iy4ursJlEpEz1Mge8yJSJ1166aXGJZdc4nLtD3/4g/Hoo486zwHjww8/NAzDMF5//XUjOjrayM3Ndb4+Z84cAzA2bdpkGIZhTJkyxejQoYPhcDic9zz66KMGYBw7dswwDMMYOXKkcffdd7t83VWrVhl+fn7GyZMnDcMwjObNmxu33nqr83WHw2E0btzYmDNnzlk/z/jx48t9ff369QZg5OTkGIZhGAcPHjSsVquxbt06wzAMo7Cw0IiNjTUWLlxoGIZhfP3110ZkZKSRn5/v8pzWrVsbr7/+umEYhvHkk08aAQEBRkZGRrlf1zAMo0uXLsZTTz1V5mvffvutS/2UNGPGDKNBgwbGjh07KlwmESlNY4BExMUFF1zgcp6QkODsJjrT9u3bufDCCwkNDXVe69WrV6l7/vjHP2KxWMq9Z+PGjaSkpPDOO+84rxmGgcPhYM+ePXTo0KFU2SwWC/Hx8eWWrSybNm3iqaeeYvPmzRw9ehSHwwFAamoqHTt2JCEhgSFDhrBgwQIuvvhiPv30U/Lz87nxxhud5Txx4gSNGjVyee7JkyedXWQAzZs3JzY29qxlefDBB7n33ntZtmwZ/fv35/rrry9V92f6/PPPmTx5Mp988glt27atVJlExJUCkIi4OHPArsVicQaFMxlljG2pyj0Oh4N77rmHBx98sNRrzZo1q1LZzpSbm0tSUhJJSUm8/fbbxMbGkpqaysCBAyksLHTeN2bMGEaOHMmrr77KG2+8wYgRI5wBz+FwkJCQwPLly0s9v0GDBs4/h4WFnbM8Y8aMYeDAgXz22WcsW7aMadOm8corr/DAAw+Uef+2bdu46aabmD59OklJSc7rFS2TiLhSABKRKuvYsSP/+c9/OHnyJCEhIQB8//33pe45c92gM++56KKL2Lp1K23atKmxsv76669kZmYyffp0EhMTAdiwYUOp+wYPHkxYWBhz5szh888/Z+XKlS7lTE9Px9/fnxYtWlS7TImJiYwdO5axY8cyZcoU5s2bV2YAOnLkCEOHDmX48OE89NBDLq+5u0wivkKDoEWkym655Rb8/PwYPXo027ZtY+nSpaVmUY0dO5Zdu3YxceJEduzYwbvvvsvChQtd7nn00UdZu3Yt9913H5s3b+a3337j448/Lrc1pCqaNWtGYGAg//jHP9i9ezcff/wxf/vb30rdZ7VaGTVqFFOmTKFNmzYu3XX9+/enV69eDBs2jC+//JK9e/eyZs0aHnvssTLD1NlMmDCBL7/8kj179vDjjz/yzTffOLv6zjR8+HBCQkJ46qmnSE9Pdx52u92tZRLxJQpAIlJl4eHhfPLJJ2zbto1u3boxdepUXnjhBZd7mjVrxuLFi/nkk0+48MILee2113j++edd7rngggtYsWIFv/32G3379qVbt248/vjjJCQkuK2ssbGxLFy4kP/973907NiR6dOnlwprxUaPHk1hYSF33nmny3WLxcLSpUvp168fd955J23btuWmm25i7969xMXFVao8drud++67jw4dOnDVVVfRrl07Zs+eXea9K1euZOvWrbRo0YKEhATnsX//freWScSXWIyKdNCLiPiQ7777jssuu4zff/9dIUKknlIAEhE5paCggP3793P33XeTkJDgMitNROoXdYGJiJzyf//3f7Rr146srCxefPFFbxdHRGqQWoBERETE56gFSERERHyOApCIiIj4HAUgERER8TkKQCIiIuJzFIBERETE5ygAiYiIiM9RABIRERGfowAkIiIiPuf/AdyGz0fuC/RBAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_layers, train_recall, train_precision, train_f1, test_recall, test_precision, test_f1, train_acc, test_acc = read_logfile() \n",
    "\n",
    "# plotting train and test accuracies \n",
    "plot_accuracies([train_acc, test_acc], hidden_layers, [\"train accuracy\", \"test accuracy\"], \"hidden layer size\", \"accuracy\", \"partb_accuracies\") \n",
    "train_f1 = np.array(train_f1) \n",
    "train_f1_avg = np.mean(train_f1, axis = 0) \n",
    "test_f1 = np.array(test_f1)\n",
    "test_f1_avg = np.mean(test_f1, axis = 0)\n",
    "print(train_f1_avg)\n",
    "print(train_f1) \n",
    "plot_accuracies([train_f1_avg, test_f1_avg], hidden_layers, [\"train f1\", \"test f1\"], \"hidden layer size\", \"f1 score\", \"partb_f1\")\n",
    "\n",
    "# for i in range(1, 6):\n",
    "#     for j in range(len(hidden_layers)):\n",
    "#         print(f\"for hidden layer {hidden_layers[j]}\") \n",
    "#         print(f\"class {i} train precision is {train_precision[i-1]}\") \n",
    "#         print(f\"class {i} test precision is {test_precision[i-1]}\")\n",
    "#         print(f\"class {i} train recall is {train_recall[i-1]}\")\n",
    "#         print(f\"class {i} test recall is {test_recall[i-1]}\")\n",
    "#         print(f\"class {i} train f1 is {train_f1[i-1]}\")\n",
    "#         print(f\"class {i} test f1 is {test_f1[i-1]}\")\n",
    "#         print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "We see that f1 score is a good measure of generalization accuracy, since plots of test f1 score and test accuracy follow the same trend. Same holds for test data. \n",
    "\n",
    "In general, increasing the hidden layer size increases the test accuracy and test f1 score. \n",
    "\n",
    "We also observe that increasing the hidden layer size doesn't necessariliy increase f1 score. This could be due to overfitting of the model and loss of generalization accuracy with overly complex model . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveNeuralNetwork(Neural_Network):\n",
    "    def __init__(self, minibatch_size, no_of_features, hidden_layers, no_of_classes):\n",
    "        super().__init__(minibatch_size, no_of_features, hidden_layers, no_of_classes) \n",
    "        self.seed = 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 15]\n",
      "[[[ 5  6  7]\n",
      "  [10 12 14]\n",
      "  [15 18 21]]\n",
      "\n",
      " [[28 32 36]\n",
      "  [35 40 45]\n",
      "  [42 48 54]]]\n",
      "[[33 38 43]\n",
      " [45 52 59]\n",
      " [57 66 75]]\n",
      "[ 6 15]\n"
     ]
    }
   ],
   "source": [
    "arr1 = np.array([[1,2,3], [4,5,6]]) \n",
    "arr2 = np.array([[5,6,7], [7,8,9]])\n",
    "# arr3 = np.outer(arr1, arr2) \n",
    "# print(np.outer([1,2,4], [5,6,7]))\n",
    "print(np.sum(arr1, axis=1))\n",
    "arr3 = np.einsum('ij,ik->ijk', arr1, arr2) \n",
    "arr4 = np.sum(arr3.reshape(2, 3, 3), axis = 0)  \n",
    "print(arr3) \n",
    "print(arr4)\n",
    "\n",
    "def func(column):\n",
    "    # Replace this with your custom function logic\n",
    "    return column.sum()  # Example: Sum of each column\n",
    "\n",
    "# Vectorize the function using np.apply_along_axis\n",
    "result = np.apply_along_axis(func, axis=1, arr=arr1) \n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "def compute_grad_z(y):\n",
    "    phi = np.array([[0.5, 0.5], [0.5, 0.5]])  \n",
    "    def temp(index):\n",
    "        phi[index, y[index]] -= 1\n",
    "        return 1   \n",
    "    # self.grad_z[y] -= 1 \n",
    "    vectorize_temp = np.vectorize(temp)\n",
    "    vectorize_temp(np.arange(y.shape[0]))   \n",
    "    # self.grad_z_sum += self.grad_z\n",
    "    print(phi) \n",
    "\n",
    "\n",
    "def compute_loss(y):\n",
    "    batch_loss = 0 \n",
    "    phi = np.array([[0.5, 0.5], [0.5, 0.5]])\n",
    "    def temp(index):\n",
    "        batch_loss -= phi[index, y[index]] \n",
    "        return 1 \n",
    "    vectorize_temp = np.vectorize(temp)\n",
    "    vectorize_temp(np.arange(y.shape[0])) \n",
    "    print(batch_loss)\n",
    "\n",
    "# arr = np.array([[1,2], [1,2], [1,2], [1,2], [1,2], [1,2]]) \n",
    "# # arr = arr.T \n",
    "# # print(arr)\n",
    "# a = (np.array(np.split(arr, 3))) \n",
    "# for i in range(a.shape[0]): a[i] = a[i].T\n",
    "# print(a)\n",
    "\n",
    "# print(np.transpose([[1,2], [1,2]]))\n",
    "# compute_grad_z(np.array([0,1])) \n",
    "# compute_loss(np.array([0,1]))  \n",
    "\n",
    "# phis = np.array([[0.7, 0.5], [0.3, 0.9]]) \n",
    "# def temp(z) : return np.argmax(z)\n",
    "# pred = np.apply_along_axis(temp, axis=0, arr=phis) \n",
    "# print(pred)\n",
    "\n",
    "print(np.array([1,2,3]).reshape(-1, 1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
